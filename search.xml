<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mermaid]]></title>
    <url>%2F2019%2F04%2F20%2Fmermaid%2F</url>
    <content type="text"></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>mermaid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[商业分析能力是怎样炼成的]]></title>
    <url>%2F2019%2F04%2F19%2Fbizanalysis%2F</url>
    <content type="text"><![CDATA[本文整理摘录自知乎问答： 商业分析能力是怎样炼成的 以下内容摘自原文作者：@李启方原文出处：知乎原文链接：https://www.zhihu.com/question/20603837/answer/560150921 什么是商业分析wiki:商业分析，是识别业务需求和确定业务问题解决方案的研究学科。 通常包括战略规划、流程改进、组织变更、以及通过数据分析等手段来研究解决问题。 如何培养商业分析能力培养结构化思维什么是结构化思维结构化思维其实就是把复杂问题分解成多种单一因素的过程。 然后将这些因素加以归纳整理，使之纲领化。 如何培养结构化思维金字塔模型 任何事情都可以归纳出中心论点， 由中心论点出发，可由三至七个论据支撑。 每个一级论点可以衍生出其他分论点。 如何搭建金字塔模型1 尽可能列出所有思考的要点2 找出关系，进行分类原则是论点相互独立，不重叠；论据穷尽划分，不遗漏。 自己需要提高的点 1 尽可能列出所有思考的点 这一点自己的方法基本是依靠个人的直觉找点。 个人直觉应当继续保留。 惊奇的发现有时候就是来源于一时的灵感而并不总是套路。 但同时，还需要了解一些常用的思考方向有哪些。 原因是灵感不是时时都有，且仅依赖于灵感找的点不一定全。 所以结论，补充思考框架。 2 找出关系，进行分类 这点在自己的思维里是没有的。 原因，之前的经验里，一般是已经拿到了现成的已量化的各类数据，所以注意力和重点都在如何利用这些数据建模上。 所以一直忽略了这一步。 而在即使没有现成特征的情况下，自己的解决方式是通过找到很多认为有关的特征即可。 不需考虑特征间的关系。 如果涉及需要将特征分类或给出特征对因变量影响强弱，只需要找到合适的模型，即可通过模型将特征分类或得到特征对因变量的影响强弱。 如果仍然涉及分析特征间关联关系，也有对应的方法可以实现。 另一方面是，从对特征的认知上，认为特征很多时候是普遍存在关联的，而关联关系，尤其是复杂的关联关系，并不是人可以获得的。 虽然，模型获得的也并不一定是正确的。 但更多时候，会选择依赖模型给出关系。 因此，从之前的角度来讲，认为自己处理的并不是相互独立的并且可拆分的单一因素，而是一张交错的关系网。 这一点，与结构化思维的拆分要求有着很大的不同。 但两者之间，其实并不矛盾。 在自己寻找特征的时候，可以是有结构的，也可以对特征进行分类思考。 即可以利用这种思维的结构帮助自己筛选特征。 举例【问题】现在有一个线下销售的产品。我们发现8月的销售额度下降，和去年同比下降了20%。 找原因 1 尽可能列出所有思考的点 想先观察时间上的波动。 看是突然暴跌还是逐渐下降分不同的地区看一下差异，是不是有区域因素我也准备问几个销售员，看一下现在的市场环境怎么样，听说有几家竞争对手也缩水了，是不是这个原因 2 (结构化梳理)找出关系，进行分类 graph LR id[销售额]-->id11[内部] id11 --> id111[时间] id -->id12[外部] id11 --> id112[地区] id112 --> id1121[地区1] id1121 --> id11211[销售单价] id1121 --> id11212[销售率] id112 --> id1122[地区2] id112 --> id1123[地区3] id11 --> id113[消费者] id12 --> id121[市场容量] id12 --> id122[市场竞争] id12 --> id123[政策风险] 思考框架总结5W2H分析法5W2H分析法主要针对5个W以及2个H提出的7个关键词进行数据指标的选取，根据选取的指标进行分析。 graph LR id1(用户行为分析) --> id11(WHY) id1 --> id12(WHAT) id1 --> id13(WHO) id1 --> id14(WHEN) id1 --> id15(WHERE) id1 --> id16(HOW MUCH) id1 --> id17(HOW) id11 --> id111(用户购买目的是什么) id11 --> id112(产品有哪些地方能吸引用户) id12 --> id121(产品是什么) id12 --> id122(是否和用户需求一致) id13 --> id131 id13 --> id132(用户的特点(用户画像)) id14 --> id141(用户什么时候购买) id14 --> id142(用户购买的频率) id15 --> id151(用户购买的成本) id16 --> id161(用户如何购买) style id11 fill: #5cb85c style id12 fill: #5cb85c style id13 fill: #5cb85c style id14 fill: #5cb85c style id15 fill: #5cb85c style id16 fill: #9954bb style id17 fill: #9954bb ## 假说演绎思维 以情况为起点的推理方法是归纳推理，以规则为起点的推理方法可以称之为演绎推理。 举例【问题】 某电商，现想将商品提价，分析下销售额会有怎样的变化 首先可以确定销量会下降，那么下降多少？ 这里假设商品流量情况，提交后转化率的变化情况，然后根据历史数据汇总出销量下降的情况，从而得出销售额的变化情况。 graph LR id1 --> id11(销量) id1 --> id12(单价) id12 -- 降低-- > id11 id11 --> id111(流量) id11 --> id112(转化率) id112 --> id1121(用户) id1121 --> id11211(忠诚用户) id1121 --> id11212(普通用户) id1121 --> id11213(羊毛党) id12 --降低--> id112 id12 --高--> id11211 id12 --低--> id11212 id12 --无感--> id11213具体的变化情况都可以根据过往的数据来拟合，统计学上也有一些科学的预测模型。 假设先行就是以假设作为思考的起点，先提出问题，然后用MECE原则梳理关联因素间的结构关系。 在实际情况中，可针对不同的项目要求进行组合应用。在经过一定阶段的训练后，可以帮助提升业务熟悉程度，完成业务的初始积累后，后续的分析过程中就可以逐步减少拓展推理的层级及组合，逐步提升问题原因定位的效率。 ## 常用的商业分析模型 拥有了科学的思维能力之后，在结合商业场景/问题来分析，可以应用现成的商业分析模型，比如PEST、4P营销理论等，通过这些商业分析模型的训练，能帮助快速完成商业感的积累，完成原始业务逻辑积累。 在此基础上快速定位业务问题，提升分析效率。 ### PEST分析模型 id1(PEST分析法) --> id11(Political政治环境) id1 --> id12(Economic经济环境) id1 --> id13(Social社会环境) id1 --> id14(Technology技术环境) 政治环境国家的社会制度，执政党性质，政府的方针、政策、法令等。不同的政治环境对行业发展有不同的影响 关键指标政治体制，经济体制，财政政策，税收政策，产业政策，投资政策，专利数量，国防开支水平，政府补贴水平，民众政治参与度 经济环境宏微观。 宏观： 国家国民收入，国民生产总值以及变化情况，以通过这些指标反应国民经济发展水平和发展速度微观： 所在地区的消费者收入水平、消费偏好、储蓄情况、就业程度等因素，这些因素决定着企业目前以及未来的市场大小。 关键指标GDP及增长率、进出口总额及增长率、利率、汇率、通货膨胀率、消费价格指数、居民可支配收入、失业率、劳动生产率 社会环境一个国家或地区的居民受教育程度和文化水平、宗教信仰、风俗习惯、审美观点、价值观等。 文化水平营销居民的需求层次，宗教信仰和风俗习惯会禁止或抵制某些活动的进行；价值观会影响居民对组织目标和组织活动存在本身的认可；审美观点则会影响人们对组织活动内容、活动方式以及活动成果的态度 关键指标人口规模、性别比例、年龄结构、出生率、死亡率、种族结构、妇女生育率、生活方式、购买习惯、教育状况、城市特点、宗教信仰状况等因素 技术环境企业所处领域直接相关的技术手段发展变化，国家队科技开发的投资和支持重点。 该领域技术发展动态和研究开发费用总额，技术转移和技术商品化速度，专利及其保护情况。 关键指标新技术的发明和进展、折旧和报废速度、技术更新速度、技术传播速度、技术商品化速度、国家重点支持项目、国家投入的研发费用、专利个数、专利保护情况。 举例graph LR id1(互联网行业分析) --> id11(Political政治环境) --> id111(国家出台哪些行业政策，有何影响) id1 --> id12(Economic经济环境) --> id121(消费价格指数) id1 --> id12(Economic经济环境) --> id122(居民可支配收入) id1 --> id13(Social社会环境) --> id131(互联网网民占比) id1 --> id13(Social社会环境) --> id132(性别比例，人口分布，年龄结构，教育状况，生活习惯，购买偏好) id1 --> id14(Technology技术环境) --> id141(国家重点支持的项目) id1 --> id14(Technology技术环境) --> id142(互联网相关技术发展情况与趋势) 4P营销理论模型graph LR id1(4P营销理论模型) --> id11(Product产品) id1 --> id12(Price价格) id1 --> id13(Place渠道) id1 --> id14(Promotion促销) 产品能提供给市场，被人们使用和消费并满足人们某种需求的任何东西，包括有形产品、服务、人员、组织、观念和它们的组合。 价格购买产品时的价格，包括基本价格、折扣价格、支付期限等。影响价格的主要因素有需求、成本和竞争。 渠道产品从生产企业流转到用户手上全过程所经历的各个环节。 促销企业通过销售行为的改变来激励用户消费，以短期的行为促进消费的增长，吸引其他品牌用户或导致提钱消费来促进销售增长。 举例graph LR id1(业务场景分析) --> id11(Product) --> id111(提供哪种产品或服务) id1 --> id11 --> id112(产品或服务与用户需求是否一致) id1 --> id11 --> id113(购买产品的目标用户特点) id1 --> id12(Price) --> id121(产品的销售收入) id1 --> id12 --> id122(用户接受的合理价格) id1 --> id12 --> id122(用户的支付方式) id1 --> id13(Place) --> id131(公司销售渠道有哪些) id1 --> id13 --> id132(用户通过什么渠道购买) id1 --> id13 --> id133(公司对渠道政策是否有吸引力) id1 --> id14(Promotion) --> id141(投入多少促销资源？效果如何) id1 --> id14 --> id142(投入多少广告宣传？效果如何) 用户行为模型用户行为指用户为获取、使用产品或服务采取的各种行动。 首先要认知熟悉，然后试用，再决定是否继续消费使用，最后成为产品或服务的忠实用户。 行为轨迹： 认知-&gt;熟悉-&gt;试用-&gt;使用-&gt;忠诚 graph LR id1(网站行为分析) --> id11(认知) --> id111(网站方位) --> id1111[PV,UV,访问来来源,访问轨迹] id1 --> id12(熟悉) --> id121(网站浏览) --> id1211[平均浏览时长、跳出率、页面偏好] id1 --> id12(熟悉) --> id122(站内搜索) --> id1221[搜索访问占比] id1 --> id13(试用) --> id131(用户注册) --> id1311[注册用户数，注册转化率] id1 --> id14(使用) --> id141(用户登录) --> id1411[登录用户数，人均登录，访问登录比] id1 --> id14(使用) --> id142(用户付费) --> id1421[付费用户数，付费转化率] id1 --> id15(忠诚) --> id151(用户粘性) --> id1511[留存率，访问深度] id1 --> id15(忠诚) --> id152(用户流失) --> id1521[流失率] 小结PEST主要针对宏观市场环境进行分析，从政治、经济、社会以及技术四个维度对产品或服务是否适合进入市场进行数据化的分析，最终得到结论，辅助判断产品或服务是否满足大环境4P营销理论模型主要用于公司或其中某一个产品线的整体运营情况分析，通过分析结论，辅助决策近期运营计划与方案用户行为分析模型应用场景比较单一，完全针对用户的行为进行研究分析。 另一个基础模型作者：同学刘链接：https://www.zhihu.com/question/20603837/answer/350257155来源：知乎 商业分析，从数据分析的角度来谈，则是通过数据驱动业务，那作为一名数据/商业分析师，很多时候的困扰是面对很多数据、场景，我要从哪个点切入找到价值点呢? 面对新行业时，只要你愿意，可以有无数细节让你发现，没有定义清自己的目的就陷入无数的数据或是场景中，回过头来做的一系列分析或许都不在我们的业务边界里面。当你陷入细节或者数据的噪音，后面所做的事情都只是在噪音中优化了。 所以第一步，先不要这么快陷入细节，站在足够高的视野，看清系统的整个流动。 如果你很有经验，那么或许你很快就能找到问题所在，在面对一个新行业，或是不了解的业务时，我们也可以通过使用这个模型梳理清楚。价值点并不会凭空的出现，通过下面的这个模型，可以快速的帮你理清你所面对的业务流程是怎样的，进而找到解决问题的切入点。 炒鸡蛋例子 把例子转换为通用描述： 很多的业务流程不一定是有反馈机制的，同时如果只关注反馈信息时会让我们忽略很多更容易实现的因素 当你已经基本梳理清楚你所面对的业务流是怎么样的，你可以向这个业务流发问：如果我们没有达到理想的业务目标，那到底是哪个环节出现了问题？咦，此时你不是已经知道该从哪个点进行切入了吗？那接下来就是大家专业表演的时间，用你懂得的数据，你懂得的商业模型来给出改善这个问题的方案/答案。 总结一下： 1 通过模型梳理清楚你所面临的业务流程是怎样的 2 定义清晰的业务边界 3 根据流程，梳理清晰这个流程中的输入和输出分别是什么 4 向你所梳理的这个业务模型发问，它现在还没有达成目标的原因是出于哪个流程 5 聚焦一个或几个你所发现的问题，给出解决方案 三句话总结作者：秦路链接：https://www.zhihu.com/question/20603837/answer/133764469来源：知乎 我将商业分析简单抽象出三层，每一层都有对应的含义：营、盈、赢 。而每一个字的核心思想也就一句话。 营让更多的用户在更长久的时间内以更频繁的次数购买更昂贵的商品。 第一层的分析是商业经营模式的分析，这是商业经营的本质。 任何企业、组织和产品都逃离不出这套框架的四个维度。你可以发展和引申它，但不能减少它。 商业模式的靠谱与否，至少需要满足四个条件中的一个。 如果一种商业即没有用户，又没有长久的消费动力和频次，人们还不肯花钱。这种商业模式还是趁早死了吧。 换言之，如果商业想要发展，就需要往四种方向中的一种或两种靠拢。 更多的用户更多的用户，衡量的是市场体量和大小。 这个商业模式好不好，第一个指标就是用户量或潜在用户量。 堪称商业模式的金线标准。 用户从哪里来，用户到哪里去，用户是谁？ 用户能从微博微信、应用商店、搜索引擎、新媒体等地方来。 用户是谁？用户有潜在用户，有普通用户，有忠诚用户，有流失用户；有竞争对手的用户，有细分市场用户，有核心用户。 用户的质量和数量等同重要，商业希望有更多的用户，这没错，更希望用户是目标用户。 用户到哪里去，就是让用户完成你预定设立的目标。超市最有价值的用户是站在收银台的用户。 甚至以更严格的定义要求自己的用户，而不是注册、安装、打开等简单流程。 更长久的时间更长久的时间，衡量的是用户生命周期（使用产品时间服务）。 在互联网领域，对应用户留存率。 更频繁的次数更频繁的次数，衡量的是用户的消费需求。不同的商业场景下，消费频次肯定不相同。 消费频次分为刚性频次和弹性频次。 一辈子基本只有一次，任你市场营销再牛逼，婚姻类的商业变现也就一次机会。 弹性频次则是相反，由用户自身的意愿决定。快到一天一次，慢到一月一次。购物、出行、社交都是弹性频次。 更昂贵的商品广告就是一种新的商业形态。无论用户点不点击，曝光了，就产生了商业价值。在更昂贵商品的维度中，不同领域有很多相关指标描述其价格或价值，客单价、ARPU、CPC等。 用户量，生命周期，消费频次，都是为这第四个维度所做的铺垫，就是从用户身上掏出钱。 价格可以很低，比如广告点击，比如佣金抽成。哪怕零点几元，也是定义出了价格。有价格就有收入，就能增长，就能改进。 不论价格是什么形式，它都说商业最基础的因素。 互联网行业有一种固定的思维模式，先获得足够的用户量，再培养用户习惯（忠诚度），最后思考商业变现。 很多企业都倒在最后一步，变现乏力甚至无法变现。实际四个维度并不是层层递进，没有因果关系。换一种方式思考，就是产品没有价格这一因素。 举例说明：小区门口有一家老王烧饼铺，生意兴旺。假设你是一位商业分析师，你会怎么分析老王的烧饼铺呢？ 老王的烧饼铺，用户和客流是受地理因素影响的，辐射地就是小区周围，也就是说客源有限。烧饼这种早餐，不管小区爷爷奶奶还是白领，每天早上都可能购买，对吧，满足高频的要求。只要老王的烧饼味道好，小区住户们不会排斥，那么忠诚度能保证。更昂贵的价格？对不起，烧饼真的很难卖贵。 老王拥有的优势是高频和长久，劣势是客源和客单价。再深入想一想，应该怎么解决客源的问题？为了获得更多的客户，开分店可不可行？越多的分店意味着辐射更多的地区，那么客源，以及潜在客源，就上升了。 我们从互联网的角度去思考，哪里有更多的用户？那就是拥有流量入口的几款APP了。用公众号推广？用大众点评吸引用户？加入O2O平台？这些都是方法，靠谱与否就需要深入分析了。 基于四维度框架思考，实际上，很多成熟的商业模式，或者商业工具，都是为四维度服务的。 甚至很多商业模式，会进行维度的转换，牺牲一个维度换取另外一个维度的数据提升。 沃尔玛的薄利多销，牺牲更昂贵的价格，换取用户更高频次以及更长久的消费。 四维度我赋予了一个很重要的特性，它们都是可以量化的，是指标。用户量，价格，生命周期等，都是能作为数据分析，被观察，被对比。 这是四维度的价值所在。 如果你不能衡量它们，就不能增长。 这就是“更”的含义。 在框架上加入商业的数据，以数据的角度去分析，这是衡量分析能力的分水岭。 盈用更高的效率以更低的成本抢占更多的市场并且更好地满足用户需求 这一层不限于经营模式，而是管理模式。 第一层的四个维度，并不能帮助商业模式活下来和活得久。因为企业需要盈利，第一层框架对应的只是GMV, 而不是利润。 第二层框架，就是商业管理框架，我们不仅要经营商业，还要能管理商业、控制商业。 中国老话说得好，第一层用来发江山，第二层是守江山。 TTPPRC以下内容摘自原文作者：@JOIN于宙原文出处：知乎原文链接：https://www.zhihu.com/question/20603837/answer/47073613 Trend(趋势)与其费尽心思解读为什么会有这样的趋势，不如时刻留心趋势的变化，紧随时代的潮流的。 绝大多数商业项目在这一环的关键原则，是不犯大错误。 Traffic(流量)无论你想卖什么东西，先得让人知道你在卖呀。 在TTPPRC中，我把流量（Traffic）简单定义为，让目标客户了解到他们有这么一个消费选择。 而其中最重要的四个字就是，目标客户。 YY平台上的在线语音视频的惊人流量拓展到在线教育上一塌糊涂，显然玩网络游戏和看美女跳艳舞的YY用户，不会摇身一变成坐在电脑前花钱上课的上进青年；人流最大的商场适合优衣库但不适合香奈儿；你永远都不会在电视上看到劳斯莱斯的广告。 流量（Traffic）是否有效，还要具体问题具体分析。 流量（Traffic）不是万能的，它和六步模型中的下一步包装（Package）紧密相连，再优质的流量，如果你的商业项目包装不好也是白费。 强势品牌可以自发的导入流量（Traffic）。 不同的商业项目对流量（Traffic）的具体需求大不相同，一般而言，越是标准化高可替代性高的业态，流量这个环节在整个生意上就越重要。 目标客户是企业而非大众消费者的（B2B和C2B），决定流量（Traffic）的是销售团队的力量。 移动互联网是当下的流量（Traffic）之王。 演艺公司远在互联网时代来临之前就懂得炒作的重要性，只因他们的生意对于大众流量的需求实在是太高了。 99%的互联网创业注定尸骨无存的原因不用太复杂的分析，流量（Traffic）这关根本就过不去。 归根结底，一个商人商业能力的高低的首要标准，就是看他能为自己的生意导入多少流量（Traffic）。 无论是老一辈企业家为了拿下一个单子在酒桌上喝到吐血，或是孟兵拜会潘石屹最终在望京SOHO中为西少爷拿下一个黄金店铺，还是郭敬明为了小时代造势情愿让杨幂发微博拿自己的性取向开玩笑，都是一流的导流能力的体现。 倚靠产品品质过硬口口相传让更多消费者知道有这么个消费选择的古典方法也并未被淘汰，只是单单凭此已经跟不上这个时代的发展速度了。 包装(Package)这里所说的包装（Package）不是包着产品的盒子，而是一个宽泛的概念，它包含了一个商业项目的设计美感，营销推广，品牌故事，定价打折等等一系列非直接产品体验的展示部分。换句话说，在目标客户得知了他们有了这样一个消费选择之后，你的生意是以怎样的面貌呈现在消费者面前的。 擅长“包装”并不是仅仅善于“营销”而已，一个优秀的包装（Package），是高超的管理统筹能力，领先的战略眼光，饥渴的销售团队，一流的综合执行力等素养的综合体现，一切在消费者买单之前对于你的产品产生的印象，都是由包装的质量决定的。 事实上，一个商人对商业的理解程度是否算的上入门，最基本的划分就在于能否意识到：包装（Package）才真正决定了消费者是否会对你的产品进行消费！而不是产品质量的好坏。产品本身决定的是是否有重复性消费和新流量的导入( 口碑)！衡量一个创业者商业能力的高低最重要的标准，除了导入流量之外，最重要的就是打造包装的能力。 一个消费者决定是否对你的产品进行消费，不并是基于产品的实际体验如何（因为他还没用上嘛），而是基于他头脑中对于该次消费的预期体验是否超过了所需要付出的货币价值。这种期待不但是感性的，而且是极其容易被引导的。更具体的说，人们对于短期吸引和实际体验的感受其实完全是两回事，但消费决策主要是由短期吸引决定的，也就是这里所强调的“包装”；而产品本身提供的是实际体验，决定的是是否重复消费和向别人推荐。 包装（Package）影响消费决策，产品（Product）影响二次消费。 产品的溢价主要是由包装决定的，包装最终成就一个品牌，而品牌就是终极的包装。Roseonly的玫瑰花可以卖到999块钱一朵，光靠那个漂亮的盒子和玫瑰花的产地是远远不够的，而是那个“一生只能送一人”的恶俗但有效的包装。 无论是海底捞还是小米Note还是你们学校的女神，他们都深深地明白世界上最厉害的包装，就是排队。而能把排队玩明白的企业家都是顶级的包装高手，无论是真排还是假排。人们喜欢从众的天性是基因决定的。 包装要和流量衔接好，同样品牌的餐厅，开在购物中心和社区里，店面设计甚至菜单内容都是有区别的。同样的电子商务品牌商，淘宝上的店铺包装应该以简洁高效地提高转化率为首要任务，官网上的包装则要提升品牌形象为主。 过度包装一定会在某种程度上伤害到产品的口碑（因为人倾向于过度期待），但未必会伤害到整个生意的整体效果。 产品(Product)商业的本质，是提供产品换取相应的回报。TTPPRC的第四个环节，我们终于谈到了产品（Product）。这并非是因为产品不重要，而是作为一个优秀的商人，强大的商业素养主要体现在可以把自己能力范围内提供的最好的产品和其他环节相互结合起来。而提供技术上绝对领先的产品，超出了一个“商人”的能力范围。 我们从两个方面来具体探讨：1.产品的主要作用是什么？2.一个好的产品应该包括什么？ 上一段谈论包装的环节中曾反复提到，产品（Product）的主要作用是促进二次消费和导入新的流量，而非促成消费决策。 在互联网时代到来之前，消费者在进行消费选择时，面临着高度的信息不对称问题。所以“口碑”一直是企业生死的决定性因素。 从TTPPRC来看，就是优秀的产品体验，促成了二次消费，导入了新的流量，而且自带和“排队”同样重量级的包装——“熟人推荐”。 而互联网时代到来后，产品的口碑传播不仅仅局限于熟人之间而可以遍及全网，很多曾经流量和包装都很局限的生意因为其优秀的产品体验，获得了大量的全网的高质量的流量和包装，彻底改变了企业的命运。总而言之，一个优秀的产品（Product）的威力之大，可以瞬间打通TTPPRC中的四个要素（产品，流量，包装，重复性消费），这是其他任何一个环节做得再好也无法做到的。所以，对于一个打算做长久生意的企业而言，产品质量永远是重中之重。 那么究竟什么才算的上是一个好的产品呢？简单来说其实就两条： 第一要具有一样鲜明的特色，第二其他环节没有硬伤。鲜明的特色可以让消费者留下深刻的印象，而其他环节没有硬伤则是为了不给消费者明显的放弃理由，二者缺一不可。 产品（Product）这个环节上最后一个值得探讨的事情，叫做细节。细节也是反映商人商业素养高低的一个显著标志。为什么必须病态地痴迷细节？因为广大消费者的思维能力虽然比较迟钝，但直觉却是敏锐的。 重复性消费(Revisit)我们从一个简单的例子入手，同样作为消费级的电子产品，笔记本电脑和智能手机的普及度是差不多的。而且电脑的平均价格还要比智能手机高一些。但为什么全世界的商人都在一窝蜂的做手机，而不是笔记本电脑？为什么一款iPhone，可以让苹果公司从一个700亿美金的公司飙升为到7000亿？从TTPPRC的角度来看，Macbook和iPhone在趋势，流量，包装，产品和成本上是大同小异的，而从700亿到7000亿的秘密，就在于这个重性复消费（Revisit）上的天壤之别。一部质量不错的电脑可以用上三到五年，可再好用的手机几乎跑不掉一年一换的命运。 一个商业项目是否具备让消费者有重复性消费（Revisit）的能力，是众多创业者和投资人最常见的盲点，尤其在TTPPRC的其他五个环节都接近完美的时候，即便一些资深的商业大鳄们也会因为一时的冲动而判断错误。 当然必须一提的是，不是所有的商业项目都需要重复性消费（Revisit）。诸如婚庆礼仪，高考培训，旅游景区门票等大多数人一生只消费已一次的商业项目，优秀产品的作用主要是赢得口碑导入新流量（Traffic）和为新流量自动增强包装(Package)。尽管如此，对于绝大多数商业项目而言，重复性消费能力依然直接决定了一个企业的发展方向和生命周期。 既然重复性消费（Revisit）这么重要，那么它是由什么决定的呢？换句话说，作为创业者，如何令你的商业项目拥有强大的重复性消费力呢？关键因素有两点：一是商业项目的天然属性，二是创业者对于该项目的设计。 所谓商业项目的重复性消费（Revisit）的天然属性，是人们长期的行为习惯和一些历史传统所决定的。 在天然属性已经确定的情况下，同样的商业项目是否能获得更高的重复性消费力，就要看创业者对于商业项目具体环节的设计的功力了。TTPPRC之间有很多内在的相互作用力，其中一个有趣的规律就是：重复性消费（Revist）弱的项目往往短期之内流量（Traffic）和包装（Pacakge）更强。 回想起来，Draw Something当年的快速陨落，也许它的重复性消费（Revisit）天然比较弱只是一部分原因，玩过这款游戏的人都知道，作为一款手游，它在流量最高的时候竟然没有一套系统的积分，升级，获得道具，解锁新任务等最基本的成瘾体系。 成本(Cost)任何商业活动的本质，都是为了盈利而服务的，任何项目利润的来源都是建立在所提供的服务价值能够超越自身成本的基础上。 成本（Cost）和趋势（Trend），流量（Traffic），包装（Package）产品（Product）重复性消费（Revisit）的关系不是顺序的，而是包含在每一个环节之中，毫无疑问，每个环节都必然受到成本的制约。 关于优秀一个优秀创业者的核心素养总是包括两点，第一步在于能够将一些模糊的想法，合理拆解成一个个需要解决的具体问题（TTPPRC），第二步就是找到最适合解决这些问题的优秀人才来执行具体的解决方案。 小孩子才吵着要改变世界，成年人应该关心如何执行。 商业分析的另一个框架作者：何明科链接：https://www.zhihu.com/question/20603837/answer/92741713来源：知乎 graph LR id1(商业分析) --> id11(商业理解) id11 --> id111(描点，事实积累) id111 --> id112 id11 --> id112(从点到线，建立关联和对比) id112 --> id113 id11 --> id113(从线到面，理解共性) id113 --> id114 id11 --> id114(超维，理解人性) id114 --> id115 id1 --> id12(事实分析) id12 --> id121(沉淀) id12 --> id122(解释) id12 --> id123(验证) id12 --> id124(探索) id12 --> id125(传播) id11 --算法和工具深化 --> id12 商业分析描点，事实积累无论是人类学习还是机器学习，基础都是大量的样本、案例、数据或者事实，如同在存储系统中描点，记录下一个个的数据点。因此要想理解商业，或者精确点说要想理解一个行业或者公司，任何人一开始都不能避免的事情就是：大量阅读这个行业相关的资料、信息及数据，构建最基础的事实基础。 这里分享两个小窍门：善用逻辑框架Framework。框架的主要作用是不重不漏（Mutually Exclusive and Collectively Exhaustive）得掌握这个行业或者公司的知识，并具有一定的系统性。框架如同拳术或者武功派别，不存在以框架定高低贵贱，核心的还是使用框架的大脑。如果非要推荐，SWOT分析和波特五力竞争模型非常易用且通用，因此内外部沟通也非常方便，理解成本很低。养成每天看数据和报告的习惯，就像习武之人每天练习蹲马步这样的基本功一样。古人所说的“熟读唐诗三百首，不会作诗也会吟”，其实也是强调日常对业内知识的积累和商业感觉的培养。认识的一个某互联网公司超级高管，对于各种业务及财务数据了然于胸，甚至能精确到十万和万级别，和他讨论问题堪称翻阅活字典。除了天赋异禀对数字的敏感之外，也来源每天孜孜不倦得关注各项业务数据。 从点到线，建立关联和对比即使在框架的帮助下，各种掌握的知识和数据仍然会非常零散，全是孤立的点。如果能通过连线将这些知识点联系在一起，战斗指数将会大增。 学习各项知识大概都是如此，比如正在或者曾经让大家烦恼的历史知识，单个事件时间记忆起来确实费劲，这主要还是因为记忆点太孤立。比如太平天国（1851年～1864年）和南北战争（1861年～1865年），单独记忆时间和历史意义肯定很难。但是如果能把两个点连成线，做成对比就会容易很多，同时也让孤立的历史事件更有逻辑性：都大约发生在19世纪的中页；一个是当时正在没落的东方最大国家的最大内战，而另一个是正在崛起的未来西方最大国家的最大内战；两者对于双方未来的命运都有分水岭般的决定作用。 回到商业分析的问题，以用户运营为例。如果能长期观测几个App的留存率，这些数据就不再会枯燥，首先在时间维度会形成价值（比如留存率如何衰减，从次日留存到7日或30日留存），其次在跨品类维度也成为一个个的标杆（比如：工具类、应考类、社交类、内容类等等的留存率范围以及DAU日均新增的比例）。当面对下一个新版本或者全新App的时候，如何评判其用户黏性以及增长潜力，就有一把现成的尺子放在心中了。 从线到面，理解共性绝大多数的商业模式，都可以精炼为非常基本但核心的公式。 对于某生活类垂直网站而言，其商业模式依赖于如下的核心公式： 商业收入 = 用户点击量x广告展现率x广告点击率x平均点击价格 流量成本 = 总流量x外购流量比例x单个流量成本 两者组合而成的公式 ROI = 商业收入 / 流量成本，毛利润=商业收入 - 流量成本 这些公式中的核心点，如同子节点一样，又可以往下繁衍出新的枝节，最后由点到线，由线及面，最后铺成一个盘根错节的树状面。 比如：要降低外购流量比例，可以涉及到提高自身产品的留存率或返客比例，或者提高产品内用户推荐新用户的比例；要提高广告展现率，可以提升广告主的数量以及购买力度；或者降低广告和呈现内容匹配的精度，使用更多的模糊或者粗暴匹配，但这同时又会降低用户体验，然而这确实百度正在一路狂奔的方向。 http://zhuanlan.zhihu.com/hemingke/20535942 某呼叫中心核心的指标是在不降低用户满意度（用接通率来表示）和不减少业务量的前提下，降低整个呼叫中心的运营成本，即人工服务成本（用人工服务总时长来表示）。 核心目标是*人工服务时长 可拆解为：人工平均服务时长 x 人工话务量 人工话务量又可拆解为：每人平均呼叫次数 x 呼叫人工的用户数 呼叫人工的用户数又可拆解为：试图呼叫人工的用户数 x 接通率 如下图所示，通过一系列得从点到线的连接和拆解公式，最终将该呼叫中心的运营拆解成平铺的一张树状图，可以根据各个节点去改进数值以达成总体的目标。 做商业分析不能仅仅局限在内部，如果能走出去，能够跨公司和跨行业获取标杆数据以及行业最佳实践做法，就能让这颗树状图进一步铺开，而蕴含的能量和价值也就会倍增。 关于指标拆解以下内容摘自原文作者：@孙金龙原文出处：知乎原文链接：https://www.zhihu.com/question/20603837/answer/190938230 数据分析不是技术流，是一种思维习惯。能帮你梳理业务，找到方向，达成目的数据分析，才是真的数据分析。 而建立这种思维习惯，你只需要记住3个步骤，掌握3个模型。 三个步骤以上的讨论都是技法，到超维去理解人性就属于心法，这也是最最难的。一旦掌握这个技能，往往能做的就是降维打击和吊打对手。 许多思考透人性的商业模式在一开始，乍一看往往是难以接受甚至反智的，然而这恰恰是这个商业模式在闷声发大财或者增长的时候。 比如某著名的美女交友和聊天软件，当年在市场上大肆买量。当大家得知这款App留存率的时候，都鄙夷得笑了，觉得人傻钱多。可是这款以陌生人社交为皮的App，根本就不是社交软件，而是一款页游类型的App。所以用社交软件的点线面来理解它是完全错误的，以为用户留存低就是垃圾软件。No！No！No！别人是真正抓住了人性——一般的矮矬穷即使在互联网中也很少被美女搭理，别人其实是页游模式。关注的不是留存，而是关注用户进来的数量、转化到首次付费的比例、从付费到大R的比例以及最核心的ROI，即使留存低和用户停留时间长，只要ROI&gt;1，就可以肆无忌惮地购买用户。 因此要想真正做好商业分析和商业理解，要常常从人性角度出发，颠覆怀疑自己的传统认知，按照Elon Musk所说的第一性原理思考商业问题。 这三个步骤是，确定目标、列出公式、确认元素。 假设互联网金融公司A正准备上市，当前核心目标为利润。 针对利润，我们列出公式。上面公式中，利润拆解成了付费用户数、投资金额、投资时长、对应利率四个元素。 其中对应利率取决于资产端，跟用户侧关系不大。 注意，核心目标会随着业务发展不断变化，比如用户运营，App初期看重新增，中期看重转化，后期看重留存。 如果难以确定，就回到核心KPI。 三个模型确认需要提升的元素后， 如何提升每个元素的量级 怎样制定策略，分配资源 如何验证策略是否有效 1 漏斗模型 适用范围：需要多个步骤达成的元素。 比如投资用户数。 达到投资用户的状态，需要多个步骤。每个步骤都存在转化率，放在一起就成了层层缩减的漏斗。 漏斗模型作用：提升量级。 通过提升转化率，提升单个元素量级。 有了漏斗模型，就可以分析每层漏斗衰减的原因。有些原因显而易见，有些需要做A/B测试。你可以逐层提升转化；也可以改变用户路径，减少漏斗层级。 注意，优化漏斗是个长期过程，需要每天关注。 2 多维坐标 适用范围：具有多重属性的单个元素。 比如，本文公式中的投资用户，就有投资金额和投资时长两个属性。可以将其作为横纵坐标轴，把所有投资用户分成四组。 电商品类运营有个经典坐标，按流水和利润划分品类。 用户运营也有个经典坐标，叫RFM坐标。 R = 最近一次行为（Recency） F = 行为频率（Frequency） M = 行为量级（Monetary） 这里的行为指和你的核心目标密切相关的行为。比如在本文的金融产品中，就是投资。 R代表可触达，毕竟6个月没来投资的用户，说不定都卸载了，甚至已经忘了你这个App；F代表忠实度，高频次的使用App，虽然ta可能每次只投几块钱的活期；M代表价值，比如累计投了50万，这可是个高净值用户。 多维坐标作用：精细化运营。 通过多维坐标将用户分组，对不同组用户采取对应的运营措施。 首先，一定有一个象限是好的。 A象限的用户，是核心用户（俗称爸爸），公司的现金牛，你的重点运营对象。 A象限往往占整体流水的80%。你的活动效果好不好，运营策略给不给力，往往要看这些爸爸们的反应。 接下来，你要把B、D两个象限的用户往A象限拉。 D象限，是高潜力用户。可以定向发一些大额度长期标的优惠券，比如投20w，6个月，送3000元红包。提升他们的投资时长。 B象限，是高忠诚用户。虽然可能没什么钱，但使用频次很高。可以定向发送梯度优惠券，比如投资1000送10。 投5000送投资80元，投10000送200元，逐步拉升他们的投资额度 A是现金牛，D是A的孵化器，B用户价值低但忠诚度高，产品开拓新场景后也有可能进化成现金牛。 重要性，A&gt;D&gt;B。 资源有限时，参照此排序。 3 分组表格 适用范围：随时间变化的用户属性元素。 比如投资用户数。 分组表格的原理，是将某一周（或一天，一个月）进入App的新用户，作为单独的一组用户。 上面的表格，就是投资用户分组表格。 横向看，是某组用户的投资用户数，随时间变化的留存情况。 比如第一行，第一周共新增200名投资用户，到第二周留存100名，到第三周留存80名…… 纵向看，是某一周投资用户的构成情况。比如第三列，显示第三周的730个投资用户，是由第一周进入的80个+第二周进入的250个+第三周进入的400个构成的。 作用：监测&amp;验证。 分组表格可以帮你分析清楚一个复杂元素的变化。 事实分析可以参考https://www.zhihu.com/question/23165279/answer/81334982 包含沉淀, 解释, 验证, 探索和传播 沉淀或数据获取找到有创意的数据源及数据沉淀办法，最后解决问题。正所谓“有数据也要做，没有数据创造数据也要做”。 解释在商业实战中，每天都要面对的问题会有：销量或者在线消耗怎么暴涨（或暴跌）了？新上的渠道效果怎么样？用户的ARPU或者人均PV怎么上升（降低）了？数据分析，需要基于数据解释产品或功能的某项核心指标（包括收入、DAU、ROI等等）的走势及背后的原因，往往需要细化到多个维度（比如：时间、区域、渠道等）。基于这些解释，做事后总结或者提前预警，试图保证产品及功能在正确的轨道上发展。 验证商业分析往往是围绕产品或服务进行。而随着技术的发展和竞争的加剧，产品或者服务在按照天或者周的速度在更新和迭代，各种功能及改进都在高频率得上上下下。对新推出的功能或者改进，验证其效果或者影响，使用的方式包括：简单的时间维度或者地理纬度或者渠道纬度的对比，复杂一些的做AB Testing。Facebook在AB Testing方面积累深厚；对于各种UI方案甚至小到文案及颜色，都需要进行AB Testing来选出最优方案。Facebook曾经利用这个系统在某个WiFi段定点发布新功能，来戏弄某杂志，诱骗其发布Facebook有产品的错误消息。数据分析，需要在验证的过程中屏蔽各种噪音来排除对真相的干扰。同时，在数据不充分或者无法实现AB Testing的情况下，找到噪音最小的方式来逼近真相。 探索商业分析需要通过研究内外部的数据（比如：用户的使用行为数据及搜索词等，百度指数及贴吧发言等等），探索规律和探索用户的需求，通过数据的方式进行初步验证；或者满足一定的功能，通过数据挖掘的方式满足功能需求。 在算法上，Deep Learning/CV/Machine Learning等等，数据分析人员不敢说要一一精通，但是最好还是能略小一二到熟练使用各种框架的程度，才能轻松完整上面的各种任务。 传播完成商业分析之后，对内对外的传播也非常重要。前者负责争取资源来推动产品服务及业务改进，后者负责通过新鲜的报告来吸引用户和维系用户。当下最易传播并适合于数据黑客的媒体类型无疑是图文并茂并结合数字的文章（被称之为Infographics）。Infographics凭借其直观易读和理性化的数据呈现方式，越来越受读者的喜欢，并逐渐成为内容制作者青睐的方式。这里有不少精彩的案例（文图），这些案例都非常方便在手机端或者微信里面传播。基于内部丰富的数据并结合外部的数据，制作有见地有传播性的图文文章，并便于在朋友圈及知乎等地传播，为自己产品赢得口碑和品牌增值。 关于商业分析究竟是干什么的以下内容摘自原文作者：@接地气的陈老师原文出处：知乎原文链接：https://www.zhihu.com/question/20603837/answer/285586650 真正工作以后做的数据分析，确切的说，应该是更偏“分析”而不是数据。 大企业的ERP系统已经很普遍，有条件的企业已经有了BI系统，他们所面临的问题，不是没有数据，而是没有结论。到底这些数说明了什么，到底我要怎么做，是最迫切需要回答的问题。 当我自己独立面对客户的时候，我终于明白领导们在纠结什么了：他们在纠结如何不被客户操死，活着把尾款收回来。 因为寻找答案，比寻找数字难的多。 客户花了几十上百万不是来听《管理统计》或者《市场营销》课的，而是结结实实的需要答案。到底数据背后说明了什么问题？到底这个数据能得出什么结论？我早知道这个情况了，你又分析了什么？是最常被提起的三个问题。而且，相信我，没有一本书能告诉你答案。请务必相信我，因为我真！的！试！过！ “我知道销售在这几天少了30%，所以又怎么样？”“模型预计增长30%，所以我只要坐着不动听模型的就对不对？所以我的销售要干什么？”“寻找高端客群，说的容易，我到哪里找？我要找多少？光找高端就够吗？”客户类似的咆哮，如同春节的鞭炮，在我耳边噼里啪啦啪啦霹雳炸的不停。以至于养成了一个习惯：但凡有新人写报告的时候附上：“我们一要提升销售连带率，二要优化产品品类”这种结论，我都会把这些爆竹拉出来再点一次，炸的新人魂飞破散为止。 当我很困惑的时候，我的领导用一种最简单的方式破开了我的困惑。某天在街边吃饭，他指着对面桌某个正在吃饭的小哥，说：“你注意到没有，他是我们某个客户的业务员，你现在告诉我，你可以怎样帮到他做业务”。我愣了半天，完全想不到该做什么。话说，要是我会做销售早就去做了好吧，为什么还要在这做分析写报告啊！我就是没有能力死皮赖脸的求人买东西啊。我只会找自己的熟人啊，介绍产品也含羞带臊啊。而且，这种基层业务员的动作，和数据有什么关系？我们分析的不是销售额，客单价，转化率这种抽象的东西吗？我们的分析和具体的业务离得很远啊。具体到一个个销售动作？有没有一个监控探头24小时拍摄业务员动作，数据都没有，分析个啥？领导说：“这就是问题的关键了。你没有能力帮助基层的业务员，你怎么有能力帮助他的老板？他的老板下边有无数的这种人要管，他要烦的事本质上还得通过这些人搞定。你不能帮助这些人做出业绩，你凭什么认为你能帮助他的老板搞掂业绩问题”。这一刻，我突然明白了为什么数据分析在企业内不被重视。 因为值钱的是数据，不是数据分析。 比如对面的业务员小王，如果你能马上给他500个对我们产品感兴趣的顾客电话，那他怎么着也能把业绩做出来。如果你只是给500个电话，他还得辛辛苦苦打上一天，看有没有机会碰到一个订单。如果你给的只是：“我们的目标客户是年薪30-35岁，年收入2万以上喜欢在高端商城逛街女性”他估计能气到吐血，这他妈都是什么？你是让我去商城门口拦人吗？会被保安痛扁的好吧。如果有优质的数据，比如银行，可以拿到用户真实的个人信息，资产情况，通过银行卡记录用户消费情况，那么即使没有什么复杂的模型，也能轻松判断用户价值和需求。如果像大部分传统零售快消耐用企业那样，只有少量订单数据，就只能做一些不接地气的经营分析。对基层而言，有用的从来都是数据，而不是分析。 决定我职业发展的重大时刻，在这里来到了。我反问了领导一个问题：是滴，我是不会教小王，可是如果我能用数据追踪到一个优秀的业务员，让他去教呢？领导笑笑说：“你小子终于开窍了”。然而这个窍依然开的很艰难。当我真的在这个项目里找到了销售团队Ace以后，发现这他妈的根本复制不了。因为那哥们不是一个人，而是一个“人精”。无论外貌、着装、言语、谈吐、思维速度，根本不是一般人可以比的。而我既不能建议客户把剩下的200个销售都炒了换成这种人精，又不能建议销售们集体再投一次胎。感觉这标杆树的跟数据分析没什么关系，应该归入《投胎学》范畴。 这时候再经过领导提示，我才真正理解了流程梳理的含义。梳理销售流程不是简单的列4个P然后填做填空题，而是真正深入到业务当中。观察每一步细节。到底这个行业销售过程分作几步？最原始的名单从哪里来？工作计划如何安排？拜访客户时间表怎么定？FAB先怎么讲后怎么讲？不同类的客户是否有区别？如何探测需求成熟度？多少天、多少频次、什么理由、什么形式做跟进？最后投标价格如何比拼？一步步，一个个细节去理解销售场景，去观摩业务员行为。去剥茧抽丝，哪些是所有人可以执行的，哪些是个人特性的；哪些是可以量化的，哪些是主观能动的；哪些需要系统工具，哪些需要人工考核。详细的梳理之后，有限收集的数据和无限复杂的业务场景结合起来，数字忽然变得有意义。这个时候开始理解为什么一个行业销售数据会有自己的波动形态；才开始理解为什么在这个行业中销售波动5%不是问题，波动10%就是问题；才开始理解为什么客户看到A类用户75%流失率也不为所动，看到B类用户10%流失率就开始发飙。虽然这样理解以后只能和客户对行业的认识打个平手，但是客户往往没有系统全面的看过数据，只有零散的凭经验的认识。逻辑性+行业理解，可以让顾问赢得客户的认可，至少能和客户平等的探讨问题，而不是被人呼来喝去了。之后就是无数的项目积累，接触的行业越来越多，被操的次数越来越多，能力也成长了很多。到现在我都坚信：好的数据分析师都是被操出来的。被人反复质疑：你这个数据说明了什么？看到数据又能做什么？我早知道了，你又分析了什么？这三大问题以后，就会越来越思考数据分析的价值，就会越来越聚焦于分析的意义，就会抛弃那些刻板的公式与理论，找到真正对企业经营有意义的分析结论。这些才是客户愿意花钱买的东西。“如果只是看数，人家的BI系统早就有数了，为什么要花钱找你们这些人”每次培训，我都会这样给新兵们强调。 But，题目是《一个优秀的商业分析师》，所以到底优秀体现在什么地方呢？在我看来，优秀就是赚钱。如果做科研的话早就读博去了，我们出来打工就是为了赚钱，对不对。赚到钱的才算优秀。而在这一点上，陈老师有一点小小的心得可以分享。就是当我开始和销售一起打标的时候，我得到了第二次分析能力突飞猛进的机会。这个机会叫弄死竞争对手。因为我只是广大乙方公司中很普通的一员。每次客户招标可能要看几份甚至十几份详细的方案。对面也是电脑E盘工作资料文件夹里收藏了几十G ppt的小哥小姐姐，怎么能脱颖而出呢？某次我去打标，做的方案太四平八稳，被客户吐槽：“你们分析都是这些套路，有什么新意思吗？”我很不服气，回来的路上一路吐槽：“净TMD扯淡，我TM就不信他TM能找到更TM厉害的，做市场进入研究不就TM这个套路其他公司还TM能玩出花来”。骂完之后我忽然有个想法：为什么不把这些常见套路直接展示给客户呢？我还就不信了谁还能比谁强多少。某次客户初步和我们沟通意向，只是抛了个很常规的新品研究与上市策略的需求。眼看着又要被人吐槽：老一套。我主动出击了：“通常市场研究公司或者咨询公司都会先看目标群体再看市场份额最后做竞争态势。输出到您面前的一个条形图，您的对手bar大概这么长，您的大概这么长。然而我们早知道了啊。这个领域我们两家独大，且我们落后于竞争对手，这是个基本事实。我们的用户画像，即使没有明确的数据大家心里也八九不离十。与其花大力气重复这个事实，不如提炼出发展新用户的逻辑。这样就能真正找到切入市场的办法”。我边说边在白板上画出图表的形态。我看到客户那边手下小弟很认真的看着我画的然后微微一笑，我看到领导表情很沉重如有所思，我知道这鱼基本上上钩了，他们应该看过类似的报告，而我正在赢得这场游戏。领导听完愣了一下，缓缓的说：“那你的建议呢？”我知道我的机会来了。我坚定的说：“既然明知道处于下风，就不要分散投入。我可以在完成基础画像完成后，快速切入竞争胜利与失败的用户群体，把更多项目费用用在测试竞争效果上，这样后续落地建议将更具体和有效”。客户领导认可的点了下头。当天晚上就发了中标通知。这是我第一次用竞争分析法搞定客户。之后我便更常使用这个方法。当然，之后应用的更灵活，没有那么激进的直接抨击对手（因为发现太过直接的抨击别人，会显得自己太过高调，会引起客户反感）。而是站在客户的角度思考：“到底这样做有什么用”。当我自己把自己当成评标委员，自己怼自己一顿以后，差不多如何打赢竞争对手的思路也差不多有了。当然，赢得客户靠的不止有专业性，还有关系、费用、品牌大小等等，陈老师也并非一路战无不胜。但是这种自我diss，站在需求方角度看问题的思维方式，使得我至少超过了平均水平。即使没有赢下单子，客户至少认可我的个人能力。 最后总结一下，一个优秀的商业分析师是如何炼成的： 第一，摆正位置，从理解如何做生意开始，理解商业问题；第二，理清流程，了解商业过程的完整流程；第三，探索规律，从优秀/差劲案例中总结商业经验；第四，提炼假设，总结出可以定性/定量分析的维度；第五，总结经验，从具体的问题分析中总结出适用于一个行业，一个企业的经验；第六，挑战经验，不断反问自己，除了这些经验以外还有没可能性，还有没有更多办法。 这六个过程不断迭代，最后自己会在梳理问题逻辑，理解行业运作上越来越深入，自然也越来越优秀。本质上，商业分析为的是解决商业问题，商业问题是盈亏利损，不是加减乘除。能理解商业运作本质，具体问题具体分析，才是商业分析师真正该做的事。把《管理统计》和《市场营销》两本书订在一起，只是证明一个人手劲很大而已。与大家共勉。 Reference 如何着手分析一个行业 如何着手商业数据分析 如何着手商业分析 如何练就数据分析思维]]></content>
      <categories>
        <category>business analysis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[regularization]]></title>
    <url>%2F2019%2F04%2F19%2Fregularization%2F</url>
    <content type="text"><![CDATA[Learning with intuition. 每一个概念，被定义就是为了去解决一个实际问题（问Why&amp;What），接着寻找解决问题的方法（问How），这个“方法”在计算机领域被称为“算法”（非常多的人在研究）。我们无法真正衡量到底是提出问题重要，还是解决问题重要，但我们可以从不同的解决问题的角度来思考问题。一方面，重复以加深印象。另一方面，具有多角度的视野，能让我们获得更多的灵感，真正做到链接并健壮自己的知识图谱。 正则化用来改善过拟合过拟合首先了解什么是过拟合。 一般机器学习的目的为了获得泛化误差小的学习器，进而预测新样本。 实现这个目的的路径是尽可能的最小化经验误差，即从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”，这样在遇到新样本时就可以依据这个”普遍规律”作出正确的判断。 当样本量足够大时，经验误差最小化能保证有很好的学习效果。 在现实中被广泛采用。 但是，当样本容量很小时，经验误差最小化学习的效果未必很好。 学习器过度追求最小化经验误差，把训练样本学得“太好了”，就很可能把训练样本自身的一些特点当作所有潜在样本都会具有的一般性质，即学到的并不是真正的“普遍规律”。这样会导致泛化性能下降。 这就是“过拟合”。 直觉考虑为什么会过拟合以下内容摘录自 https://charlesliuyx.github.io/2017/10/03/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E4%BB%80%E4%B9%88%E6%98%AF%E6%AD%A3%E5%88%99%E5%8C%96/ 一般机器学习方法，都依据损失函数L(y_i, h_{\theta}(x^{i}))(代价函数)最小的方法寻找最优的模型。 假如我们使用平方损失函数，则需要求解以下最优化问题： min R_{emp} = \frac{1}{m}L(y_i, h_{\theta}(x^{i})) = \frac{1}{m}\sum_{i=1}^{m}{(y^{i} - h_{\theta}(x^{i}))^2}这里，可以令$h_{\theta}(x^{i}) = w_0x_0 + w_1x_1 + \cdots + w_nx_n$ 为什么$f(x)$可以用多项式模拟？ 泰勒展开式，任何函数都可以用多项式的方式去趋近。 比如下图，会发现，过拟合的情况可能使用了过多的特征。 因此，首先想到的一个方法就是控制n的量，即让n最小化。 再考虑什么是过拟合 过拟合的本质是模型参数矩阵W是一个病态矩阵, 即矩阵W的特征值很大，在特征方向上自由度大, 输入的任何微小改变，都会引起输出的较大改变。 这样的模型参数是不稳定的，因为你不能保证输入数据都是无噪声的，如果你的模型因为噪声而输出千差万别，那这个模型有什么用呢？ 所以正则是什么，正则是限制在输入训练数据之后，防止解参数W剧烈震动，如果剧烈变化很大，那么你最后SGD的模型就是个病态模型。怎么防止震动，就是SRM思想，去控制压制参数，不让其自由发展。 过拟合解决方法： 让n最小化那现在的目标就是让R_{emp}和||w||_{0} 都最小化。 所以可一求解让两者的和都最小化。 如果||w||_{0}很小，但R_{emp}很大，会如下图左一，出现线性拟合高偏差欠拟合的状态，没有意义。 如果||w||_{0}过大，R_{emp}再小也没有用，则会出现过于灵活的非线性拟合高方差过拟合的状态。 正则化项的存在使得权重矩阵不会取值过大，就不会出现过拟合高方差。 它相当于尽量把第一张图右一的所代表的状态尽量往第一张图左一代表的状态那边拉，从而得到图中间”just right”的状态。 这就是为什么||w||_{0}能够防止过拟合。 下面给出更专业的名称，R_{emp} 经验风险，||w||_{0}正则化项，表示模型的复杂度。 R_{emp} + ||w||_{0} 结构风险。 结构风险最小化等价于正则化。 通过风险结构最小化或正则化，可以使得模型对训练以及未来的数据都有较好的预测。 即之前的最优解问题转变为最优化以下问题 min \frac{1}{m}L(y_i, h_{\theta}(x^{i})) + ||w||_{0}L0范数到L1范数不难发现，正则项为L0范数。范数 ||x||_{p} = (\sum_{i=1}^{n}|x_i|^p)^{\frac{1}{p}} 各位计算机界的叔叔、阿姨、伯伯、婶婶，经过不懈的努力，发现0范数比较恶心，很难求，求解的难度是个NP完全问题。 然后很多脑袋瓜子聪明的叔叔、阿姨、伯伯、婶婶就想啊，0范数难求，咱们就求1范数呗，然后就研究出以下结论：一定条件下，可以转换为求1范数的最小值。 因此，关于0和1范数总结为：1范数和0范数可以实现稀疏，1因具有比L0更好的优化求解特性而被广泛应用。 L1范数和L2范数L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的正则项$||w||_2$最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦；所以大家比起1范数，更钟爱2范数。 二次正则项的优势，处处可导，方便计算，限制模型的复杂度。 在多项式模型中，直观理解是每一个不同幂次的x前的系数，0（或很小的值）越多，模型越简单。这就从数学角度解释了，为什么正则化（规则化）可以限制模型的复杂度，进而避免过拟合 一次正则项的优势，$||w||_1$取到最小值的位置，是$||w||_1 = 0$的位置。 意味着从另一种角度来说，使用一次正则项可以降低维度（降低模型复杂度，防止过拟合）二次正则项也做到了这一点，但是一次正则项做的更加彻底,更稀疏。不幸的是，一次正则项有拐点，不是处处可微，给计算带来了难度，很多厉害的论文都是巧妙的使用了一次正则项写出来的，效果十分强大。 正则化的一般形式min \frac{1}{m}L(y_i, h_{\theta}(x^{i})) + \lambda||w||_{2}^{2}其中，$\lambda \geqslant 0$ 是系数，用于权衡经验风险和模型的复杂度。 更一般的，可以写成以下形式 min \frac{1}{m}L(y_i, h_{\theta}(x^{i})) + \lambdaJ(h)其中，$J(h) = \sum_{j=1}^{n}|w_j|^{p}$ 其他概述作者：呃呃链接：https://www.zhihu.com/question/20924039/answer/16625563来源：知乎 正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识(正则化参数等价于对参数引入先验分布)。 约束有引导作用，在优化误差函数的时候倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识(如一般的l-norm先验，表示原问题更可能是比较简单的，这样的优化倾向于产生参数值量级小的解，一般对应于稀疏参数的平滑解)。 同时正则化，解决了逆问题的不适定性。 产生的解是存在，唯一同时也依赖于数据的，噪声对不适定的影响就弱，解就不会过拟合，而且如果先验(正则化)合适，则解就倾向于是符合真解(更不会过拟合了)，即使训练集中彼此间不相关的样本数很少。 作者：Miruku Zhang链接：https://www.zhihu.com/question/20924039/answer/584403078来源：知乎 缩小参数空间，防止过拟合 由于模型选择的问题，比如一个应该2次回归的问题用了3次方程来fit，这个时候参数对数据的噪声过于敏感。 所以不想要在这么大的搜索空间进行搜索，想要把搜索空间约束到更适合该问题的地方，于是加了一个regularizer来约束。 如果这个regularizer加的合适，那么在新的约束下的搜索空间进行搜索，便有可能得到更好的解。 加先验知识 使得求解变得可行（比如样本较少时候OLS的逆矩阵求解） 其他改善过拟合的方法数据增强减缓过拟合的最简单方法就是增加训练集的数量。在机器学习中，我们有时无法增加训练数据的数量，因为寻找标签数据成本很高。 假如我们现在需要处理一些图像，可以通过尝试一些做法来扩充数据集——旋转图像、剪切、缩放、变换等等。 这种方法就是我们所说的数据增强，借助它我们通常能大幅提高模型的准确度，同时也是优化模型预测的常用技巧。 提前停止（Early Stopping）“提前停止”（Early Stopping）是一种交叉验证策略，我们把训练集的一部分用作验证集。当我们看到模型的性能在验证集上开始糟糕时，我们就立马停止训练模型，这就是Early Stopping。 Reference 为什么正则化可以防止过拟合 L0,L1与L2范数 Reference 正则化的各类直观解释]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>overfit</tag>
        <tag>regularization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BAT及各类机器学习面试整理]]></title>
    <url>%2F2019%2F04%2F10%2Fsource_postsbatinterview%2F</url>
    <content type="text"><![CDATA[写在前面：1 本文的内容部分来源于七月在线发布的BAT机器学习面试1000题系列。 部分是自己面试其他公司时被问到的一些其他问题。剩余是自己在平时思考整理的一些内容。 定期复习，定期检查自己对于一些算法的理解。 遇到好的内容，也会不断迭代更新。 2 部分摘抄自其它的博客的内容，都在后面全部跟上了链接。 简要介绍下SVM(进阶-&gt;手推SVM)分类算法。 目标是确定一个分类超平面，从而将不同的数据分隔开。 支持向量机学习方法包括构建由简至繁的模型：线性可分支持向量机、线性支持向量机及非线性支持向量机。当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。 SVM经典博客 支持向量机通俗导论(理解SVM的三层境界) 机器学习之深入理解SVM 在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别【中】欧氏距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点$x = (x_1,\cdots,x_n)$ 和 $y = (y_1,\cdots,y_n)$ 之间的距离为： De_{xy} = \sqrt{(x_1 - y_1)^{2} + (x_1 - y_1)^{2} + \cdots + (x_n -y_n)^{2}} = \sqrt{\sum_{i = 1}^{n}{(x_i - y_i)^{2}}}曼哈顿距离，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标$(x_1, y_1)$的点$P_1$与坐标$(x_2, y_2)$的点$P_2$的曼哈顿距离为: Dm_{xy} = |x_1 - x_2| + |y_1 - y_2|要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。 通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。 此外，欧几里得以及曼哈顿距离都是$L_p$距离的特例。 点$x = (x_1,\cdots,x_n)$ 和 $y = (y_1,\cdots,y_n)$ 的$L_p$距离为 Dl_{xy} = ((x_1 - y_1)^{p} + (x_1 - y_1)^{p} + \cdots + (x_n -y_n)^{p})^{\frac{1}{p}}相关内容 KNN,距离度量 LR 【难】@rickjin：把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。 LR前世今生 机器学习之logistic回归 LR和SVM的联系与区别@朝阳在望，联系： 1 LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）2 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 区别：1 LR是参数模型，SVM是非参数模型。2 从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。3 SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。4 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。5 logic能做的svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。6 logic给出的是概率 GBDT和XGBoost的区别是什么@Xijun LI：XGBoost类似于GBDT的优化版，不论是精度还是效率上都有了提升。与GBDT相比，具体的优点有 机器学习算法与Python实践之LR 1.损失函数是用泰勒展式二项逼近，而不是像GBDT里的就是一阶导数2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性3.节点分裂的方式不同，GBDT是用的基尼系数，XGBoost是经过优化推导后的 相关内容 集成学习内容 LR与线性回归的区别与联系【中】@AntZ: LR工业上一般指Logistic Regression(逻辑回归)而不是Linear Regression(线性回归). LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围, 其目标函数也因此从差平方和函数变为对数损失函数, 以提供最优化所需导数（sigmoid函数是softmax函数的二元特例, 其导数均为函数值的f*(1-f)形式）。请注意, LR往往是解决二元0/1分类问题的, 只是它和线性回归耦合太紧, 不自觉也冠了个回归的名字(马甲无处不在). 若要求多元分类,就要把sigmoid换成大名鼎鼎的softmax了。 决策树, Random Forest, Boosting, Adaboost, GBDT和XGBoost的区别是什么随机森林Random Forest是一个包含多个决策树的分类器。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。 @Xijun LI：xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的 https://xijunlee.github.io/2017/06/03/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/ KNN中的K如何选取KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说： 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合； 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。 K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。 在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。 相关内容 KNN 防止过拟合的方法过拟合的原因是算法的学习能力过强；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。 处理方法有： 早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练。 决策树pre-pruning(cp, maxdepth, minsplit) 数据集扩增：原有数据增加、原有数据加随机噪声、重采样 正则化 交叉验证 特征选择/特征降维 哪些机器学习算法不需要做归一化处理概率模型(比如树模型)不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。 @管博士：我理解归一化和标准化主要是为了使计算更方便, 比如两个变量的量纲不同, 可能一个的数值远大于另一个。 那么他们同时作为变量的时候, 可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确 或者梯度下降法的收敛比较困难。 还有如果需要计算欧式距离的话可能量纲也需要调整。 对于树形结构为什么不需要归一化数值缩放，不影响分裂点位置。 因为第一步都是按照特征进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。 数据归一化（或者标准化，注意归一化和标准化不同）的原因@我愛大泡泡 能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。 相关内容 数据归一化 有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。 有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。 补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。 请简要说说一个完整机器学习项目的流程@寒小阳、龙心尘 1 抽象成数学问题 明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。 2 获取数据 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。数据要有代表性，否则必然会过拟合。对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。 3 特征预处理与特征选择 良好的数据要能够提取出良好的特征才能真正发挥效力。特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。 4 训练模型与调优 直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。 5 模型诊断 如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。 过拟合、欠拟合判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。 过拟合的基本调优思路是增加数据量，降低模型复杂度。 欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。误差分析也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。 6 模型融合 一般来说，模型融合后都能使得效果有一定提升。而且效果很好。 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。 7 上线运行 这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。 机器学习算法大体步骤 1 对于一个问题，用数学语言来描述它； 然后对应一个模型来描述这个问题 2 通过极大似然、最大后验概率或最小化分类误差等等建立模型的代价函数， 问题转化为最优化问题。 3 求这个代价函数的最优化问题的解。 求解大体分以下几种情况： 代价函数存在解析解。 一般方法是对代价函数求导，找到导数为0的点即为最优解。 如果代价函数能简单求导，并且求导后为0的式子存在解析解，就可以直接得到最优参数 如果代价函数很难求导数，例如函数里存在隐含变量或变量间存在相互依赖的情况；或求导后为0的式子得不到解析解，就需要借助迭代算法来一步步找到最优解。 逻辑斯特回归为什么要对特征进行离散化@严林 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。 相关内容 LR特征离散化 什么是熵熵、联合熵、条件熵、相对熵、互信息的定义简单说下有监督学习和无监督学习的区别有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT） 无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL) 协方差和相关性有什么区别相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。 线性分类器与非线性分类器的区别以及优劣如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。 常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归常见的非线性分类器：决策树、RF、GBDT、多层感知机SVM两种都有(看线性核还是高斯核)线性分类器速度快、编程方便，但是可能拟合效果不会很好非线性分类器编程复杂，但是效果拟合能力强 简单说说贝叶斯定理全概率公式：P(A) = \sum_{i = 1}^{n}{P(A|B_i)*P(B_i)} 贝叶斯公式：P(A = a_i|B) = \frac{P(B|A = a_i)*P(A = a_i)}{\sum_{i=1}^{n}{P(B|A = a_i)*P(A = a_i)}} 相关内容 从贝叶斯方法到贝叶斯网络 简要介绍分类决策树决策树呈树型结构。 分类决策树是基于特征对实例进行分类的过程。 它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。 其主要优点是具有可读性，分类速度快。 决策树学习包含3个步骤： 特征选择，决策树的生成和决策树的剪枝。 特征选择决策树特征选择在于选取对训练数据具有分类能力的特征。 通常特征选择的准则是信息增益(ID3), 信息增益比(C4.5), 基尼系数(CART)。 决策树生成ID3算法的核心是在决策树各个结点上应用信息增益准侧选择特征， 递归地构建决策树。 具体方法是：从根结点开始，对结点计算所有特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点； 再对子结点递归地调用以上方法，构建决策树； 直到所有特征信息增益都很小或没有特征选择为止。 最后的得到一个决策树。 C4.5与ID3的算法类似，只是使用信息增益比来选择特征。 决策树剪枝决策树生成算法递归地产生决策树，直到不能再继续下去为止。 这样产生的分类树可能会存在过拟合。 过拟合的原因是由于学习时过多考虑如何提高对训练数据的准确分类，从而构建出过于复杂的分类树。 解决这个问题的方法是考虑决策树的复杂度， 对已生成的树进行简化。 这一简化过程称为剪枝。 决策树的剪枝往往通过极小化损失函数来实现。 剪枝算法为从叶结点开始，向上回缩至父结点。 如果回缩之后，损失函数变小或增大值小于阈值， 则剪枝，该父结点变为新的叶结点。。 KNN中的K如何选取 determining k in knn The trick is that — in general — the lower the k value, the better the performance in the training set. That is to say, the better your model will capture the variability for the set of data it was trained on. You can think of it this way: k = 1 is the most overfit case for all instances. The prediction is based solely on the training sample nearest the sample provided. The trouble is that — even in a low dimensional, intuitive space — this cannot (or rather, does not frequently) generalize well. On larger data sets, it is better to increase the number of neighbors to better represent the shared characteristics of the class being discriminated: some variability is acceptable but it (hopefully) generallly cancels out to best reflect the average properties of the class(es) being identified. In general, there is no magic bullet for this problem. Sometimes, it might be obvious: plot the generalization error as a function of k. If there is an obvious elbow (rapid decrease followed by plateau) that is a good indication of an appropriately selected value for k. It means that there is a value of k “suggested” by the training data: a value that generalizes optimally without undue calculations of the class of nearest neighbors. There is no clear analytical solution, though. Fundamentally, this is a question of how well your training data reflects your testing data and how well your training and testing data reflect the data outside of the collected samples. Let me know if you have additional questions! I’m passionate about data science and happy to refine my answer! For a loose intuition, a low value of k corresponds to “sharp” decision boundaries in the classification space. Higher values of k correspond to “curvier” or, in the limit flat, decision boundaries. My recommendation would to make some synthetic data to get an intuition for the effect of varying k! If you carry on going, you will eventually end up with the CV error beginning to go up again. This is because the larger you make k, the more smoothing takes place, and eventually you will smooth so much that you will get a model that under-fits the data rather than over-fitting it (make k big enough and the output will be constant regardless of the attribute values). I’d extend the plot until the CV error starts to go noticably up again, just to be sure, and then pick the k that minimizes the CV error. The bigger you make k the smoother the decision boundary and the more simple the model, so if computational expense is not an issue, I would go for a larger value of k than a smaller one, if the difference in their CV errors is negligible. If the CV error doesn’t start to rise again, that probably means the attributes are not informative (at least for that distance metric) and giving constant outputs is the best that it can do. 如何确定聚类算法k-means中的k, 有几种方法并简要介绍下 kmeans in r determine k 选择题1 下面哪种不属于数据预处理的方法？ (D)A变量代换 B离散化 C 聚集 D 估计遗漏值 Reference BAT机器学习面试1000题系列jianshu BAT机器学习面试1000题系列CSDN 如何准备机器学习工程师的面试]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模型评价]]></title>
    <url>%2F2019%2F04%2F10%2Fmodelfit%2F</url>
    <content type="text"><![CDATA[没有测量，就没有科学。 —— 门捷列夫 模型评估是模型开发过程不可或缺的一部分。 它有助于发现表达数据的最佳模型(模型与模型的调参)和所选模型将来工作的性能(泛化能力)如何。 为什么要做模型评估想找到最有效的模型。 这里的最有效包含两个层面：一是能尽可能好地表达当前数据，二是尽可能好地表达(预测)未来的数据,即泛化能力强。 两者都需满足。 一般情况下，泛化能力强的基础是模型首先能够尽可能地表达当前数据。 因此，可以说模型评估的目的是找到泛化能力强的模型。 如果泛化能力不够强，则重新对模型进行调整，优化甚至重建。 不断迭代，直到泛化能力达到指定标准。 怎么做模型评估模型评估贯穿于机器学习的两个大的阶段。 不同阶段，评估的方法以及指标并不相同。 如果将机器学习分为两个阶段，原型设计阶段（Prototyping）与应用阶段（Deployed），则原型设计阶段（Prototyping）主要为离线评估，而应用阶段（Deployed）为在线评估(online evaluation)。 离线评估Prototyping阶段使用历史数据训练一个适合解决目标任务的一个或多个机器学习模型，并对模型进行验证（Validation）与离线评估（Offline evaluation），然后通过评估指标选择一个较好的模型并预测该模型未来的工作性能。 其中，Validation用于模型选择与参数调优，Evaluation用于预测该模型未来的工作性能。 离线评估是包含有效性的两个层面的。 一方面，它使用训练集中的数据建模，并借助Validation不断调整直到找到最佳的模型，来表达当前数据。 另一方面，Evaluation通常会使用训练时没有遇到过的测试集来评估模型的未来工作性能是否达到指定标准，即最终判定当前模型是有效的。 过拟合我们真正想要的是泛化误差小的学习器， 即在新样本上能够表现很好的学习器。 但很多情况下，训练集上表现良好的学习器，在面对新样本时，表现并不好。 因此，并不能简单认为对现有样本表达好的模型就可以很好的预测未来的新样本。 即低经验误差并不等于低泛化误差。 相反，两者之间大体是一个先正相关后负相关的关系。 为了达到泛化误差小的学习器，路径是尽可能的最小化经验误差，即从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”， 这样在遇到新样本时就可以依据这个”普遍规律”作出正确的判断。 即泛化误差会随着经验误差的降低而减小，当我们学到的是真正的“普遍规律”的时候。 但当学习器过度追求最小化经验误差，即把训练样本学得“太好了”，就很可能把训练样本自身的一些特点当作所有潜在样本都会具有的一般性质，即学到的并不是真正的“普遍规律”。这样就会导致泛化性能下降。 这就是“过拟合”。 与之相对的是“欠拟合”，即连训练样本的一般性质尚未学好。 导致过拟合的原因是学习器学习能力过于强大，把训练样本不太一般的特性都学到了，而欠拟合通常是由于学习能力低下造成的。 欠拟合比较容易克服，例如在决策树中扩展分枝，神经网络增加训练轮数； 而过拟合则比较麻烦。 过拟合是机器学习面临的关键障碍，各类学习器都必然带有一些针对过拟合的措施。 同时，过拟合是无法彻底避免的。 在线评估Deployed阶段是当模型达到设定的指标值时便将模型上线，投入生产，使用新生成的在线数据来对该模型进行在线评估（Online evaluation）。在线测试不同于离线测试，有着不同的测试方法（最常见的便是A/B testing，它是一种统计假设检验方法）以及评价指标（在线评估时会采用业务指标,如用户点击率等。而离线评估则更多采用评估模型本身拟合能力的指标,如常用的经验误差的方法）。 在线评估使用的验证指标对模型在不断新生的数据集上进行性能跟踪。 当性能开始下降时，说明该模型已经无法拟合当前的数据了，因此需要对模型进行重新训练。即返回到原型涉及阶段。 离线评估既然我们最终的目标是得到一个泛化误差最小的模型。 最理想的方案就是对所有候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。 然而在模型构建时我们无法获得泛化误差，而训练你误差又由于过拟合现象的存在而不适合作为标准。 那么，现实中如何进行模型离线评估呢？ 通过实验测试对学习器的泛化误差进行预测。 即使用一个测试集(testing set) 来测试学习器对新样本的判别能力，然后以测试集上的“测试误差” 作为泛化误差的近似估计。但我们只有一个数据集，既要训练，又要测试。 实现路径是： 从该数据集中产生出训练集和测试集。 具体方法包括划分法，如训练集-验证集二划分校验（Hold-out validation）、交叉校验（Cross-validation)。 这类方法训练集与测试集与原数据集结构相似, 大小不同。 另一种为重采样技术，如Bootstrapping与Jackknife。（此类方法并不一定保证与原数据结构相似。 但可以充分利用现有数据信息，一定程度减少过拟合） 测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、 未在训练过程中使用。 留出法(Hold-out validation)使用这种方法时，通常大的数据集会被随机分成2个子集，1个用于训练，1个用于测试。 或数据量更大时，将训练集拆分成3个子集，1个用于训练，1个用于模型选择和参数优化，1个用于测试。 训练集(Training set)： 用于构建模型 验证集(Validation set)： 用于评估训练阶段所得模型的性能。 它为模型参数优化和选择最优模型提供了测试平台 测试集(Test set)： 用于评估模型未来可能的性能(泛化能力) 这种数据集划分相当于“分层采样(Stratified Sampling)” 单次使用留出法得到的估计结果往往不够稳定可靠(数据集划分的因素)。 在使用时，一般采用若干次随机划分重复实验评估后取平均值(取多次随机的平均消除划分因素影响)作为留出法的评估结果。 例如进行100次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果，而留出法返回的则是这100个结果的平均。 对于选取多少样本量用于训练，没有完美的解决方案，常见做法是2/3 - 4/5用于训练。（训练样本过多则训练接近于使用全量样本，过少则与全量样本相差过大，可能也无法学习出“普遍规律”） 交叉检验(Cross Validation)当仅有有限数量的数据时， 可以使用k折交叉验证(k-fold cross validation)。 使用这种方法时，数据被随机分成k份大小与结构均相似的互斥的子集。 然后，每次用k-1个子集的并集用于训练，余下的那个作为测试集。 进行k次训练。 最终使用k个学习器性能平均预测未来可能的性能。 k最常见的取值是10。 其他常用的还有5，20(实际中，还需要考虑时间，存储开销等其他因素。 如果学习器本身比较复杂，k如果较大则意味着时间和存储的开销都会比较大)。 与留出法相似，将数据集划分为k个子集同样存在多种划分方式，为减小因样本划分不同而引入的差别, k折交叉验证通常要重复p次不同的随机划分，最终的评估结果是这p次交叉验证结果的均值，常见的是10次10折交叉验证。（取多次随机的平均，去掉划分的影响） 在初始数据量足够时，留出法和交叉验证法更常用一些。 自助法我们希望评估的是用D训练出的模型。 由于保留了一部分样本用于测试， 这必然会引入一些因训练样本规模不同而导致的伏击偏差。 留一法虽受训练样本规模的变化影响较小，但是计算复杂度太高。 自助法(Boostraping)就是一个比较好的解决方案。 自助法在数据集较小，难以有效划分训练/测试集时很有用； 此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。 然而，自助法产生的数据集改变了初始数据集的分布(而这恰好是集成学习需要的)，这会引入估计偏差。 离线评估过程中的调优在进行模型评估与选择时，除了要对适用学习算法进行选择，还需要对参数进行设定。 这就是通常所说的“参数调节(Parameter Tuning)”。 参数除了模型参数还有超参数（hyperparameters）。例如logistic回归中的特征系数为模型参数,需要使用多少个特征进行表征，特征的数目这个参数便是该模型的超参数。 调参和算法选择没什么本质区别： 对每种参数配置都训练出模型，热爱后把对应最好模型的参数作为结果。 需要注意的一点是学习算法的很多参数是在实数范围内取值。 现实中常用的做法是对每个参数选定一个范围和变化步长。 这样选定的参数往往不是“最佳”值，但这是在计算开销和性能估计之间进行折中的结果。 在很多强大的学算法中有大量参数需要设定，这将导致极大的调惨工程量。 以至于在不少应用任务中，参数调得好不好往往对最终模型性能有关键性影响。 可以用格搜索（grid search）、随机搜索（random search）以及启发式搜索（smart search）等进行Hyperparameter tuning, 从超参数空间中寻找最优的值。 格搜索(grid search)格搜索便是将超参数的取值范围划分成一个个格子,对每一个格子所对应的值进行评估，选择评估结果最好的格子所对应的超参数值。例如，对于决策树叶子节点个数这一超参数，可以将值划分为这些格子：10, 20, 30, …, 100, … 随机搜索（random search）它是格搜索的变种。相比于搜索整个格空间，随机搜索只对随机采样的那些格进行计算，然后在这中间选择一个最好的。因此随机搜索比格搜索的代价低。 需要注意的是，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上只使用了一部分数据训练模型。 在模型选择完成后，学习算法与参数配置已选定， 应该用全量数据集重新训练模型。 这个模型训练过程中使用了所有样本，才是最终提交的模型。 离线评估常用的评价(性能度量)指标对学习器泛化性能进行评估，不仅要有有效可行的实验估计方法，还需要有衡量泛化能力的评价标准，这就是性能度量(performance measure)。 回归和分类任务分别对应不同的评价指标。 sklearn模型评价 回归任务msemse = \frac{1}{m}\sum_{i=1}^{m}{(y^{(i)} - h_{\theta}(\boldsymbol x^{(i)})^2}rmsermse = \sqrt{\frac{1}{m}\sum_{i=1}^{m}{(y^{(i)} - h_{\theta}(\boldsymbol x^{(i)})^2}}maemae = \frac{1}{m}\sum_{i=1}^{m}{|y^{(i)} - h_{\theta}(\boldsymbol x^{(i)})|}mape(Mean Absoloute Precentage Error)mape = \frac{1}{m}\sum_{i=1}^{m}{|\frac{y^{(i)} - h_{\theta}(\boldsymbol x^{(i)})}{y^{(i)}}|}分类任务 recallrecall = sensitivity = true positive rate(TPR) = \frac{TP}{TP+FN}precisionprecision = \frac{TP}{TP+FP}P-R曲线P-R曲线绘制方法： 纵轴P，横轴R. 点： 根据预测结果，概率从高到低，按顺序逐个把样本作为positive进行预测，每次可计算出当前的R和P 什么时候用P-R Particularly, if true negative is not much valuable to the problem, or negative examples are abundant. Then, PR-curve is typically more appropriate. For example, if the class is highly imbalanced and positive samples are very rare, then use PR-curve. One example may be fraud detection, where non-fraud sample may be 10000 and fraud sample may be below 100.In other cases, ROC curve will be more helpful. 缺点：有交叉时不好比较，更好的方法是计算面积，但这个区域不好估算。 所以设计了一些综合考虑两者的性能度量指标。 如F1 F1R(recall)和P(precision)的调和平均 F1 = \frac{1}{2}(\frac{1}{P}+ \frac{1}{R})ROCROC和PR类似，ROC曲线绘制方法 纵轴FPR，横轴TPR 根据分类器预测结果。 将概率值从高到低排序。 按顺序逐个把从第0个到截止到该样本的所有样本作为positive进行预测。计算出对应的TPR和FPR 将所有的TPR与FPR连接得到ROC曲线。 AUC另一种情形是，我们有多个二分类混淆矩阵，例如进行了多次训练，每次得到一个混淆矩阵；或者执行多分类任务，每两两类别组合都对应一个混淆矩阵；…… 这时，我们希望在n个二分类混淆矩阵上综合考虑R和P。 一种直接的做法是先计算每个混淆矩阵的R和P,再求平均值。 这样就得到“macro-”系的性能度量指标； 还可以先将个混淆矩阵对应元素进行平均，得到TP, FP, FN, TN的平均值，再计算P,R, 这种算法得到的是“micro-”系。 macro-Pmacro-P = \frac{1}{n}\sum_{i=1}^{n}{P_i}macro-Rmacro-R = \frac{1}{n}\sum_{i=1}^{n}{R_i}macro-F1macro-F1 = \frac{1}{2}\frac{1}{\frac{1}{macro-R}+\frac{1}{macro-P}}micro-Pmicro-P = \frac{\sum{FP}}{\sum{FP + TN}}micro-Rmicro-R = \frac{\sum{TP}}{\sum{TP + FN}}micro-F1micro-F1 = \frac{1}{2}\frac{1}{\frac{1}{micro-R}+\frac{1}{micro-P}}Rreference 模型评价与标准 模型评价指标 All models are wrong. But some are useful.]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Random Forest]]></title>
    <url>%2F2019%2F04%2F02%2Frf%2F</url>
    <content type="text"><![CDATA[集成学习(Ensemble Learning)集成学习通过构建并结合多个学习器来完成学习任务。 集成学习的一般结构： 先产生一组“个体学习器”，再用某种策略将它们结合起来。 个体学习器通常由一个现有的学习算法从训练数据产生。 如果个体学习器是“同质”的，即个体学习器由相同的算法生成，则这些个体学习器亦称“基学习器”，相应的学习算法称为“基学习算法”。 如果个体学习器是“异质”的，即个体学习器由不同的学习算法生成，则个体学习器常称为“组件学习器”或直接称为个体学习器。 根据个体学习器的生成方式，目前的集成学习大致可以分为两大类，一类是个体学习器间存在强依赖关系，必须串行生成的序列化方法，代表是Boosting. 另一类是个体学习器间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和Random Forest. BoostingBoosting 是一族可将弱学习器提升为强学习器的算法。 这族算法的工作机制类似： 先从初始训练集训练出一个基学习器， 再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续得到更多的关注。 然后基于调整后的样本分布来训练下一个基学习器。 如此重复进行，直到基学习器数目达到事先指定的值T。 最后将这T个基学习器进行加权结合。 Boosting算法要求基学习器能对特定的数据分布进行学习。 这可通过”重赋权法(re-weighting)”实施，即在训练过程的每一轮，根据样本分布为每个训练样本重新赋予一个权重。 Boosting主要关注降低偏差， 因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。 Bagging与Random Forest欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然“独立”在现实任务中无法做到(因为所有的分类器是为解决同一个问题训练出来的)，但可以设法使基学习器尽可能具有较大的差异。 给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。 这样，由于训练数据不同，我们获得的基学习器有望具有比较大的差异。 然而，为获得好的集成，又希望个体学习器不能太差。 如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。 为了解决这个问题，可以考虑使用互相有交叠的采样子集。 不同于Boosting, Bagging是通过re-sampling实现对不同的数据分布进行学习的。 BaggingBagging(Bootstrap Aggregating)是并行式集成学习方法最著名的代表。 它直接基于Boostraping. 给定包含m个样本的数据集，有放回随机抽样得到含m个样本的采样集(初始训练集中有的样本在采样集里多次出现，有的则从未出现。 初始训练集中约有63.2%的样本出现在采样集中)。 照这样，可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。 这就是Bagging的基本流程。 在对预测输出进行结合时，Bagging对分类任务使用多数表决法，对回归任务使用简单平均法。 若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选一个， 也可以进一步考察学习器投票的置信度来确定最终胜者。 假定基学习器的计算复杂度为$O(m)$, 则Bagging的复杂度大致为$T(O(m) + O(s))$, 考虑到采样与投票/平均过程的复杂度$O(s)$很小， 而T通常是一个不太大的常数，因此， 训练一个Bagging集成与直接使用基学习算法训练一个学习器的复杂度同阶，这说明Bagging是一个很高效的集成学习算法。 另外，与标准Adaboost只适用于二分类任务不同，Bagging能不经修改地用于多分类、回归等任务。 另外，自主采样过程还给Bagging带来了另一个优点： 由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用作验证集来对泛化性能进行“包外估计”(out-of-bag estimate)。 包外样本还有其他用途。 例如当基学习器是决策树时， 可使用包外样本来辅助剪枝， 或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理； 当基学习器是神经网络时， 可使用包外样本来辅助早期停止以减小过拟合风险。 Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。 Random ForestRandom Forest是Bagging的一个扩展变体。 RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。 具体来说， 传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性； 而在RF中，对基决策树的每个结点， 先从该结点的属性集合中随机选择一个包含k个属性的子集，再从这个子集中选择一个最优属性用于划分。 这里的参数k控制了随机性的引入程度： 若k = 0, 则基决策树的构建与传统决策树相同；若k = 1，则随机选择一个树型用于划分； 一般情况下， 推荐$k = log_{2}d$. 随机森林简单，容易实现，计算开销小，令人惊奇的是，在很多现实任务中展现出强大的性能，被誉为“代表集成学习技术水平的方法”。 相比Bagging中基学习器的“多样性”仅通过样本扰动(初始训练集采样)而来不同， 随机森林中的基学习器的多样性还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。 此外，随机森林的收敛性与Bagging相似。 随机森林的起始性能往往相对较差。 这很容易理解，因为通过引入属性扰动， 随机森林中个体学习器的性能往往有所降低。 然而，随着个体学习器数目的增加， 随机森林通常会收敛到更低的泛化误差。 值得一提的是，随机森林的训练效率常常优于Bagging. 因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，即在选择划分属性时要对结点的所有属性进行考察，而随机森林使用“随机型”决策树只需考察一个属性子集。 几个优点 准确率高 有效运行在大数据集上 可以处理高维特征样本，不需要降维 大数据集高维特征也相对稳定 可评估各特征在分类问题上的重要性 生成过程中可以获取到内部生成误差的一种无偏估计 可以处理缺失值 大多情况下不容易过拟合(双random-sampling) 缺点 在数据噪音比较大的情况下会过拟合，过拟合的缺点对于随机森林来说还是较为致命的。 森林中每棵树的生成(并行) 对于每棵树而言，随机有放回地从训练集中抽取$N$个训练样本(Bootstraping)作为该树的训练集。 如果每个样本的特征维度为$M$, 指定一个常数m &lt;&lt; M, 随机从$M$个特征中选取$m$个特征。 每次基于这m个特征生成单棵树。 每颗树尽最大程度生长，没有剪枝 随机森林的错误率相关因素 任意两颗树的相关性： 相关性越大， 错误率越大 每棵树的分类能力： 每颗树的分类能力越强，整个森林的错误率越低 减小$m$, 树的相关性和分类能力会相应降低；增大$m$, 两者也会随之增大。 所以关键问题是如何选择最优的m。 袋外错误率(obb error)上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。 随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。 我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。 而这样的采样特点就允许我们进行oob估计，它的计算方式如下： 对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树） 以简单多数投票作为该样本的分类结果 最后用误分个数占样本总数的比率作为随机森林的oob误分率 oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。 和其他算法的比较v.s. CART两者中的决策树均是二叉树。 CART RF CART在树生成时，从特征集中挑选最优的特征分列 RF在树生成时，会随机从特征集中挑选子特征集，再从该子集中挑选最优特征分裂 CART uses different stopping rules for tree growth, which ususally leads to a much shallower tree RF中每颗子树都完全生长直到叶结点“纯”。 因此RF中的子树可能会比较大 此外，RF相比CART，有两点新增： 新增bagging. 即RF基于Boostraping构建出T个与原数据集相同大小的训练集。然后在每个训练集上构建tree model. 最终将所有树集成森林。 在每颗树生成时，节点分裂仅使用部分特征。 结合策略简单平均加权平均绝对多数投票法加权投票法有趣的是， 虽然分类器估计出的类概率值一般都不太准确， 但基于类概率进行结合往往比直接基于类标记进行结合性能更好。 学习法当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。 Stacking是学习法的典型代表。 把个体学习器称为初级学习器， 用于结合的学习器称为次级学习器或元学习器。 Stacking 先从初始数据集训练出初级学习器， 然后“生成” 一个新数据集用于训练次级学习器。 在这个新数据集中， 初级学习器的输出被当作样例输入特征， 而初始样本的标记仍被当作样例标记。 在训练阶段，次级训练集是利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险比较大； 因此，一般通过交叉验证或留一法这样的方式， 用训练初级学习器未使用的样本来产生次级学习器的训练样本。 以k折交叉验证为例。 初始训练集D被随机划分为k个大小相似的集合，$D_1,D_2,\cdots,D_k$. 令$D_j$和$\bar{D_j}$分别表示第j折的测试集和训练集。 给定T个初级学习算法，初级学习器$h_{t}^{j}$为在$\bar{D_j}$上使用第t个学习算法而得。 对每个样本，$x_i$, 令$z_{it} = h_t(x_i)$ 表示$x_i$在第t个初级学习器上的预测值。 则由$x_i$产生的次级训练集为 ${z_i} = (z_{i1},z_{i2},\cdots,z_{iT})$ 在整个交叉验证结束后，从这T个初级学习器产生的次级训练集是$D^{‘} = {({z_i},y_i)}_{i = 1}^{m}$ 思考1 设计一种能提升KNN性能的集成学习算法 2 MultiBoosting将Adaboost作为Bagging的基学习器， Iterative Bagging 将Bagging作为Adaboost的基学习器。 比较两者的优缺点。 MultiBoosting由于集合了Bagging，Wagging，AdaBoost，可以有效的降低误差和方差，特别是误差。但是训练成本和预测成本都会显著增加。Iterative Bagging相比Bagging会降低误差，但是方差上升。由于Bagging本身就是一种降低方差的算法，所以Iterative Bagging相当于Bagging与单分类器的折中。 3 随机森林为什么比决策树Bagging集成的训练速度更快 随机森林不仅会随机样本，还会在所有样本属性中随机几种出来计算。这样每次生成分类器时都是对部分属性计算最优，速度会比Bagging计算全属性要快。 4 Bagging通常为何难以提升朴素贝叶斯分类器的性能 Bagging主要是降低分类器的方差，而朴素贝叶斯分类器没有方差可以减小。对全训练样本生成的朴素贝叶斯分类器是最优的分类器，不能用随机抽样来提高泛化性能。 5 GradientBoosting是一种常用的Boosting算法，试分析其与AdaBoost的异同。 相同在于：都要生成多个分类器以及每个分类器都有一个权值，最后将所有分类器加权累加起来。 不同在于：AdaBoost通过每个分类器的分类结果，改变样本权重用于生成新的分类器和计算对应权值，但用于训练的样本本身不会改变； GradientBoosting将每个分类器对样本的预测值与真实值的差值传入下一个分类器来生成新的分类器和对应权值(这个差值就是下降方向)，而每个样本的权值不变。 6 试编程实现Bagging，以决策树桩为基学习器。 iris数据集 7 编程实现AdaBoost，以不剪枝决策树为基学习器。 iris数据集 Reference 随机森林主页 Python集成学习 随机森林通俗教程 资源: 各类资料 RF/CART比较 周志华ensemble learning相关内容 Stacking算法说明 Stacking技术分享 模型融合效果]]></content>
      <categories>
        <category>machine learning</category>
        <category>ensemble learning</category>
      </categories>
      <tags>
        <tag>boosting</tag>
        <tag>bagging</tag>
        <tag>random forest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tree Models]]></title>
    <url>%2F2019%2F03%2F29%2Fc50%2F</url>
    <content type="text"><![CDATA[信息是用来消除随机不确定性的东西。 —— 香农 本篇主要包含ID3,C4.5在示例数据集上的手动实现以及C5.0的R实现。 决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵(或其他目标)为度量，构造一棵熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶节点中的实例都属于同一类。 决策树各类算法简介ID3ID3由Ross Quinlan在1986年提出。ID3决策树可以有多个分支，但是不能处理特征值为连续的情况。决策树是一种贪心算法，每次选取的分割数据的特征都是当前的最佳选择，并不关心是否达到最优。在ID3中，每次根据“最大信息熵增益”选取当前最佳的特征来分割数据，并按照该特征的所有取值来切分，也就是说如果一个特征有4种取值，数据将被切分4份，一旦按某特征切分后，该特征在之后的算法执行中，将不再起作用，所以有观点认为这种切分方式过于迅速。ID3算法十分简单，核心是根据“最大信息熵增益”原则(沿信息熵下降最快的方向)选择划分当前数据集的最好特征，信息熵是信息论里面的概念，是信息的度量方式，不确定度越大或者说越混乱，熵就越大。在建立决策树的过程中，根据特征属性划分数据，使得原本“混乱”的数据的熵(混乱度)减少，按照不同特征划分数据熵减少的程度会不一样。在ID3中选择熵减少程度最大的特征来划分数据（贪心），也就是“最大信息熵增益”原则。 C4.5C4.5是Ross Quinlan在1993年在ID3的基础上改进而提出的。.ID3采用的信息增益度量存在一个缺点，它一般会优先选择有较多属性值的Feature,因为属性值多的Feature会有相对较大的信息增益(信息增益反映的给定一个条件以后不确定性减少的程度,必然是分得越细的数据集确定性更高,也就是条件熵越小,信息增益越大)。为了避免这个不足C4.5中是用信息增益比率(gain ratio)来作为选择分支的准则。信息增益比率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的Feature。除此之外，C4.5还弥补了ID3中不能处理特征属性值连续的问题。但是，对连续属性值需要扫描排序，会使C4.5性能下降。 CARTCART（Classification and Regression tree）分类回归树由L.Breiman,J.Friedman,R.Olshen和C.Stone于1984年提出。ID3中根据属性值分割数据，之后该特征不会再起作用，这种快速切割的方式会影响算法的准确率。CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。下图显示信息熵增益的一半，Gini指数，分类误差率三种评价指标非常接近。回归时使用均方差作为loss function。 决策树生成计算过程数据集 ID 年龄 有工作 有自己的房子 信贷情况 类别 1 青年 否 否 一般 否 2 青年 否 否 好 否 3 青年 是 否 好 是 4 青年 是 是 一般 是 5 青年 否 否 一般 否 6 中年 否 否 一般 否 7 中年 否 否 好 否 8 中年 是 是 好 是 9 中年 否 是 非常好 是 10 中年 否 是 非常好 是 11 老年 否 是 非常好 是 12 老年 否 是 好 是 13 老年 是 否 好 是 14 老年 是 否 非常好 是 15 老年 否 否 一般 否 ID3InfoGain g(D|A)ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体方法是：从根结点开始，对结点计算所有特征的信息增益，选择信息增益最大的特征作为结点特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。 ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。后续包含决策树的剪枝。决策树的剪枝往往通过最小化决策树整体的损失函数实现。 分别以$A_1, A_2, A_3, A_4$表示年龄、有工作、有自己的房子和信贷情况4个特征，则 (1) H(D) = -\frac{9}{15}\times log_2{\frac{9}{15}} -\frac{6}{15}\times log_2{\frac{6}{15}} = 0.971(2) H(D,A_1) = \frac{5}{15}\times (-\frac{3}{5}\times log_2{\frac{3}{5}} -\frac{2}{5}\times log_2{\frac{2}{5}}) + \\ \frac{5}{15}\times (-\frac{3}{5}\times log_2{\frac{3}{5}} -\frac{2}{5}\times log_2{\frac{2}{5}}) + \\ \frac{5}{15}\times (-\frac{1}{5}\times log_2{\frac{1}{5}} -\frac{4}{5}\times log_2{\frac{4}{5}}) = 0.888(3) H(D,A_2) = \frac{5}{15}\times 0 + \frac{10}{15}\times (-\frac{6}{10}\times log_2{\frac{6}{10}} - \frac{4}{10}\times log_2{\frac{4}{10}}) = 0.647(4) H(D,A_3) = \frac{6}{15}\times 0 + \frac{9}{15}\times (-\frac{3}{9}\times log_2{\frac{3}{9}} - \frac{6}{9}\times log_2{\frac{6}{9}}) = 0.551(5) H(D,A_4) = \frac{5}{15}\times (-\frac{1}{5}log_2{\frac{1}{5}} -\frac{4}{5}log_2{\frac{4}{5}}) + \\ \frac{6}{15}\times (-\frac{4}{6}log_2{\frac{4}{6}} - \frac{2}{6}log_2{\frac{2}{6}}) + \\ \frac{4}{15}\times 0 = 0.608由于$H(D,A_3)=0.551$最小，因此，首先选择$A_3$作为最优特征，生成树的第一个分枝。 try Variance like ANOVA(1) V(D) = 15\times \frac{9}{15}\times \frac{6}{19} = 3.6$V(D,A)$为基于特征A划分后的组内方差之和；$g_{v}(D,A)$为组间方差。选择组间方差最大的特征 (2) g_{V}(D,A_1) = 5\times \frac{3}{5}\times \frac{2}{5} + 5\times \frac{3}{5}\times \frac{2}{5} + 5\times \frac{1}{5}\times \frac{4}{5} = 3.2(3) g_{V}(D,A_2) = 5\times 0 + 10\times \frac{4}{10}\times \frac{6}{10} = 2.4(4) g_{V}(D,A_3) = 6\times 0 + 9\times \frac{3}{9}\times \frac{6}{9} = 2(5) g_{V}(D,A_4) = 5\times \frac{1}{5}\times \frac{4}{5} + 6\times \frac{2}{6}\times \frac{4}{6} + 4\times 0 = 2.1$g_{V}(D,A_3)=2$最小，所以选择$A_3$为最优特征进行分割。 C4.5: Info Gain Ratio计算信息增益比 g_{R}(D,A) = \frac{g(D,A)}{H(D)}CART: GiniCART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分枝是取值为“是”的分枝，右分枝是取值为“否”的分枝。这样的决策树相当于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并且在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。 已1，2，3表示年龄为青年，中年，老年；以1，2表示是有工作和有自己的房子的值为是和否；以1，2，3表示信贷情况的值为非常好、好和一般。 (1) Gini(D, A_{1} = 1) = \frac{5}{15}\times (1 - (\frac{2}{5})^2 - (\frac{3}{5})^2) + \\ \frac{10}{15}\times (1 - (\frac{3}{10})^2 - (\frac{7}{10})^2) = 0.44(2) Gini(D, A_{2} = 1) = Gini(D, A_{2} = 2) = 0.32(3) Gini(D, A_{3} = 1) = Gini(D, A_{3} = 2) = 0.27(4) Gini(D, A_{4} = 1) = 0.36(5) Gini(D, A_{4} = 1) = 0.47(6) Gini(D, A_{4} = 3) = 0.32由于$Gini(D, A_3 = 1) = 0.27$最小，所以选择特征$A_3$为最优特征，$A_3 = 1$为其最优分割点。于是，根结点生成两个子结点，一个是叶结点。对另一个结点继续使用以上方法在$A_1,A_2,A_3,A_4$中选择最优的特征及其最优切分点。 R实现： rpartR中决策树的包: rpart, party, RWeka, ipred, randomForest, gbm, C50. rpart:CART/ID3. The default criterion, which is maximized in each split is the Gini. rpart常用参数 参数 说明 formula y ∼ x1 + x2 + ⋯ + xn data 数据集 na.action 去掉 y 缺失，保留自变量缺失 method class:分类变量，anova:回归树 parms 只适用分类树 parms = list(split,prior,loss) split = c(&quot;info&quot;,&quot;gini&quot;) controls 控制决策树形状大小 minsplit,maxdepth,cp rpart参数详解 rpart.plot常用参数 参数 说明 tree tree模型 type 节点形状1,2,3,4 branch 若=1，则为垂直决策树 cex 符号大小 controls参数详解 12library(rpart)library(rpart.plot) ## Warning: package &#39;rpart.plot&#39; was built under R version 3.5.3 12library(gmodels)library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 123456789101112131415library(magrittr)data("iris")# seperate the dataset into two parts: the training and test datasetspicker &lt;- sample(150, 110)iris_train = iris[picker,]iris_test = iris[-picker,]# fit the tree model: classification tree# method: switched according to the type of the response variable. # class for categorial, anova for numerical, poisson for count data and exp for survival data.treecart &lt;- rpart(Species ~., data = iris_train, method = "class")# plot the treerpart.plot(treecart) 123456# cross-check its validity by pitching it against our test datapredictions &lt;- predict(treecart, iris_test, type = "class")# confusion matrix: overfitting# CrossTable(predictions,iris_test$Species, prop.r = FALSE,prop.c = FALSE, chisq = FALSE, prop.chisq = FALSE, prop.t = TRUE)confusionMatrix(predictions,iris_test$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 13 1 ## virginica 0 1 10 ## ## Overall Statistics ## ## Accuracy : 0.95 ## 95% CI : (0.8308, 0.9939) ## No Information Rate : 0.375 ## P-Value [Acc &gt; NIR] : 2.044e-14 ## ## Kappa : 0.9244 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.000 0.9286 0.9091 ## Specificity 1.000 0.9615 0.9655 ## Pos Pred Value 1.000 0.9286 0.9091 ## Neg Pred Value 1.000 0.9615 0.9655 ## Prevalence 0.375 0.3500 0.2750 ## Detection Rate 0.375 0.3250 0.2500 ## Detection Prevalence 0.375 0.3500 0.2750 ## Balanced Accuracy 1.000 0.9451 0.9373 12345678# pruning treecart_ms5 &lt;- rpart(Species ~., data = iris_train, method = "class", control = rpart.control(minsplit = 5))treecart_ms10 &lt;- rpart(Species ~., data = iris_train, method = "class", control = rpart.control(minsplit = 10))# rpart.plotpar(mfrow = c(1,2))rpart.plot(treecart, main = "tree_with_parms", branch = 1, cex = 0.8)rpart.plot(treecart_ms5, main = "minsplit=5") C5012345library(C50)# C5.0是一个boosting算法 trials 控制循环次数# build modeltreec50 &lt;- C5.0(Species ~ ., data = iris_train)summary(treec50) ## ## Call: ## C5.0.formula(formula = Species ~ ., data = iris_train) ## ## ## C5.0 [Release 2.07 GPL Edition] Fri Mar 29 10:47:45 2019 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 110 cases (5 attributes) from undefined.data ## ## Decision tree: ## ## Petal.Width &lt;= 0.4: setosa (35) ## Petal.Width &gt; 0.4: ## :...Petal.Width &gt; 1.7: virginica (35) ## Petal.Width &lt;= 1.7: ## :...Petal.Length &lt;= 5: versicolor (36/1) ## Petal.Length &gt; 5: virginica (4/1) ## ## ## Evaluation on training data (110 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 4 2( 1.8%) &lt;&lt; ## ## ## (a) (b) (c) &lt;-classified as ## ---- ---- ---- ## 35 (a): class setosa ## 35 1 (b): class versicolor ## 1 38 (c): class virginica ## ## ## Attribute usage: ## ## 100.00% Petal.Width ## 36.36% Petal.Length ## ## ## Time: 0.0 secs 1plot(treec50) 12# make predictionstable(predict(treec50, newdata = iris_test), iris_test$Species) ## ## setosa versicolor virginica ## setosa 13 0 0 ## versicolor 2 13 1 ## virginica 0 1 10 ID3, C4.5,C5.0,CART优缺点ID3C4.5C5.0CART优点 缺点 1 不能处理连续特征 2 倾向于选择属性值较多的特征进行分枝，可能导致庞大的树。 解决方法特征属性合并，减少特征属性值 3 对噪声敏感 4 可能过拟合 解决方法将敏感的缺点转化为优点，使用Random ForestID3换C5.0 5 不能处理缺失值 优点 1 能处理连续特征 2 能处理缺失值 缺点 1 产生的树可能不够稳定 解决方法将不稳定的缺点转化为优点，使用Random Forest C5.0是对C4.5的改进。 优点 1 主要增加了对boosting的支持 2 比C4.5构造的树更简单 3 内存更小 缺点 分类问题各类情况处理类不平衡分类问题https://www.r-bloggers.com/handling-class-imbalance-with-r-and-caret-an-introduction/ http://bourneli.github.io/[object Object]2018/11/06/handling-imbalance-with-R.html Reference Classification with Decision Trees C4.5,ID3,C5.0,CART比较 C4.5,ID3,C5.0,CART user c5.0 决策树对不同分类分布的拟合情况 R实现决策树 CRAN: C5.0 sklearn 决策树]]></content>
      <categories>
        <category>machine learning</category>
        <category>tree</category>
      </categories>
      <tags>
        <tag>tree</tag>
        <tag>C50</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adaboost]]></title>
    <url>%2F2019%2F03%2F29%2Fadaboost%2F</url>
    <content type="text"><![CDATA[本篇包含adaboost的手动实现例子以及adaboost在R上的实现例子。 Adaboost简介几个特点Adaboost是一种比较有特点的算法，可以总结如下： 1 每次迭代改变的是样本的分布，而不是重复采样（reweighting not resampling) 2 样本分布的改变取决于样本是否被正确分类 总是分类正确的样本权值低 总是分类错误的样本权值高（通常是边界附近的样本） 3 最终的结果是弱分类器的加权组合。 权值表示该弱分类器的性能 几个优点: 简单，有效1 adaboost是一种有很高精度的分类器 2 可以使用各种方法构建子分类器，adaboost算法提供的是框架 3 当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单 4 简单，不用做特征筛选 5 不用担心overfitting 实例计算过程 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 第1个分类器$G_1$样本权重$w_1$首先给$x_1,x_2,\cdots,x_{10}$ 相同的权重，为$\frac{1}{10}$ 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $w_1$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ 分类结果 $G_1$找到阈值$v = 2.5$, 使该分类器对上述数据分类误差最低。 分类规则 12345def g(x): if(x &gt; 2.5): return -1 elif(x &lt; 2.5): return 1 分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 $w_1$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ 误差$e_{1}$误差实际就是错分案例的权重之和。 $e_{1} = \sum_{i=1}^{10}{w_{1i}I(G_{1}(x_{i}) \neq y_{i}}) = \frac{3}{10} = 0.3$ $G_1$在最终分类器中的权重$\alpha_{1}$$\alpha_{1} = \frac{1}{2}log\frac{1-e_1}{e_1} = \ \frac{1}{2}log\frac{1-0.3}{0.3} = 0.4236$ 可见，被误分类样本的权值之和影响误差率，误差率影响基本分类器在最终分类器中所占的权重。 最终分类器$f_1$$f_1(x) = sign(\alpha_{1}G_{1}(x)) = sign(0.4236G_1(x))$ 最终分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $f_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 分类器$f_1(x)$ 分类结果： 样本7,8,9最终被分错。 第2个分类器$G_2$基于$G_1$分类误差调整所有样本权重$w_2$$w_{2i} = \frac{w_{1i}}{Z_1}e^{-\alpha_{1}*G_1(x_i)y_i}$ 其中，$Z_{1} = \sum_{i = 1}^{10}w_{1i}e^{-\alpha_{1}*G_1(x_i)y_i}$ 由权重公式可知，每个样本的权重在下一轮是变大还是变小取决于该样本在上一轮分类中分类正确或错误。如果正确，则$-\alpha_{1}G_1(x_i)y_i &lt; 0$,即$e^{-\alpha_{1}G_1(x_i)y_i} &lt; 1$, 权重将会减小；如果错误, 则$-\alpha_{1}G_1(x_i)y_i &gt; 0$,即$e^{-\alpha_{1}G_1(x_i)y_i} &gt; 1$ 权重会增大。 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 $w_1$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $w_2$ 0.0715 0.0715 0.0715 0.0715 0.0715 0.0715 0.1666 0.1666 0.1666 0.0715 由此可以看出，因为样本7,8,9被$G_1(x)$分错了，所以它们的权值由之前的0.1增大到0.1666，反之，其它数据皆被分正确，所以它们的权值皆由之前的0.1减小到0.0715。 基于新样本权重$w_2$得到新分类器$G_2$找到阈值$v = 8.5$, 使得该分类器对上述数据分类误差最低。 分类规则 12345def g(x): if(x &gt; 8.5): return -1 elif(x &lt; 8.5): return 1 分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_2(x)$ 1 1 1 1 1 1 1 1 1 -1 $w_2$ 0.0715 0.0715 0.0715 0.0715 0.0715 0.0715 0.1666 0.1666 0.1666 0.0715 计算误差$e_{2}$$e_{2} = \sum_{i=1}^{10}{w_{2i}I(G_{2}x_{i} \neq y_{i})} = 3*0.0715 = 0.2145$ 根据$e_{2}$计算$G_2$在最终分类器中的权重$\alpha_{2} = \frac{1}{2}log\frac{1-e_2}{e_2} = \frac{1}{2}log\frac{1-0.2145}{0.2145} = 0.649$ 最终分类器$f_2$$f_{2}(x) = sign(\alpha_{1}G_{1}(x) + \alpha_{2}G_{2}(x)) \ = sign(0.4236G_1(x) + 0.649G_2(x))$ 最终分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $f_2(x)$ 1 1 1 1 1 1 1 1 1 -1 $G_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 $G_2(x)$ 1 1 1 1 1 1 1 1 1 -1 分类器$f_2(x)$分类结果： 样本4,5,6最终被分错。 第3个分类器$G_3$基于$G_2$分类误差调整所有样本权重$w_3$$w_{3i} = \frac{w_{2i}}{Z_2}e^{-\alpha_{2}*G_2(x_i)y_i}$ 其中，$Z_{2} = \sum_{i = 1}^{10}w_{2i}e^{-\alpha_{2}*G_2(x_i)y_i}$ 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_2(x)$ 1 1 1 1 1 1 1 1 1 -1 $w_1$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $w_2$ 0.0715 0.0715 0.0715 0.0715 0.0715 0.0715 0.1666 0.1666 0.1666 0.0715 $w_3$ 0.0455 0.0455 0.0455 0.1666 0.1666 0.1666 0.1060 0.1060 0.1060 0.0455 同理，由于上一轮结束后，样本4,5,6被$G_2(x)$分错了，所以它们的权值由之前的0.0715增大到.1666，反之，其它数据皆被分正确，所以它们的权值皆由之前的0.0715减小到0.0455。 基于新样本权重$w_3$得到新分类器$G_3$找到阈值$v = 5.5$, 使得该分类器对上述数据分类误差最低。 分类规则 12345def g(x): if(x &gt; 5.5): return 1 elif(x &lt; 5.5): return -1 分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_3(x)$ -1 -1 -1 -1 -1 -1 1 1 1 1 $w_3$ 0.0455 0.0455 0.0455 0.1666 0.1666 0.1666 0.1060 0.1060 0.1060 0.0455 计算误差$e_{3}$$e_{3} = \sum_{i=1}^{10}{w_{2i}I(G_{2}x_{i} \neq y_{i})} = 4*0.0455 = 0.182$ 根据$e_{3}$计算$G_3$在最终分类器$f_3$中的权重$\alpha_{3} = \frac{1}{2}log\frac{1-e_3}{e_3} = \frac{1}{2}log\frac{1-0.182}{0.182} = 0.7514$ 最终分类器$f_3$$f_{3}(x) = sign(\alpha_{1}G_{1}(x) + \alpha_{2}G_{2}(x) + \alpha_{3}G_{3}(x)) \\= sign(0.4236G_1(x) + 0.649G_2(x) + 0.7514G_3(x))$ 最终分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $f_3(x)$ 1 1 1 -1 -1 -1 1 1 1 -1 $G_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 $G_2(x)$ 1 1 1 1 1 1 1 1 1 -1 $G_3(x)$ -1 -1 -1 -1 -1 -1 1 1 1 1 分类器$f_3(x)$ 分类结果全部正确，没有错分情况。 至此，整个训练过程结束。 总结现在来总结一下整个训练过程, 包含各样本权重，分类器误差率的变化： 1 训练之前，各个样本的权重被初始化为 w_1 = (0.1, 0.1,0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)2 第一轮分类后，样本7,8,9被分错，对应误差率为$e_1 = 0.3$，此第一个基本分类器在最终分类器中所占的权重为$\alpha_{1} = 0.4236$。 同时，用于下一轮迭代的样本新权重为 w2 = (0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.1666, 0.1666, 0.1666, 0.0715)3 第二轮迭代后，样本3,4,5被分错，对应误差率为$e_2 = 0.2145$，此第二个基本分类器在最终分类器中所占的权重为$\alpha_{2} = 0.649$。 同时，用于下一轮迭代的样本新权值为 w_3 = (0.0455, 0.0455, 0.0455, 0.1666, 0.1666, 0.01666, 0.1060, 0.1060, 0.1060, 0.0455)4 第三轮迭代中，样本样本1,2,3,10被分错，对应误差率为$e_3 = 0.1820$，此第三个基本分类器在最终分类器中所占的权重为$\alpha_{3} = 0.7514$。 从上述过程中可以发现，如果某些个样本被分错，它们在下一轮迭代中的权值将被增大，同时，其它被分对的样本在下一轮迭代中的权值将被减小。就这样，分错样本权值增大，分对样本权值变小。 最终分类器: $f(x) = sign(f_3(x)) = sign(\alpha_1G_1(x) + \alpha_2G_2(x) + \alpha_3G_3(x)) \\= 0.4236G_1(x) + 0.6490G_2(x)+0.7514G_3(x)$ Reference Adaboost算法原理与推导 Adaboost简明解释 通俗理解Gradient Boost和Adaboost Bagging and Boosting]]></content>
      <categories>
        <category>machine learning</category>
        <category>ensemble learning</category>
      </categories>
      <tags>
        <tag>adaboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xlwings]]></title>
    <url>%2F2019%2F03%2F22%2Fxlwings%2F</url>
    <content type="text"><![CDATA[python加持excel, 速度起飞。 背景开始之前, 回忆一下使用Excel的场景。 同时打开多个Excel文件，多个workbook，每个 workbook 又可以用多个 sheet；且需要在多个 sheet workbook Excel 窗口之间切换 对一个或多个单元格进行增删改查，设置格式，合并分拆等操作 以上场景，需要不停地来回切换，且不同的对象可能需要重复的却又不同的操作，手动工作量大。 xlwings 用于解决以上各种切换和操作，提高效率。 使用说明基于BSD-licensed的Python第三方模块，可以很方便的和Excel交互，它有以下优点： 语法接近 VBA 可以用Python代码取代VBA编写宏 windows中可以用Python写Excel用户自定义函数 全功能支持Numpy,Pandas,matplotlib等库 示例如何执行xlwings编写的自定义函数 将xxxx.xlsx文件另存为 hong.xlsm 文件 确保hong.py 与 hong.xlsm是在同一个文件夹下 确保hong.xlsm文件的VBA是引用xlwings的，如下图 导入自定义函数，点击“xlwings”栏目的“Import Functions”按钮，如下图 可以像调用系统自带函数一样，使用自定义开发的函数，如下图：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Plot in R]]></title>
    <url>%2F2019%2F03%2F14%2Fplotinr%2F</url>
    <content type="text"><![CDATA[本篇内容主要包含常用的可视化图的R实现代码及效果。 Histogram(直方图)base:hist()12345678910library(DMwR)library(car)library(magrittr)df &lt;- algaepar(mfrow = c(1,3))hist(df$mxPH,prob = T)hist(df$mxPH,prob = T,xlab = '',main = 'Histogram of mxPH',ylim = 0:1)density(df$mxPH, na.rm = TRUE) %&gt;% lines()jitter(df$mxPH) %&gt;% rug()qqPlot(df$mxPH, main = 'Q-Q plot of mxPH',ylab = 'mxPH') ## [1] 56 57 1par(mfrow = c(1,1)) 12345678library(lattice)library(gridExtra)hisc1 &lt;- histogram(~ mxPH | season, data = df)# multiple factorshisc2 &lt;- histogram(~ mxPH | season * speed, data = df)# stripplot waysstri1 &lt;- stripplot(size ~ mxPH | speed, data = df, jitter = TRUE)grid.arrange(hisc1,hisc2,stri1,nrow = 3) 箱线图(boxplot)lattice: bwplot()123456library(ggplot2)library(gridExtra)# condition boxplot with latticebx1 &lt;- bwplot(size ~ a1, data = df, ylab = "River Size", xlab = "Algae a1")bx2 &lt;- ggplot(aes(x = size, y = a1),data = df) + geom_boxplot() + coord_flip()grid.arrange(bx1, bx2, ncol = 2) Hmisc:bwplot()1234567891011121314151617181920library(Hmisc)# point: mean# vertical lines: quantiles# 下图结论：小型河流更高频率海藻且分布更加分散bxq1 &lt;- bwplot(size ~ a1, data = df, panel = panel.bpplot, probs = seq(0.01,0.49, by = 0.01), datadensity = TRUE, ylab = "size", xlab = "a1")# 多个因子，因子为分类变量bxq2 &lt;- bwplot(season ~ a1 | size, data = df, panel = panel.bpplot, probs = seq(0.01,0.49, by = 0.01), datadensity = TRUE)bxq3 &lt;- bwplot(season ~ a1 | size, data = df)# 多个因子，因子中包含连续变量 将其离散化后即可## 连续变量离散化mno2i &lt;- equal.count(na.omit(df$mnO2), number = 4, overlap = 1/5)## 类似散点图，从左到右从下到上bxq4 &lt;- stripplot(season ~ a1|mno2i, data = df[!is.na(df$mnO2),])grid.arrange(bxq1,bxq2,bxq4,bxq3,nrow = 2,ncol = 2) 可视化参考书籍 Statistics for Technology, Chatfield(1983)。简单很好的统计书籍。例子简单能说明问题 Introductry Statistics with R, Dalgaard(2002) Visualizing Data, Cleveland(1993), 物有所值 The Element of Graphing Data, Cleveland(1995), 更正式 Data Visualization, Chen(2008) R Graphics, Murrell(2006), R 软件绘图]]></content>
      <categories>
        <category>Visualization</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅地系列之: Sublime Text]]></title>
    <url>%2F2019%2F03%2F05%2F%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E5%9C%B0%E7%B3%BB%E5%88%97-with-Sublime-Text%2F</url>
    <content type="text"><![CDATA[主要包含与Sublime Text 3相关的快捷键，命令，插件或它特有的一些代码或功能。让速度飞起。 git bash中直接使用sublime text打开指定文件进行编辑 在C:/User/username 下新建一个.bash_profile文件文件内容为alias subl=&quot;/d/installations/tools/Sublime\ Text\ 3/sublime_text.exe&quot;。即sublime的安装地址 之后在git bash 中输入 subl .filename 即可直接用sublime text 打开该文档进行编辑]]></content>
      <categories>
        <category>tools</category>
        <category>text editor</category>
      </categories>
      <tags>
        <tag>SublimeText</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Assets Categorisation]]></title>
    <url>%2F2019%2F03%2F05%2Fassetscategorisation%2F</url>
    <content type="text"><![CDATA[本研究为2014年在NUS做的研究项目ASSETS CATEGORISATION的FINAL REPORT节选。（以下内容更换了数据未更换方法） 问题描述与目标一些研究证明多元投资组合有利于使投资者以较低的风险获得较高的回报。本研究即想探索股票投资的中的多元投资组合问题。 针对这一问题的具体目标，设定为：选取构成Dow Jones Index 30家公司，通过分析找到能够尽可能降低风险；在低风险的前提下，获得较理想收益的股票投资组合。 现有一些方法分析有些投资者会根据公司所属行业将公司进行分组。例如 J.P.Morgan Chase &amp; Co(JPM), The Goldman Sachs Group Inc(GS) 被划分在银行这一分类下；CISCO System Inc(CSCO), International Business Machines Corporation(IBM) and Microsoft Corporation(MSFT) 被划分在软件这一分类下；The Procter &amp; Gamble Company(PG), Johnson &amp; Johnson(JNJ), Pfizer Inc(PFE) 被划分在制药这一分类下。传统投资者常会选择某一行业分类下的股票投资或从不同行业中各选一些股票组合后进行投资。 有些投资者也会根据股价的波动情况进行分组后投资。由于波动与风险相关，因此，可以认为投资者是基于投资风险分组后进行投资的。如将JPM, GS, PG, JNJ, PFE划分为高风险股票，而CSCO,IBM则被划分为低风险股票。投资者则根据自己的风险偏好，选择投资一组或多组进行投资。 评价准则基于30支股票2015-2017年的收益时间序列，通过分析得到股票投资组合。比较该组合在2018年，风险是否低于购买单一股票或分行业选取股票进行投资的策略。 同时，给出对应的收益率。 问题的不同解决方案因子分析运用因子分析，找到“隐藏的”导致30支股票出现这样变动的“看不见的手”，即“潜在因子”。然后，按照30支股票和“潜在因子”的关联关系，将30支股票划分至不同的“潜在因子”下。我们认为如果按照分别从这些“潜在因子”中挑选股票最终得到投资组合，是有可能降低—选择了受同一市场因素影响的总是同向变动的股票的组合—在某因素负面作用下全部下跌这一风险的。 因子分析后，将继续利用Efficient Frontier 算法，得到每支股票的投资比重，使得该投资组合有最小的风险。 数据说明数据获取Yahoo!Finance, 下载30支股票，2015-2018，周维度交易数据。 其中，2015-2017周维度交易数据作为训练集，2018作为测试集。 数据处理原始回报率 $r_i = \frac{p_i - p_j}{p_j} , j = i -1$ $r_i = \frac{p_i - p_j}{p_j} , j = i -1$ 结果与分析讨论改善附录]]></content>
      <categories>
        <category>project</category>
      </categories>
      <tags>
        <tag>case study</tag>
        <tag>fa</tag>
        <tag>stock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows:利用Hexo搭建个人博客]]></title>
    <url>%2F2019%2F02%2F27%2Fblogwithhexo%2F</url>
    <content type="text"><![CDATA[本文介绍利用Hexo搭建个人博客，并将该博客部署至git。以及对于Hexo主题，页面等的设置方法。 需要安装git和nodejs git nodejs 安装完成后，git bash中使用以下命令检查是否安装成功。12node -v npm -v git安装成功后，就可以使用git bash敲命令了，不再用windowscmd了。 安装hexo在安装好git和nodejs后，继续安装hexo。 创建一个文件夹，如命名为blog git bash中, cd到这个文件夹下(或在这个文件夹下右键git bash打开) 输入npm install -g hexo-cli 安装 使用hexo -v检查是否安装成功 至此，hexo安装结束 初始化hexo使用以下命令初始化hexo。hexo init [subdirname] 新建博客文章(post)使用以下命令新建博客文章。12hexo new post_namehexo n post_name 之后就可以在.md中尽情写文章了。 生成静态网页文章写完后，使用以下命令生成静态网页。12hexo generate hexo g 启动预览服务使用以下命令，本地预览。localhost:4000中打开。12hexo server hexo s 或本地测试1hexo debug 发布网站站点配置文件中部署blog位置blog文件夹下 _config.yml为站点配置文件。打开该文件，新增部署语句如下：1234deploy: type: git repo: https://github.com/username/username.github.io.git branch: master 安装git部署插件1npm install hexo-deployer-git --save 部署12hexo deployhexo d 至此，blog上线。 如果是内容重新更新，则在deploy之前先clean。 1hexo clean Hexo个性化设置更换themecd到blog文件夹，git bash中继续输入1git clone https://github.com/iissnan/hexo-theme-next themes/next 找到站点配置文件中的theme: landscape 修改为theme: next 导航栏设置分类和标签设置 添加”分类” cd 到blog下，执行命令hexo new page categories 成功后会提示INFO Created: d:/blog/source/tags/index.md 找到index.md,添加type: &quot;tags&quot; 注意主题配置文件配置以下内容。打开首页categories 123456789menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 打开需要添加标签的文章，为其添加categories属性。 下方的categories:tools表示添加这篇文章到”tools”这个分类下。注意：hexo中一篇文章只能属于一个分类，也就是说如果在”-tools”下方添加”-xxx”,hexo不会产生两个分类，而是把分类嵌套，即该文章属于”-tools”下的”-xxx”分类 1234567---title: 利用Hexo搭建个人博客date: categories:- web 前端- xxx--- 至此，成功给文章添加分类，点击首页的”分类”可以看到该分类下的所有文章。 添加”标签” cd 到blog下，执行命令hexo new page tags 成功后会提示INFO Created: d:/blog/source/categories/index.md 找到index.md, 添加type: &quot;categories&quot; 打开需要添加标签的文章，为其添加tags属性。下方的tags: blog -hexo 就是这篇文章的标签了 123456789---title: 利用Hexo搭建个人博客date: categories:- toolstags:- blog - hexo --- 至此，成功给文章添加标签，点击首页的”标签”可以看到该标签下的所有文章。 添加”about”并添加想说的话 cd 到blog下，执行命令hexo new page about 成功后会提示INFO Created: d:/blog/source/categories/index.md 找到index.md,写上你想说的话 至此，成功给文章添加标签，点击首页的”标签”可以看到该标签下的所有文章。 添加搜索功能：LocalSearch搜索安装hexo-generator-searchdb. cd到blog下，执行以下命令npm install hexo-generator-searchdb --save 编辑站点配置文件，新增以下内容到任意位置 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件，启用本地搜索功能 123# locallocal_search: enable: ture 首页设置不显示全文Hexo的Next主题默认首页显示每篇文章的全文内容，下面将其修改为只显示部分内容。 方法1: 修改主题配置文件 主题配置文件，找到以下代码： 12345# Automatically Excerpft. Not recommend.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150 把enable的false改成true即可。length设定文章预览的文本长度。 修改后重启hexo即可。 方法2: 在文章内容后加上&lt;!--more--&gt; 在.md文章内容后加上&lt;!--more--&gt;,首页和列表页显示的文章内容就是&lt;!--more--&gt;之前的文字，之后的不会显示 第一种会格式化文章的样式，直接把文章挤在一起显示，最后会有...。第二种不会有 文章设置置顶安装node插件 12npm uninstall hexo-generator-index --save npm install hexo-generator-index-pin-top --save 在需要置顶的文章的Front-matter中加上top:true即可。 也可以指定top: 数字。 数字越大排序越靠前。 123456title: date:tags:categories:description:top: true 显示版权信息主题配置文件，修改以下部分： 12345# Declare license on postspost_copyright: enable: false license: CC BY-NC-SA 3.0 license_url: https://creativecommons.org/licenses/by-nc-sa/3.0/ 将其中的enable: false 改为enable: true。 同时，在站点配置文件中，修改URL为自己的站点的域名地址。 访问统计功能添加博客的访问量。不蒜子统计显示文章的访客数，浏览量等信息 主题配置文件, 找到busuanzi_count，将enable: false改为enable: true 1234567891011121314151617# Show PV/UV of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzi/busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: 访客数 &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; site_uv_footer: 人次 # custom pv span for the whole site site_pv: true site_pv_header: 浏览量 &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: 阅读量 &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; page_pv_footer: 次 当enable: true时，代表开启全局开关； 当site_uv: true时，代表页面底部显示站点UV 当site_pv: true时，代表页面底部显示站点PV 当page_uv: true时，代表文章页面标题下显示该页面的PV 显示文章更新时间主题配置文件，post_meta部分: 123456# Post meta display settingspost_meta: item_text: true created_at: true updated_at: false categories: true 将updated_at: false改为updated_at: true即可。 网站底部去掉底部hexo强力驱动主题配置文件，修改 12345# If not defined, will be used `author` from Hexo main config.copyright:# -------------------------------------------------------------# Hexo link (Powered by Hexo).powered: false 添加博客运行时间themes/next/layout/_partials/footer.swig 1234567891011121314151617181920212223&lt;span class=&quot;footer__copyright&quot;&gt;&lt;div&gt;&lt;span id=&quot;span_dt_dt&quot;&gt; &lt;/span&gt;&lt;script language=&quot;javascript&quot;&gt;function show_date_time()&#123;window.setTimeout(&quot;show_date_time()&quot;, 1000);BirthDay=new Date(&quot;8/25/2016 00:00:00&quot;);//这个日期是可以修改的today=new Date();timeold=(today.getTime()-BirthDay.getTime());//其实仅仅改了这里sectimeold=timeold/1000secondsold=Math.floor(sectimeold);msPerDay=24*60*60*1000e_daysold=timeold/msPerDaydaysold=Math.floor(e_daysold);e_hrsold=(e_daysold-daysold)*24;hrsold=Math.floor(e_hrsold);e_minsold=(e_hrsold-hrsold)*60;minsold=Math.floor((e_hrsold-hrsold)*60);seconds=Math.floor((e_minsold-minsold)*60);span_dt_dt.innerHTML=&quot;博客已运行&quot;+daysold+&quot;天&quot;+hrsold+&quot;小时&quot;+minsold+&quot;分&quot;+seconds+&quot;秒 🐼 &quot;;&#125;show_date_time();&lt;/script&gt;&lt;/div&gt;&lt;/span&gt; 底部的心跳动主题配置文件12footer: icon: heart ~/blog/themes/next/layout/_partials/footer.swig,编辑1&lt;span class=&quot;with-love&quot; id=&quot;heart&quot;&gt; ~/blog/themes/next/source/css/_custom/custom.styl, 添加12345678910111213// 自定义页脚跳动的心样式@keyframes heartAnimate &#123; 0%,100%&#123;transform:scale(1);&#125; 10%,30%&#123;transform:scale(0.9);&#125; 20%,40%,60%,80%&#123;transform:scale(1.1);&#125; 50%,70%&#123;transform:scale(1.1);&#125;&#125;#heart &#123; animation: heartAnimate 1.33s ease-in-out infinite;&#125;.with-love &#123; color: rgb(255, 113, 168);&#125; 代码块代码块样式12```[language][title][url][link-text]code 说明 + [language] 是代码语言的名称，用来设置代码块颜色高亮，非必须； + [title] 是顶部左边的说明，非必须； + [url] 是顶部右边的超链接地址，非必须； + [link text] 如它的字面意思，超链接的名称，非必须。 PythonPython1print("Hello,world!") 设置代码高亮主题主题配置文件，修改highlight_theme 1234# Code Highlight theme# Available value: normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: night Next各种样式文本块内容样式note defaultdefault note infoinfo note successsuccess note warningwarnig note dangerdanger note primary 使用html标签写法primary 123456789&#123;%note default%&#125;### note defaultdefault &#123;%endnote%&#125;&lt;div class=&quot;note primary&quot;&gt; &lt;h4&gt;note primary&lt;/h4&gt; &lt;p&gt;使用html标签写法primary&lt;/p&gt;&lt;/div&gt; 如果想用包含图标的，到主题配置文件，修改为icons: true 12345678910111213# Note tag (bs-callout).note: # Note tag style values: # - simple bs-callout old alert style. Default. # - modern bs-callout new (v2-v3) alert style. # - flat flat callout style with background, like on Mozilla or StackOverflow. # - disabled disable all CSS styles import of note tag. style: simple icons: true border_radius: 3 # Offset lighter of background in % for modern and flat styles (modern: -12 | 12; flat: -18 | 6). # Offset also applied to label tag variables. This option can work with disabled note tag. light_bg_offset: 0 文字内容样式复道行空, 不霁何虹。 长桥卧波,未云何龙。 1&#123;%label @复道%&#125;&#123;%label primary@行空%&#125;, &#123;%label default@不霁%&#125;&#123;%label success@何虹%&#125;。 文字增加背景色块打开themes/next/source/css/_custom下custom.styl文件,添加属性样式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// 颜色块-红span#inline-red &#123;display:inline;padding:.2em .6em .3em;font-size:80%;font-weight:bold;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:0;background-color: #E34018;&#125;// 颜色块-黄span#inline-yellow &#123;display:inline;padding:.2em .6em .3em;font-size:80%;font-weight:bold;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:0;background-color: #f0ad4e;&#125;// 颜色块-绿span#inline-green &#123;display:inline;padding:.2em .6em .3em;font-size:80%;font-weight:bold;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:0;background-color: #5cb85c;&#125;// 颜色块-蓝span#inline-blue &#123;display:inline;padding:.2em .6em .3em;font-size:80%;font-weight:bold;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:0;background-color: #2780e3;&#125;// 颜色块-紫span#inline-purple &#123;display:inline;padding:.2em .6em .3em;font-size:80%;font-weight:bold;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:0;background-color: #9954bb;&#125; 在你需要编辑的文章地方。放置如下代码：1234&lt;span id=&quot;inline-blue&quot;&gt; 站点配置文件 &lt;/span&gt;&lt;span id=&quot;inline-purple&quot;&gt; 主题配置文件 &lt;/span&gt;&lt;span id=&quot;inline-yellow&quot;&gt; 站点配置文件 &lt;/span&gt;&lt;span id=&quot;inline-green&quot;&gt; 主题配置文件 &lt;/span&gt; 改变文字颜色使用html语法直接写即可。 我可以设置这一句的颜色哈哈 &lt;font color=&quot;#FF0000&quot;&gt; 我可以设置这一句的颜色哈哈 &lt;/font&gt; 我还可以设置这一句的大小嘻嘻 &lt;font size=6&gt; 我还可以设置这一句的大小嘻嘻 &lt;/font&gt; 我甚至可以设置这一句的颜色和大小呵呵 &lt;font size=5 color=&quot;#FF0000&quot;&gt; 我甚至可以设置这一句的颜色和大小呵呵&lt;/font&gt; 文字居中&lt;center&gt;这一行需要居中&lt;/center&gt; tabs标签ID3C4.5C5.0这是选项卡 1 这是选项卡 2 这是选项卡 3 1234567891011&#123;% tabs tab, 1 %&#125; &lt;!-- tab ID3 --&gt;**这是选项卡 1**&lt;!-- endtab --&gt;&lt;!-- tab C4.5--&gt;**这是选项卡 2**&lt;!-- endtab --&gt;&lt;!-- tab C5.0--&gt;**这是选项卡 3**&lt;!-- endtab --&gt;&#123;% endtabs %&#125; 按钮BottonNexT进度条加载 1&#123;% btn url,test_tile,,title %&#125; Botton with IconNexT主题设置 1&#123;%btn url,showlabel,hand-o-right%&#125; Botton with Fix-widthNexT个性化设置 1&#123;%btn url, Next样式, hand-o-right fa-fw %&#125; Center 12&lt;div class=&quot;text-center&quot;&gt;&lt;span&gt;&#123;% btn url,,google%&#125;&#123;%btn url,,edge %&#125;&#123;% btn url,,chrome %&#125;&lt;/span&gt;&lt;span&gt;&#123;% btn url,,terminal %&#125;&#123;% btn url,,diamond fa-rotate-270 %&#125;&lt;/span&gt;&lt;/div&gt; 酷绚效果点击发射桃心效果 /themes/next/source/js/src下新建文件clicklove.js。 把下面代码贴到该文件中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950! function (e, t, a) &#123; function n() &#123; c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &apos;&apos;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;), o(), r() &#125; function r() &#123; for (var e = 0; e &lt; d.length; e++) d[e].alpha &lt;= 0 ? (t.body.removeChild(d[e].el), d.splice(e, 1)) : (d[e].y--, d[e].scale += .004, d[e].alpha -= .013, d[e].el.style.cssText = &quot;left:&quot; + d[e].x + &quot;px;top:&quot; + d[e].y + &quot;px;opacity:&quot; + d[e].alpha + &quot;;transform:scale(&quot; + d[e].scale + &quot;,&quot; + d[e].scale + &quot;) rotate(45deg);background:&quot; + d[e].color + &quot;;z-index:99999&quot;); requestAnimationFrame(r) &#125; function o() &#123; var t = &quot;function&quot; == typeof e.onclick &amp;&amp; e.onclick; e.onclick = function (e) &#123; t &amp;&amp; t(), i(e) &#125; &#125; function i(e) &#123; var a = t.createElement(&quot;div&quot;); a.className = &quot;heart&quot;, d.push(&#123; el: a, x: e.clientX - 5, y: e.clientY - 5, scale: 1, alpha: 1, color: s() &#125;), t.body.appendChild(a) &#125; function c(e) &#123; var a = t.createElement(&quot;style&quot;); a.type = &quot;text/css&quot;; try &#123; a.appendChild(t.createTextNode(e)) &#125; catch (t) &#123; a.styleSheet.cssText = e &#125; t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a) &#125; function s() &#123; return &quot;rgb(&quot; + ~~(255 * Math.random()) + &quot;,&quot; + ~~(255 * Math.random()) + &quot;,&quot; + ~~(255 * Math.random()) + &quot;)&quot; &#125; var d = []; e.requestAnimationFrame = function () &#123; return e.requestAnimationFrame || e.webkitRequestAnimationFrame || e.mozRequestAnimationFrame || e.oRequestAnimationFrame || e.msRequestAnimationFrame || function (e) &#123; setTimeout(e, 1e3 / 60) &#125; &#125;(), n()&#125;(window, document); \themes\next\layout_layout.swig, &lt;/body&gt; 上方，添加 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clicklove.js&quot;&gt;&lt;/script&gt; 插件字数与时间统计1npm install hexo-symbols-count-time --save 绘制图mermaid安装1npm i hexo-filter-mermaid-diagrams 主题配置文件, 增加123456# mermaid chartmermaid: ## mermaid url https://github.com/knsv/mermaid enable: true # default true version: &quot;7.1.2&quot; # default v7.1.2 options: # find more api options from https://github.com/knsv/mermaid/blob/master/src/mermaidAPI.js #startOnload: true // default true 123456&#123;% note danger %&#125;注意set `external_link: false`&#123;% endnote %&#125;&lt;span id=&quot;inline-purple&quot;&gt;themes/next/layout/_partials/footer.swig&lt;/span&gt;, 添加 12345678&#123;% btn, https://stazhenggy.github.io/2019/04/20/mermaid/ , 更多mermaid内容, % &#125;### 无响应问题在国内可能会出现`npm`无响应的情况，这时可以换用`cnpm````bash npm install cnpm -g --registry=https://registry.npm.taobao.org 之后使用cnpm install [name] CERT_NOT_VALID_YET先运行 1npm config set strict-ssl false 即可。 .Rmd直接导入hexo有大量的建模使用R实现; 日后查看整个建模流程或查阅一些常用命令。具体查看需求的内容包含两点： 代码以及代码的执行结果，包含图片 以上需同步到博客 针对以上两个需求的实现 用rmarkdown直接实现 借助hexo &lt;- 将.rmd转成.md, 放到_post文件夹下即可。 可能有bug的点，是图片以及公式能否显示成功。(已解决) 已有方案Xie yihui(谢益辉),谢大大已经有新的package: blogdown可以实现在RStudio中生成博客, 支持Hugo主题。 但自己，已经有基于Hexo搭建的博客，且现阶段基于sublime text + markdown + git bash 写博客的流程已经比较顺手且暂时不想再转换。所以，准备鼓捣利用rmarkdown，生成.md文档。还保持之前写博客的方法。 从网上查了一些资料写了以下方案，亲测代码，代码结果输出，图片，公式，网页链接引用均没有问题。 流程不算复杂，基本满足我个人需求。 解决方案.rmd -&gt; .md -&gt; hexo在.rmd文件yaml head里加入以下代码： 12345--- output: md_document variant: markdown_github--- 直接knit即可生成.md，然后将这个.md移到_post文件夹下即可。 需要注意的一点是，如果.rmd中包含的了图片，在knit的时候，会创建名为rmdname_files的文件夹，存放该.md中对应的图。 .md引用该图使用的是markdown语法的相对路径，即![](rmdname_files/figure-markdown_github/chunkname-1.png)。为了保证能正确显示图片，建议使用以下流程： 在站点配置文件中, 将post_asset_folder: false 改为 true。 修改后，每次执行hexo n(new) post_name 将不只在_post下生成post_name.md, 同时新建一个与post_name`同名的文件夹。 在Rstudio中, setwd()到post_name文件夹下。新建.rmd并保存到该文件夹下，注意.rmd命名为post_name.rmd 开始写.rmd文档。写好后，点击knit, 将会在post_name文件夹中生成post_name.md以及新文件夹:post_name_files 将post_name文件夹中的post_name.md替换_post文件夹下的同名.md即可。 其他方案参见BaoDuGe_飽蠹閣的博文：如何将我的R项目更好地展示在Hexo博客上 参考 Next各种样式 Next更多自定义样式 Next深度优化 Hexo+NexT优化设置 Hexo各种自定义 怎么在hexo博客系统中用Rmarkdown写文章 资源文件夹 Hexo图片插入 Hexo用Latex渲染数学公式 Hexo中渲染MathJax数学公式 MathJax MathJax Reference 绘图插件 mermaid实用教程 mermaid绘图基础命令]]></content>
      <categories>
        <category>tools</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Way of Thinking]]></title>
    <url>%2F2019%2F02%2F25%2Fwayofthinking%2F</url>
    <content type="text"><![CDATA[类型 思维要点 STAR法则 Situation Task Action Result 背景： 阐述为什么做这件事/事情是在什么情况下发生的 目标： 目的达到什么样的效果/你是如何明确你的任务的 方案：怎么去做这件事，具体有哪些动作/针对这样的情况分析，你采用了什么行动方式 (难点：实施过程中存在哪些难点，可形成里程碑式攻克点) 成果：最终取得的成果/结果怎样，在这样的情况下你学习到了什么 分析 数据实际情况 发现的问题点 建议解决方案 监控追踪是分析的基础 问题-&gt;全方案 描述具体问题：发现并描述一个具体问题点 评估整体影响：问题对整体项目的影响 给出单一回答：精准回答此问题的解决办法 给出规模化方案：规模化复制形成迭代的新方案 5W2H方法 What：工作的内容和达成的目标 Why：做这项工作的原因 When：在什么时间进行工作 Where：工作发生的地点 Where：工作发生的地点 How much：需要多少成本 SMART目标管理原则 Specific：具体的 Measurable：可测量的 Attainable：可达到的 Relevant：相关的 Time based：时间的]]></content>
  </entry>
  <entry>
    <title><![CDATA[分析报告包含的内容框架参考]]></title>
    <url>%2F2018%2F12%2F08%2Freportframework%2F</url>
    <content type="text"><![CDATA[本篇内容主要为分析内容的框架参考。 问题描述与目标 一般包含问题的背景（遇到什么问题/解决问题的意义） 主要解决的问题 解决该问题带来的收益 样本以及变量概述 数据说明 数据来源 数据集每条记录概述，变量描述 数据集训练集，测试集划分 数据可视化和摘要 统计描述，一般是探索性数据分析，连续变量均值，中位数，四分位数，极值等一系列统计信息。 可以通过观察均值与中位数的差异以及四分位距，了解数据偏度和分散情况；对于离散性变量，可以看到每个取值的频数，了解样本分布是否平均等信息。 可以绘制直方图(箱线图)了解数据分布 数据缺失定义预测任务 预测什么 变量是什么 预测任务 特征选择 问题的不同解决方案 模型评价准则 实验方法 预测模型 如何应用训练集数据建模 建模工具/技术 从预测到实践 如何应用预测模型 与实际相关的评价准则 模型集成 模型评价和选择 模型比较 实验比较 结果分析 系统集成]]></content>
      <tags>
        <tag>case study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lr]]></title>
    <url>%2F2017%2F09%2F23%2Flr%2F</url>
    <content type="text"></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>logistic regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caret]]></title>
    <url>%2F2017%2F09%2F23%2Fcaret%2F</url>
    <content type="text"></content>
  </entry>
</search>
