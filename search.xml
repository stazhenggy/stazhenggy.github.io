<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[BAT及各类机器学习面试整理]]></title>
    <url>%2F2019%2F04%2F10%2Fsource_postsbatinterview%2F</url>
    <content type="text"><![CDATA[写在前面：1 本文的内容部分来源于七月在线发布的BAT机器学习面试1000题系列。 部分是自己面试其他公司时被问到的一些其他问题。剩余是自己在平时思考整理的一些内容。 定期复习，定期检查自己对于一些算法的理解。 遇到好的内容，也会不断迭代更新。 2 部分摘抄自其它的博客的内容，都在后面全部跟上了链接。 简要介绍下SVM(进阶-&gt;手推SVM)分类算法。 目标是确定一个分类超平面，从而将不同的数据分隔开。 支持向量机学习方法包括构建由简至繁的模型：线性可分支持向量机、线性支持向量机及非线性支持向量机。当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。 SVM经典博客 支持向量机通俗导论(理解SVM的三层境界) 机器学习之深入理解SVM 在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别【中】欧氏距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点$x = (x_1,\cdots,x_n)$ 和 $y = (y_1,\cdots,y_n)$ 之间的距离为： De_{xy} = \sqrt{(x_1 - y_1)^{2} + (x_1 - y_1)^{2} + \codts + (x_n -y_n)^{2}} = \sqrt{\sum_{i = 1}^{n}{(x_i - y_i)^{2}}}曼哈顿距离，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标$(x_1, y_1)$的点$P_1$与坐标$(x_2, y_2)$的点$P_2$的曼哈顿距离为: Dm_{xy} = |x_1 - x_2| + |y_1 - y_2| 要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。 通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。 此外，欧几里得以及曼哈顿距离都是$L_p$距离的特例。 点$x = (x_1,\cdots,x_n)$ 和 $y = (y_1,\cdots,y_n)$ 的$L_p$距离为 $$Dl_{xy} = ((x_1 - y_1)^{p} + (x_1 - y_1)^{p} + \codts + (x_n -y_n)^{p})^{\frac{1}{p}}相关内容 KNN,距离度量 LR 【难】@rickjin：把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。 LR前世今生 机器学习之logistic回归 LR和SVM的联系与区别@朝阳在望，联系： 1 LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）2 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 区别：1 LR是参数模型，SVM是非参数模型。2 从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。3 SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。4 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。5 logic能做的svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。6 logic给出的是概率 GBDT和XGBoost的区别是什么@Xijun LI：XGBoost类似于GBDT的优化版，不论是精度还是效率上都有了提升。与GBDT相比，具体的优点有 机器学习算法与Python实践之LR 1.损失函数是用泰勒展式二项逼近，而不是像GBDT里的就是一阶导数2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性3.节点分裂的方式不同，GBDT是用的基尼系数，XGBoost是经过优化推导后的 相关内容 集成学习内容 LR与线性回归的区别与联系【中】@AntZ: LR工业上一般指Logistic Regression(逻辑回归)而不是Linear Regression(线性回归). LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围, 其目标函数也因此从差平方和函数变为对数损失函数, 以提供最优化所需导数（sigmoid函数是softmax函数的二元特例, 其导数均为函数值的f*(1-f)形式）。请注意, LR往往是解决二元0/1分类问题的, 只是它和线性回归耦合太紧, 不自觉也冠了个回归的名字(马甲无处不在). 若要求多元分类,就要把sigmoid换成大名鼎鼎的softmax了。 决策树, Random Forest, Boosting, Adaboost, GBDT和XGBoost的区别是什么随机森林Random Forest是一个包含多个决策树的分类器。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。 @Xijun LI：xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的 https://xijunlee.github.io/2017/06/03/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/ KNN中的K如何选取KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说： 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合； 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。 K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。 在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。 相关内容 KNN 防止过拟合的方法过拟合的原因是算法的学习能力过强；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。 处理方法有： 早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练。 决策树pre-pruning(cp, maxdepth, minsplit) 数据集扩增：原有数据增加、原有数据加随机噪声、重采样 正则化 交叉验证 特征选择/特征降维 哪些机器学习算法不需要做归一化处理概率模型(比如树模型)不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。 @管博士：我理解归一化和标准化主要是为了使计算更方便, 比如两个变量的量纲不同, 可能一个的数值远大于另一个。 那么他们同时作为变量的时候, 可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确 或者梯度下降法的收敛比较困难。 还有如果需要计算欧式距离的话可能量纲也需要调整。 对于树形结构为什么不需要归一化数值缩放，不影响分裂点位置。 因为第一步都是按照特征进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。 数据归一化（或者标准化，注意归一化和标准化不同）的原因@我愛大泡泡 能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。 相关内容 数据归一化 有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。 有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。 补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。 请简要说说一个完整机器学习项目的流程@寒小阳、龙心尘 1 抽象成数学问题 明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。 2 获取数据 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。数据要有代表性，否则必然会过拟合。对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。 3 特征预处理与特征选择 良好的数据要能够提取出良好的特征才能真正发挥效力。特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。 4 训练模型与调优 直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。 5 模型诊断 如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。 过拟合、欠拟合判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。 过拟合的基本调优思路是增加数据量，降低模型复杂度。 欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。误差分析也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。 6 模型融合 一般来说，模型融合后都能使得效果有一定提升。而且效果很好。 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。 7 上线运行 这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。 机器学习算法大体步骤 1 对于一个问题，用数学语言来描述它； 然后对应一个模型来描述这个问题 2 通过极大似然、最大后验概率或最小化分类误差等等建立模型的代价函数， 问题转化为最优化问题。 3 求这个代价函数的最优化问题的解。 求解大体分以下几种情况： + 代价函数存在解析解。 一般方法是对代价函数求导，找到导数为0的点即为最优解。 如果代价函数能简单求导，并且求导后为0的式子存在解析解，就可以直接得到最优参数 + 如果代价函数很难求导数，例如函数里存在隐含变量或变量间存在相互依赖的情况；或求导后为0的式子得不到解析解，就需要借助迭代算法来一步步找到最优解。 逻辑斯特回归为什么要对特征进行离散化@严林 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。 相关内容 LR特征离散化 什么是熵熵、联合熵、条件熵、相对熵、互信息的定义简单说下有监督学习和无监督学习的区别有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT） 无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL) 协方差和相关性有什么区别相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。 线性分类器与非线性分类器的区别以及优劣如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。 常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归常见的非线性分类器：决策树、RF、GBDT、多层感知机SVM两种都有(看线性核还是高斯核)线性分类器速度快、编程方便，但是可能拟合效果不会很好非线性分类器编程复杂，但是效果拟合能力强 简单说说贝叶斯定理全概率公式：P(A) = \sum_{i = 1}^{n}{P(A|B_i)*P(B_i)} 贝叶斯公式：P(A = a_i|B) = \frac{P(B|A = a_i)*P(A = a_i)}{\sum_{i=1}^{n}{P(B|A = a_i)*P(A = a_i)}} 相关内容 从贝叶斯方法到贝叶斯网络 简要介绍分类决策树决策树呈树型结构。 分类决策树是基于特征对实例进行分类的过程。 它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。 其主要优点是具有可读性，分类速度快。 决策树学习包含3个步骤： 特征选择，决策树的生成和决策树的剪枝。 特征选择决策树特征选择在于选取对训练数据具有分类能力的特征。 通常特征选择的准则是信息增益(ID3), 信息增益比(C4.5), 基尼系数(CART)。 决策树生成ID3算法的核心是在决策树各个结点上应用信息增益准侧选择特征， 递归地构建决策树。 具体方法是：从根结点开始，对结点计算所有特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点； 再对子结点递归地调用以上方法，构建决策树； 直到所有特征信息增益都很小或没有特征选择为止。 最后的得到一个决策树。 C4.5与ID3的算法类似，只是使用信息增益比来选择特征。 决策树剪枝决策树生成算法递归地产生决策树，直到不能再继续下去为止。 这样产生的分类树可能会存在过拟合。 过拟合的原因是由于学习时过多考虑如何提高对训练数据的准确分类，从而构建出过于复杂的分类树。 解决这个问题的方法是考虑决策树的复杂度， 对已生成的树进行简化。 这一简化过程称为剪枝。 决策树的剪枝往往通过极小化损失函数来实现。 剪枝算法为从叶结点开始，向上回缩至父结点。 如果回缩之后，损失函数变小或增大值小于阈值， 则剪枝，该父结点变为新的叶结点。。 KNN中的K如何选取 determining k in knn The trick is that — in general — the lower the k value, the better the performance in the training set. That is to say, the better your model will capture the variability for the set of data it was trained on. You can think of it this way: k = 1 is the most overfit case for all instances. The prediction is based solely on the training sample nearest the sample provided. The trouble is that — even in a low dimensional, intuitive space — this cannot (or rather, does not frequently) generalize well. On larger data sets, it is better to increase the number of neighbors to better represent the shared characteristics of the class being discriminated: some variability is acceptable but it (hopefully) generallly cancels out to best reflect the average properties of the class(es) being identified. In general, there is no magic bullet for this problem. Sometimes, it might be obvious: plot the generalization error as a function of k. If there is an obvious elbow (rapid decrease followed by plateau) that is a good indication of an appropriately selected value for k. It means that there is a value of k “suggested” by the training data: a value that generalizes optimally without undue calculations of the class of nearest neighbors. There is no clear analytical solution, though. Fundamentally, this is a question of how well your training data reflects your testing data and how well your training and testing data reflect the data outside of the collected samples. Let me know if you have additional questions! I’m passionate about data science and happy to refine my answer! For a loose intuition, a low value of k corresponds to “sharp” decision boundaries in the classification space. Higher values of k correspond to “curvier” or, in the limit flat, decision boundaries. My recommendation would to make some synthetic data to get an intuition for the effect of varying k! If you carry on going, you will eventually end up with the CV error beginning to go up again. This is because the larger you make k, the more smoothing takes place, and eventually you will smooth so much that you will get a model that under-fits the data rather than over-fitting it (make k big enough and the output will be constant regardless of the attribute values). I’d extend the plot until the CV error starts to go noticably up again, just to be sure, and then pick the k that minimizes the CV error. The bigger you make k the smoother the decision boundary and the more simple the model, so if computational expense is not an issue, I would go for a larger value of k than a smaller one, if the difference in their CV errors is negligible. If the CV error doesn’t start to rise again, that probably means the attributes are not informative (at least for that distance metric) and giving constant outputs is the best that it can do. 如何确定聚类算法k-means中的k, 有几种方法并简要介绍下 kmeans in r determine k 选择题1 下面哪种不属于数据预处理的方法？ (D)A变量代换 B离散化 C 聚集 D 估计遗漏值 Reference BAT机器学习面试1000题系列jianshu BAT机器学习面试1000题系列CSDN 如何准备机器学习工程师的面试]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模型评价]]></title>
    <url>%2F2019%2F04%2F10%2Fmodelfit%2F</url>
    <content type="text"><![CDATA[没有测量，就没有科学。 —— 门捷列夫 模型评估是模型开发过程不可或缺的一部分。 它有助于发现表达数据的最佳模型和所选模型将来工作的性能如何。 在数据挖掘中， 使用训练集中的数据评估模型性能是不可接受的， 容易过拟合。 为了避免过拟合，都使用模型没有遇到过的测试集来评估模型性能。 为什么要做模型评价想找到最有效的模型.模型的应用是循环迭代的过程,只有通过持续调整和调优才能适应在线数据和业务目标.选用模型开始都是假设数据的分布是一定的,然而数据的分布会随着时间的移动而改变，这种现象称为分布漂移（Distribution Drift）。验证指标可以对模型在不断新生的数据集上进行性能跟踪。当性能开始下降时，说明该模型已经无法拟合当前的数据了，因此需要对模型进行重新训练了。 模型能够拟合新的数据称为模型的泛化能力。 怎么检验和评估模型机器学习过程分为原型设计阶段（Prototyping）与应用阶段（Deployed）。其中原型设计阶段（Prototyping）为离线评估，应用阶段（Deployed）为在线评估(online evaluation). Prototyping阶段是使用历史数据训练一个适合解决目标任务的一个或多个机器学习模型，并对模型进行验证（Validation）与离线评估（Offline evaluation），然后通过评估指标选择一个较好的模型。 Deployed阶段是当模型达到设定的指标值时便将模型上线，投入生产，使用新生成的在线数据来对该模型进行在线评估（Online evaluation）。在线测试不同于离线测试，有着不同的测试方法（最常见的便是A/B testing，它是一种统计假设检验方法）以及评价指标（在线评估时会采用业务指标,如用户点击率等，而离线评估则采用不同的评估指标,如常用的经验误差的方法）。 模型评估在离线评估阶段从历史数据据获取数据集校验模型的方法包括训练集-验证集二划分校验（Hold-out validation）、交叉校验（Cross-validation）, 另一种方式是重采样技术，如bootstrapping与Jackknife,此类方法可以充分利用现有数据信息,一定程度减少过拟合。 模型性能离线评估方法我们把学习器在训练集上的误差称为“训练误差”或“经验误差”, 在新样本上的误差称为“泛化误差”。 在很多情况下，我们可以学得一个经验误差很小，在训练集上表现很好的学习器。 这也是学习器学习的目标。 但很多情况下，训练集上表现良好的学习器，在面对新样本时，表现并不好。 这并不是我们真正想要的。 我们真正想要的是泛化误差小的学习器， 即在新样本上能够表现很好的学习器。 为了达到这个目的， 应该从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”， 这样此案嗯在遇到新样本时作出正确的判断。 然而，当学习器把训练样本学得“太好了”，就很可能把训练样本自身的一些特点当作所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降。 这就是“过拟合”。 与之相对的是“欠拟合”，即连训练样本的一般性质尚未学好。 导致过拟合的原因是学习器学习能力过于强大，把训练样本不太一般的特性都学到了，而欠拟合通常是由于学习能力低下造成的。 欠拟合比较容易克服，例如在决策树中扩展分枝，神经网络增加训练轮数； 而过拟合则比较麻烦。 过拟合是机器学习面临的关键障碍，各类学习器都必然带有一些针对过拟合的措施。 同时，过拟合是无法彻底避免的。 我们往往有多种学习算法可供选择，甚至同一个算法也可以配置不同的参数。 那么究竟该选用哪一个学习算法，哪一种参数配置呢？ 这就是机器学习中的“模型选择(model selection)” 问题。 理想的方案是度候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。 然而在模型构建时我们无法获得泛化误差，而训练你误差又由于过拟合现象的存在而不适合作为标准。 那么，现实中如何进行模型评估与选择呢？ 通过实验测试对学习器的泛化误差进行评估并选择。 为此，需使用一个测试集(testing set) 来测试学习器对新样本的判别能力，然后以测试集上的“测试误差” 作为泛化误差的近似估计。 但需要注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、 未在训练过程中使用。 我们只有一个数据集，既要训练，又要测试。实现方法是： 从该数据集中产生出训练集和测试集。 留出法(Hold-out validation)使用这种方法时，通常大的数据集会被随机分成2个子集，1个用于训练，1个用于测试。 或数据量更大时，将训练集拆分成3个子集，1个用于训练，1个用于模型选择和参数优化，1个用于测试。 需要注意的是，这种数据集划分相当于“分层采样(Stratified Sampling)”。 另外一个问题是，单次使用留出法得到的估计结果往往不够稳定可靠，在使用时，一般采用若干次随机划分重复实验评估后取平均值作为留出法的评估结果。 例如进行100次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果，而留出法返回的则是这100个结果的平均。 最后一点是，训练样本过多则训练接近于使用全量样本，过少则与全量样本相差过大，可能也无法学习出“普遍规律”。 对于这一点，也没有完美的解决方案，常见做法是2/3 - 4/5用于训练。 训练集(Training set)： 用于构建模型 验证集(Validation set)： 用于评估训练阶段所得模型的性能。 它为模型参数优化和选择最优模型提供了测试平台 测试集(Test set)： 用于评估模型未来可能的性能(泛化能力) 为什么不能在训练集上直接验证？ 训练误差小的，泛化误差不一定小。 交叉检验(Cross Validation)当仅有有限数量的数据时， 可以使用k折交叉验证(k-fold cross validation)。使用这种方法时， 数据被随机分成k份大小与结构均相似的互斥的子集。 然后，每次用k-1个子集的并集用于训练，余下的那个作为测试集。 进行k次训练。 最终使用k个学习器性能平均预测未来可能的性能。 k最常见的取值是10。 其他常用的还有5，20(实际中，还需要考虑时间，存储开销等其他因素。 如果学习器本身比较复杂，k如果较大则意味着时间和存储的开销都会比较大)。 与留出法相似，将数据集划分为k个子集同样存在多种划分方式，为减小因样本划分不同而引入的差别, k折交叉验证通常要重复p次不同的随机划分，最终的评估结果是这p次交叉验证结果的均值，常见的是10次10折交叉验证。（平均的平均，去掉划分的影响） 自助法在数据集较小，难以有效划分训练/测试集时很有用； 此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。 然而，自助法产生的数据集改变了初始数据集的分布(而这恰好是集成学习需要的)，这会引入估计偏差。 在初始数据量足够时，留出法和交叉验证法更常用一些。 自助法我们希望评估的是用D训练出的模型。 由于保留了一部分样本用于测试， 这必然会引入一些因训练样本规模不同而导致的伏击偏差。 留一法虽受训练样本规模的变化影响较小，但是计算复杂度太高。 自助法(Boostraping)就是一个比较好的解决方案。 评估过程中如何调优在进行模型评估与选择时，除了要对适用学习算法进行选择，还需要对参数进行设定。 这就是通常所说的“参数调节(Parameter Tuning)”。 参数除了模型参数还有超参数（hyperparameters）。例如logistic回归中的特征系数为模型参数,需要使用多少个特征进行表征，特征的数目这个参数便是该模型的超参数。 调参和算法选择没什么本质区别： 对每种参数配置都训练出模型，热爱后把对应最好模型的参数作为结果。 需要注意的一点是学习算法的很多参数是在实数范围内取值。 现实中常用的做法是对每个参数选定一个范围和变化步长。 这样选定的参数往往不是“最佳”值，但这是在计算开销和性能估计之间进行折中的结果。 在很多强大的学算法中有大量参数需要设定，这将导致极大的调惨工程量。 以至于在不少应用任务中，参数调得好不好往往对最终模型性能有关键性影响。 可以用格搜索（grid search）、随机搜索（random search）以及启发式搜索（smart search）等进行Hyperparameter tuning, 从超参数空间中寻找最优的值。 格搜索(grid search)格搜索便是将超参数的取值范围划分成一个个格子,对每一个格子所对应的值进行评估，选择评估结果最好的格子所对应的超参数值。例如，对于决策树叶子节点个数这一超参数，可以将值划分为这些格子：10, 20, 30, …, 100, … 随机搜索（random search）它是格搜索的变种。相比于搜索整个格空间，随机搜索只对随机采样的那些格进行计算，然后在这中间选择一个最好的。因此随机搜索比格搜索的代价低。 需要注意的是，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上只使用了一部分数据训练模型。 在模型选择完成后，学习算法与参数配置已选定， 应该用全量数据集重新训练模型。 这个模型训练过程中使用了所有样本，才是最终提交的模型。 模型评价指标(性能度量)对学习器泛化性能进行评估，不仅要有有效可行的实验估计方法，还需要有衡量泛化能力的评价标准，这就是性能度量(performance measure). https://blog.csdn.net/u014182497/article/details/79384233 sklearn模型评价 回归任务msermsemaemape(Mean Absoloute Precentage Error)mape = \frac{1}{m}|\frac{trues - preds}{trues}|$分类任务recallprecisionF1P-R曲线 P-R曲线绘制方法： 纵轴P，横轴R. 点： 根据预测结果，概率从高到低，按顺序逐个把样本作为positive进行预测，每次可计算出当前的R和P 缺点：有交叉时不好比较，更好的方法是计算面积，但这个区域不好估算。 所以设计了一些综合考虑两者的性能度量指标。 R和P的调和平均 F1 = \frac{1}{2}(\frac{1}{P}+ \frac{1}{R})一种情形是，我们有多个二分类混淆矩阵，例如进行了多次训练，每次得到一个混淆矩阵；或者执行多分类任务，每两两类别组合都对应一个混淆矩阵；…… 这时，我们希望在n个二分类混淆矩阵上综合考虑R和P。 一种直接的做法是先计算每个混淆矩阵的R和P,再求平均值。 这样就得到“macro-”系的性能度量指标； 还可以先将个混淆矩阵对应元素进行平均，得到TP, FP, FN, TN的平均值，再计算P,R, 这种算法得到的是“micro-”系。 macro-Pmacro-P = \frac{1}{n}\sum_{i=1}^{n}{P_i}macro-Rmacro-R = \frac{1}{n}\sum_{i=1}^{n}{R_i}macro-F1macro-F1 = \frac{1}{2}\frac{1}{\frac{1}{macro-R}+\frac{1}{macro-P}}micro-Pmicro-Rmicro-F1ROCROC和PR类似，ROC曲线绘制方法： 纵轴FPR，横轴TPR. 点： 根据预测结果，概率从高到低，按顺序逐个把样本作为positive进行预测，每次可计算出当前的TPR和FPR AUCPR All models are wrong. But some are useful. Rreference+模型评价与标准 https://blog.csdn.net/u014182497/article/details/79384233]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Random Forest]]></title>
    <url>%2F2019%2F04%2F02%2Frf%2F</url>
    <content type="text"><![CDATA[信息是用来消除随机不确定性的东西。 —— 香农 集成学习(Ensemble Learning)集成学习通过构建并结合多个学习器来完成学习任务。 集成学习的一般结构： 先产生一组“个体学习器”，再用某种策略将它们结合起来。 个体学习器通常由一个现有的学习算法从训练数据产生。 如果个体学习器是“同质”的，即个体学习器由相同的算法生成，则这些个体学习器亦称“基学习器”，相应的学习算法称为“基学习算法”。 如果个体学习器是“异质”的，即个体学习器由不同的学习算法生成，则个体学习器常称为“组件学习器”或直接称为个体学习器。 根据个体学习器的生成方式，目前的集成学习大致可以分为两大类，一类是个体学习器间存在强依赖关系，必须串行生成的序列化方法，代表是Boosting. 另一类是个体学习器间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和Random Forest. BoostingBoosting 是一族可将弱学习器提升为强学习器的算法。 这族算法的工作机制类似： 先从初始训练集训练出一个基学习器， 再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续得到更多的关注。 然后基于调整后的样本分布来训练下一个基学习器。 如此重复进行，直到基学习器数目达到事先指定的值T。 最后将这T个基学习器进行加权结合。 Boosting算法要求基学习器能对特定的数据分布进行学习。 这可通过”重赋权法(re-weighting)”实施，即在训练过程的每一轮，根据样本分布为每个训练样本重新赋予一个权重。 Boosting主要关注降低偏差， 因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。 Bagging与随机森林欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然“独立”在现实任务中无法做到(因为所有的分类器是为解决同一个问题训练出来的)，但可以设法使基学习器尽可能具有较大的差异。 给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。 这样，由于训练数据不同，我们获得的基学习器有望具有比较大的差异。 然而，为获得好的集成，又希望个体学习器不能太差。 如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。 为了解决这个问题，可以考虑使用互相有交叠的采样子集。 不同于Boosting, Bagging是通过re-sampling实现对不同的数据分布进行学习的。 BaggingBagging(Bootstrap Aggregating)是并行式集成学习方法最著名的代表。 它直接基于Boostraping. 给定包含m个样本的数据集，有放回随机抽样得到含m个样本的采样集(初始训练集中有的样本在采样集里多次出现，有的则从未出现。 初始训练集中约有63.2%的样本出现在采样集中)。 照这样，可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。 这就是Bagging的基本流程。 在对预测输出进行结合时，Bagging对分类任务使用多数表决法，对回归任务使用简单平均法。 若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选一个， 也可以进一步考察学习器投票的置信度来确定最终胜者。 假定基学习器的计算复杂度为$O(m)$, 则Bagging的复杂度大致为$T(O(m) + O(s))$, 考虑到采样与投票/平均过程的复杂度$O(s)$很小， 而T通常是一个不太大的常数，因此， 训练一个Bagging集成与直接使用基学习算法训练一个学习器的复杂度同阶，这说明Bagging是一个很高效的集成学习算法。 另外，与标准Adaboost只适用于二分类任务不同，Bagging能不经修改地用于多分类、回归等任务。 另外，自主采样过程还给Bagging带来了另一个优点： 由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用作验证集来对泛化性能进行“包外估计”(out-of-bag estimate)。 包外样本还有其他用途。 例如当基学习器是决策树时， 可使用包外样本来辅助剪枝， 或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理； 当基学习器是神经网络时， 可使用包外样本来辅助早期停止以减小过拟合风险。 Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。 Random ForestRandom Forest是Bagging的一个扩展变体。 RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。 具体来说， 传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性； 而在RF中，对基决策树的每个结点， 先从该结点的属性集合中随机选择一个包含k个属性的子集，再从这个子集中选择一个最优属性用于划分。 这里的参数k控制了随机性的引入程度： 若k = 0, 则基决策树的构建与传统决策树相同；若k = 1，则随机选择一个树型用于划分； 一般情况下， 推荐$k = log_{2}d$. 随机森林简单，容易实现，计算开销小，令人惊奇的是，在很多现实任务中展现出强大的性能，被誉为“代表集成学习技术水平的方法”。 相比Bagging中基学习器的“多样性”仅通过样本扰动(初始训练集采样)而来不同， 随机森林中的基学习器的多样性还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。 此外，随机森林的收敛性与Bagging相似。 随机森林的起始性能往往相对较差。 这很容易理解，因为通过引入属性扰动， 随机森林中个体学习器的性能往往有所降低。 然而，随着个体学习器数目的增加， 随机森林通常会收敛到更低的泛化误差。 值得一提的是，随机森林的训练效率常常优于Bagging. 因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，即在选择划分属性时要对结点的所有属性进行考察，而随机森林使用“随机型”决策树只需考察一个属性子集。 几个优点 准确率高 有效运行在大数据集上 可以处理高维特征样本，不需要降维 大数据集高维特征也相对稳定 可评估各特征在分类问题上的重要性 生成过程中可以获取到内部生成误差的一种无偏估计 可以处理缺失值 大多情况下不容易过拟合(双random-sampling) 缺点 在数据噪音比较大的情况下会过拟合，过拟合的缺点对于随机森林来说还是较为致命的。 森林中每棵树的生成(并行) 对于每棵树而言，随机有放回地从训练集中抽取$N$个训练样本(Bootstraping)作为该树的训练集。 如果每个样本的特征维度为$M$, 指定一个常数m &lt;&lt; M, 随机从$M$个特征中选取$m$个特征。 每次基于这m个特征生成单棵树。 每颗树尽最大程度生长，没有剪枝 随机森林的错误率相关因素 任意两颗树的相关性： 相关性越大， 错误率越大 每棵树的分类能力： 每颗树的分类能力越强，整个森林的错误率越低 减小$m$, 树的相关性和分类能力会相应降低；增大$m$, 两者也会随之增大。 所以关键问题是如何选择最优的m。 袋外错误率(obb error)上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。 随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。 我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。 而这样的采样特点就允许我们进行oob估计，它的计算方式如下： 对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树） 以简单多数投票作为该样本的分类结果 最后用误分个数占样本总数的比率作为随机森林的oob误分率 1Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests. oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。 和其他算法的比较v.s. CART两者中的决策树均是二叉树。 CART RF CART在树生成时，从特征集中挑选最优的特征分列 RF在树生成时，会随机从特征集中挑选子特征集，再从该子集中挑选最优特征分裂 CART uses different stopping rules for tree growth, which ususally leads to a much shallower tree RF中每颗子树都完全生长直到叶结点“纯”。 因此RF中的子树可能会比较大 此外，RF相比CART，有两点新增： 新增bagging. 即RF基于Boostraping构建出T个与原数据集相同大小的训练集。然后在每个训练集上构建tree model. 最终将所有树集成森林。 在每颗树生成时，节点分裂仅使用部分特征。 结合策略简单平均加权平均绝对多数投票法加权投票法类标记 类概率 有趣的是， 虽然分类器估计出的类概率值一般都不太准确， 但基于类概率进行结合往往比直接基于类标记进行结合性能更好。 学习法当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。 Stacking是学习法的典型代表。 把个体学习器称为初级学习器， 用于结合的学习器称为次级学习器或元学习器。 Stacking 先从初始数据集训练出初级学习器， 然后“生成” 一个新数据集用于训练次级学习器。 在这个新数据集中， 初级学习器的输出被当作样例输入特征， 而初始样本的标记仍被当作样例标记。 在训练阶段，次级训练集是利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险比较大； 因此，一般通过交叉验证或留一法这样的方式， 用训练初级学习器未使用的样本来产生次级学习器的训练样本。 以k折交叉验证为例。 初始训练集D被随机划分为k个大小相似的集合，$D_1,D_2,\cdots,D_k$. 令$D_j$和$\bar{D_j}$分别表示第j折的测试集和训练集。 给定T个初级学习算法，初级学习器$h_{t}^{j}$为在$\bar{D_j}$上使用第t个学习算法而得。 对每个样本，$x_i$, 令$z_{it} = h_t(x_i)$ 表示$x_i$在第t个初级学习器上的预测值。 则由$x_i$产生的次级训练集为 ${z_i} = (z_{i1},z_{i2},\cdots,z_{iT})$ 在整个交叉验证结束后，从这T个初级学习器产生的次级训练集是$D^{‘} = {({z_i},y_i)}_{i = 1}^{m}$ 思考1 设计一种能提升KNN性能的集成学习算法 2 MultiBoosting将Adaboost作为Bagging的基学习器， Iterative Bagging 将Bagging作为Adaboost的基学习器。 比较两者的优缺点。 MultiBoosting由于集合了Bagging，Wagging，AdaBoost，可以有效的降低误差和方差，特别是误差。但是训练成本和预测成本都会显著增加。Iterative Bagging相比Bagging会降低误差，但是方差上升。由于Bagging本身就是一种降低方差的算法，所以Iterative Bagging相当于Bagging与单分类器的折中。 3 随机森林为什么比决策树Bagging集成的训练速度更快 随机森林不仅会随机样本，还会在所有样本属性中随机几种出来计算。这样每次生成分类器时都是对部分属性计算最优，速度会比Bagging计算全属性要快。 4 Bagging通常为何难以提升朴素贝叶斯分类器的性能 Bagging主要是降低分类器的方差，而朴素贝叶斯分类器没有方差可以减小。对全训练样本生成的朴素贝叶斯分类器是最优的分类器，不能用随机抽样来提高泛化性能。 5 GradientBoosting是一种常用的Boosting算法，试分析其与AdaBoost的异同。 相同在于：都要生成多个分类器以及每个分类器都有一个权值，最后将所有分类器加权累加起来。 不同在于：AdaBoost通过每个分类器的分类结果，改变样本权重用于生成新的分类器和计算对应权值，但用于训练的样本本身不会改变； GradientBoosting将每个分类器对样本的预测值与真实值的差值传入下一个分类器来生成新的分类器和对应权值(这个差值就是下降方向)，而每个样本的权值不变。 6 试编程实现Bagging，以决策树桩为基学习器。 iris数据集 7 编程实现AdaBoost，以不剪枝决策树为基学习器。 iris数据集 Reference 随机森林主页 Python集成学习 随机森林通俗教程 资源: 各类资料 RF/CART比较 周志华ensemble learning相关内容 Stacking算法说明 Stacking技术分享 模型融合效果]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>random forest</tag>
        <tag>ensemble learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adaboost]]></title>
    <url>%2F2019%2F03%2F29%2Fadaboost%2F</url>
    <content type="text"><![CDATA[本篇包含adaboost的手动实现例子以及adaboost在R上的实现例子。 Adaboost简介几个特点Adaboost是一种比较有特点的算法，可以总结如下： 1 每次迭代改变的是样本的分布，而不是重复采样（reweighting not resampling) 2 样本分布的改变取决于样本是否被正确分类 总是分类正确的样本权值低 总是分类错误的样本权值高（通常是边界附近的样本） 3 最终的结果是弱分类器的加权组合。 权值表示该弱分类器的性能 几个优点: 简单，有效1 adaboost是一种有很高精度的分类器 2 可以使用各种方法构建子分类器，adaboost算法提供的是框架 3 当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单 4 简单，不用做特征筛选 5 不用担心overfitting 实例计算过程 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 第1个分类器$G_1$样本权重$w_1$首先给$x_1,x_2,\cdots,x_{10}$ 相同的权重，为$\frac{1}{10}$ 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $w_1$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ 分类结果 $G_1$找到阈值$v = 2.5$, 使该分类器对上述数据分类误差最低。 分类规则 12345def g(x): if(x &gt; 2.5): return -1 elif(x &lt; 2.5): return 1 分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 $w_1$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ 计算误差$e_{1}$误差实际就是错分案例的权重之和。 $e_{1} = \sum_{i=1}^{10}{w_{1i}I(G_{1}(x_{i}) \neq y_{i}}) = \frac{3}{10} = 0.3$ 根据$e_{1}$计算$G_1$在最终分类器中的权重$\alpha_{1} = \frac{1}{2}log\frac{1-e_1}{e_1} = \frac{1}{2}log\frac{1-0.3}{0.3} = 0.4236$ 可见，被误分类样本的权值之和影响误差率，误差率影响基本分类器在最终分类器中所占的权重。 最终分类器$f_1$$f_1(x) = sign(\alpha_{1}G_{1}(x)) = sign(0.4236G_1(x))$ 最终分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $f_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 分类器$f_1(x)$ 分类结果： 样本6 7 8最终被分错。 第2个分类器$G_2$基于$G_1$分类误差调整所有样本权重$w_2$$w_{2i} = \frac{w_{1i}}{Z_1}e^{-\alpha_{1}*G_1(x_i)y_i}$ 其中，$Z_{1} = \sum_{i = 1}^{10}w_{1i}e^{-\alpha_{1}*G_1(x_i)y_i}$ 由权重公式可知，每个样本的权重在下一轮是变大还是变小取决于该样本在上一轮分类中分类正确或错误。如果正确，则$-\alpha_{1}G_1(x_i)y_i &lt; 0$,即$e^{-\alpha_{1}G_1(x_i)y_i} &lt; 1$, 权重将会减小；如果错误, 则$-\alpha_{1}G_1(x_i)y_i &gt; 0$,即$e^{-\alpha_{1}G_1(x_i)y_i} &gt; 1$ 权重会增大。 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 $w_1$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $w_2$ 0.0715 0.0715 0.0715 0.0715 0.0715 0.0715 0.1666 0.1666 0.1666 0.0715 由此可以看出，因为样本数据”6 7 8”被$G_1(x)$分错了，所以它们的权值由之前的0.1增大到0.1666，反之，其它数据皆被分正确，所以它们的权值皆由之前的0.1减小到0.0715。 基于新样本权重$w_2$得到新分类器$G_2$找到阈值$v = 8.5$, 使得该分类器对上述数据分类误差最低。 分类规则 12345def g(x): if(x &gt; 8.5): return -1 elif(x &lt; 8.5): return 1 分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_2(x)$ 1 1 1 1 1 1 1 1 1 -1 $w_2$ 0.0715 0.0715 0.0715 0.0715 0.0715 0.0715 0.1666 0.1666 0.1666 0.0715 计算误差$e_{2}$$e_{2} = \sum_{i=1}^{10}{w_{2i}I(G_{2}x_{i} \neq y_{i})} = 3*0.0715 = 0.2145$ 根据$e_{2}$计算$G_2$在最终分类器中的权重$\alpha_{2} = \frac{1}{2}log\frac{1-e_2}{e_2} = \frac{1}{2}log\frac{1-0.2145}{0.2145} = 0.649$ 最终分类器$f_2$$f_{2}(x) = sign(\alpha_{1}G_{1}(x) + \alpha_{2}G_{2}(x)) = sign(0.4236G_1(x) + 0.649G_2(x))$ 最终分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $f_2(x)$ 1 1 1 1 1 1 1 1 1 -1 $G_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 $G_2(x)$ 1 1 1 1 1 1 1 1 1 -1 分类器$f_2(x)$分类结果： 样本3 4 5 最终被分错。 第3个分类器$G_3$基于$G_2$分类误差调整所有样本权重$w_3$$w_{3i} = \frac{w_{2i}}{Z_2}e^{-\alpha_{2}*G_2(x_i)y_i}$ 其中，$Z_{2} = \sum_{i = 1}^{10}w_{2i}e^{-\alpha_{2}*G_2(x_i)y_i}$ 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_2(x)$ 1 1 1 1 1 1 1 1 1 -1 $w_1$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $\frac{1}{10}$ $w_2$ 0.0715 0.0715 0.0715 0.0715 0.0715 0.0715 0.1666 0.1666 0.1666 0.0715 $w_3$ 0.0455 0.0455 0.0455 0.1666 0.1666 0.1666 0.1060 0.1060 0.1060 0.0455 同理，由于上一轮结束后，样本数据3 4 5被$G_2(x)$分错了，所以它们的权值由之前的0.0715增大到.1666，反之，其它数据皆被分正确，所以它们的权值皆由之前的0.0715减小到0.0455。 基于新样本权重$w_3$得到新分类器$G_3$找到阈值$v = 5.5$, 使得该分类器对上述数据分类误差最低。 分类规则 12345def g(x): if(x &gt; 5.5): return 1 elif(x &lt; 5.5): return -1 分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $G_3(x)$ -1 -1 -1 -1 -1 -1 1 1 1 1 $w_3$ 0.0455 0.0455 0.0455 0.1666 0.1666 0.1666 0.1060 0.1060 0.1060 0.0455 计算误差$e_{3}$$e_{3} = \sum_{i=1}^{10}{w_{2i}I(G_{2}x_{i} \neq y_{i})} = 4*0.0455 = 0.182$ 根据$e_{3}$计算$G_3$在最终分类器$f_3$中的权重$\alpha_{3} = \frac{1}{2}log\frac{1-e_3}{e_3} = \frac{1}{2}log\frac{1-0.182}{0.182} = 0.7514$ 最终分类器$f_3$$f_{3}(x) = sign(\alpha_{1}G_{1}(x) + \alpha_{2}G_{2}(x) + \alpha_{3}G_{3}(x)) = sign(0.4236G_1(x) + 0.649G_2(x) + 0.7514G_3(x))$ 最终分类结果 序号 1 2 3 4 5 6 7 8 9 10 x 0 1 2 3 4 5 6 7 8 9 y 1 1 1 -1 -1 -1 1 1 1 -1 $f_3(x)$ 1 1 1 -1 -1 -1 1 1 1 -1 $G_1(x)$ 1 1 1 -1 -1 -1 -1 -1 -1 -1 $G_2(x)$ 1 1 1 1 1 1 1 1 1 -1 $G_3(x)$ -1 -1 -1 -1 -1 -1 1 1 1 1 分类器$f_3(x)$ 分类结果全部正确，没有错分情况。 至此，整个训练过程结束。 总结现在来总结一下整个训练过程, 包含各样本权重，分类器误差率的变化： 1 训练之前，各个样本的权重被初始化为$w_1 = (0.1, 0.1,0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)$ 2 第一轮分类后，样本6 7 8被分错，对应误差率为$e_1 = 0.3$，此第一个基本分类器在最终分类器中所占的权重为$\alpha_{1} = 0.4236$。 同时，用于下一轮迭代的样本新权重为$w2 = (0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.1666, 0.1666, 0.1666, 0.0715)$ 3 第二轮迭代后，样本3 4 5被分错，对应误差率为$e_2 = 0.2145$，此第二个基本分类器在最终分类器中所占的权重为$\alpha_{2} = 0.649$。 同时，用于下一轮迭代的样本新权值为$w_3 = (0.0455, 0.0455, 0.0455, 0.1666, 0.1666, 0.01666, 0.1060, 0.1060, 0.1060, 0.0455)$ 4 第三轮迭代中，样本0 1 2 9被分错，对应误差率为$e_3 = 0.1820$，此第三个基本分类器在最终分类器中所占的权重为$\alpha_{3} = 0.7514$。 从上述过程中可以发现，如果某些个样本被分错，它们在下一轮迭代中的权值将被增大，同时，其它被分对的样本在下一轮迭代中的权值将被减小。就这样，分错样本权值增大，分对样本权值变小。 最终分类器: $f(x) = sign(f_3(x)) = sign(\alpha_1G_1(x) + \alpha_2G_2(x) + \alpha_3G_3(x)) = 0.4236G_1(x) + 0.6490G_2(x)+0.7514G_3(x)$ Reference Adaboost算法原理与推导 Adaboost简明解释 通俗理解Gradient Boost和Adaboost Bagging and Boosting]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>adaboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tree Models]]></title>
    <url>%2F2019%2F03%2F29%2Fc50%2F</url>
    <content type="text"><![CDATA[本篇主要包含ID3,C4.5在示例数据集上的手动实现以及C5.0的R实现。 决策树生成计算过程数据集 ID 年龄 有工作 有自己的房子 信贷情况 类别 1 青年 否 否 一般 否 2 青年 否 否 好 否 3 青年 是 否 好 是 4 青年 是 是 一般 是 5 青年 否 否 一般 否 6 中年 否 否 一般 否 7 中年 否 否 好 否 8 中年 是 是 好 是 9 中年 否 是 非常好 是 10 中年 否 是 非常好 是 11 老年 否 是 非常好 是 12 老年 否 是 好 是 13 老年 是 否 好 是 14 老年 是 否 非常好 是 15 老年 否 否 一般 否 ID3: InfoGain g(D|A)ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体方法是：从根结点开始，对结点计算所有特征的信息增益，选择信息增益最大的特征作为结点特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。 ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。后续包含决策树的剪枝。决策树的剪枝往往通过最小化决策树整体的损失函数实现。 分别以A1, A2, A3, A4表示年龄、有工作、有自己的房子和信贷情况4个特征，则 (1) H(D) = -\frac{9}{15}\times log\_2{\frac{9}{15}} -\frac{6}{15}\times log\_2{\frac{6}{15}} = 0.971(2) H(D,A\_1) = \frac{5}{15}\times (-\frac{3}{5}\times log\_2{\frac{3}{5}} -\frac{2}{5}\times log\_2{\frac{2}{5}}) + \ \frac{5}{15}\times (-\frac{3}{5}\times log\_2{\frac{3}{5}} -\frac{2}{5}\times log\_2{\frac{2}{5}}) + \ \frac{5}{15}\times (-\frac{1}{5}\times log\_2{\frac{1}{5}} -\frac{4}{5}\times log\_2{\frac{4}{5}}) = 0.888(3) H(D,A\_2) = \frac{5}{15}\times 0 + \frac{10}{15}\times (-\frac{6}{10}\times log\_2{\frac{6}{10}} - \frac{4}{10}\times log\_2{\frac{4}{10}}) = 0.647(4) H(D,A\_3) = \frac{6}{15}\times 0 + \frac{9}{15}\times (-\frac{3}{9}\times log\_2{\frac{3}{9}} - \frac{6}{9}\times log\_2{\frac{6}{9}}) = 0.551(5) H(D,A\_4) = \frac{5}{15}\times (-\frac{1}{5}log\_2{\frac{1}{5}} -\frac{4}{5}log\_2{\frac{4}{5}}) + \ \frac{6}{15}\times (-\frac{4}{6}log\_2{\frac{4}{6}} - \frac{2}{6}log\_2{\frac{2}{6}}) + \frac{4}{15}\times 0 = 0.608由于H(D, A3)=0.551最小，因此，首先选择A3作为最优特征，生成树的第一个分枝。 ID3: try Variance like ANOVA(1) V(D) = 15\times \frac{9}{15}\times \frac{6}{19} = 3.6V(D, A)为基于特征A划分后的组内方差之和；gV(D, A)为组间方差。选择组间方差最大的特征 (2) g\_{V}(D,A\_1) = 5\times \frac{3}{5}\times \frac{2}{5} + 5\times \frac{3}{5}\times \frac{2}{5} + 5\times \frac{1}{5}\times \frac{4}{5} = 3.2(3) g\_{V}(D,A\_2) = 5\times 0 + 10\times \frac{4}{10}\times \frac{6}{10} = 2.4(4) g\_{V}(D,A\_3) = 6\times 0 + 9\times \frac{3}{9}\times \frac{6}{9} = 2(5) g\_{V}(D,A\_4) = 5\times \frac{1}{5}\times \frac{4}{5} + 6\times \frac{2}{6}\times \frac{4}{6} + 4\times 0 = 2.1gV(D, A3)=2最小，所以选择A3为最优特征进行分割。 C4.5: Info Gain Ratio计算信息增益比 g\_{R}(D,A) = \frac{g(D,A)}{H(D)}CART: GiniCART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分枝是取值为“是”的分枝，右分枝是取值为“否”的分枝。这样的决策树相当于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并且在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。 已1，2，3表示年龄为青年，中年，老年；以1，2表示是有工作和有自己的房子的值为是和否；以1，2，3表示信贷情况的值为非常好、好和一般。 (1) Gini(D, A\_{1} = 1) = \frac{5}{15}\times (1 - (\frac{2}{5})^2 - (\frac{3}{5})^2) + \frac{10}{15}\times (1 - (\frac{3}{10})^2 - (\frac{7}{10})^2) = 0.44(2) Gini(D, A\_{2} = 1) = Gini(D, A\_{2} = 2) = 0.32(3) Gini(D, A\_{3} = 1) = Gini(D, A\_{3} = 2) = 0.27(4) Gini(D, A\_{4} = 1) = 0.36(5) Gini(D, A\_{4} = 1) = 0.47(6) Gini(D, A\_{4} = 3) = 0.32由于Gini(D, A3 = 1) = 0.27最小，所以选择特征A3为最优特征，A3 = 1为其最优分割点。于是，根结点生成两个子结点，一个是叶结点。对另一个结点继续使用以上方法在A1, A2, A4中选择最优的特征及其最优切分点。 rpartR中决策树的包: rpart, party, RWeka, ipred, randomForest, gbm, C50. rpart:CART/ID3. The default criterion, which is maximized in each split is the Gini. rpart常用参数 参数 说明 formula y ∼ x1 + x2 + ⋯ + xn data 数据集 na.action 去掉 y 缺失，保留自变量缺失 method class:分类变量，anova:回归树 parms 只适用分类树 parms = list(split,prior,loss) controls 控制决策树形状大小 minsplit,maxdepth,cp rpart.plot常用参数 参数 说明 tree tree模型 type 节点形状1,2,3,4 branch 若=1，则为垂直决策树 cex 符号大小 rpart 参数详解 controls 参数详解 12library(rpart)library(rpart.plot) ## Warning: package &#39;rpart.plot&#39; was built under R version 3.5.3 12library(gmodels)library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 123456789101112131415library(magrittr)data("iris")# seperate the dataset into two parts: the training and test datasetspicker &lt;- sample(150, 110)iris_train = iris[picker,]iris_test = iris[-picker,]# fit the tree model: classification tree# method: switched according to the type of the response variable. # class for categorial, anova for numerical, poisson for count data and exp for survival data.treecart &lt;- rpart(Species ~., data = iris_train, method = "class")# plot the treerpart.plot(treecart) 123456# cross-check its validity by pitching it against our test datapredictions &lt;- predict(treecart, iris_test, type = "class")# confusion matrix: overfitting# CrossTable(predictions,iris_test$Species, prop.r = FALSE,prop.c = FALSE, chisq = FALSE, prop.chisq = FALSE, prop.t = TRUE)confusionMatrix(predictions,iris_test$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 13 1 ## virginica 0 1 10 ## ## Overall Statistics ## ## Accuracy : 0.95 ## 95% CI : (0.8308, 0.9939) ## No Information Rate : 0.375 ## P-Value [Acc &gt; NIR] : 2.044e-14 ## ## Kappa : 0.9244 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.000 0.9286 0.9091 ## Specificity 1.000 0.9615 0.9655 ## Pos Pred Value 1.000 0.9286 0.9091 ## Neg Pred Value 1.000 0.9615 0.9655 ## Prevalence 0.375 0.3500 0.2750 ## Detection Rate 0.375 0.3250 0.2500 ## Detection Prevalence 0.375 0.3500 0.2750 ## Balanced Accuracy 1.000 0.9451 0.9373 12345678# pruning treecart_ms5 &lt;- rpart(Species ~., data = iris_train, method = "class", control = rpart.control(minsplit = 5))treecart_ms10 &lt;- rpart(Species ~., data = iris_train, method = "class", control = rpart.control(minsplit = 10))# rpart.plotpar(mfrow = c(1,2))rpart.plot(treecart, main = "tree_with_parms", branch = 1, cex = 0.8)rpart.plot(treecart_ms5, main = "minsplit=5") C5012345library(C50)# C5.0是一个boosting算法 trials 控制循环次数# build modeltreec50 &lt;- C5.0(Species ~ ., data = iris_train)summary(treec50) ## ## Call: ## C5.0.formula(formula = Species ~ ., data = iris_train) ## ## ## C5.0 [Release 2.07 GPL Edition] Fri Mar 29 10:47:45 2019 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 110 cases (5 attributes) from undefined.data ## ## Decision tree: ## ## Petal.Width &lt;= 0.4: setosa (35) ## Petal.Width &gt; 0.4: ## :...Petal.Width &gt; 1.7: virginica (35) ## Petal.Width &lt;= 1.7: ## :...Petal.Length &lt;= 5: versicolor (36/1) ## Petal.Length &gt; 5: virginica (4/1) ## ## ## Evaluation on training data (110 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 4 2( 1.8%) &lt;&lt; ## ## ## (a) (b) (c) &lt;-classified as ## ---- ---- ---- ## 35 (a): class setosa ## 35 1 (b): class versicolor ## 1 38 (c): class virginica ## ## ## Attribute usage: ## ## 100.00% Petal.Width ## 36.36% Petal.Length ## ## ## Time: 0.0 secs 1plot(treec50) 12# make predictionstable(predict(treec50, newdata = iris_test), iris_test$Species) ## ## setosa versicolor virginica ## setosa 13 0 0 ## versicolor 2 13 1 ## virginica 0 1 10 Reference lassification with Decision Trees R实现决策树 CRAN: C5.0 user_c5.0 决策树对不同分类分布的拟合情况]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xlwings]]></title>
    <url>%2F2019%2F03%2F22%2Fxlwings%2F</url>
    <content type="text"><![CDATA[python加持excel, 速度起飞。 背景开始之前, 回忆一下使用Excel的场景。 同时打开多个Excel文件，多个workbook，每个 workbook 又可以用多个 sheet；且需要在多个 sheet workbook Excel 窗口之间切换 对一个或多个单元格进行增删改查，设置格式，合并分拆等操作 以上场景，需要不停地来回切换，且不同的对象可能需要重复的却又不同的操作，手动工作量大。 xlwings 用于解决以上各种切换和操作，提高效率。 使用说明基于BSD-licensed的Python第三方模块，可以很方便的和Excel交互，它有以下优点： 语法接近 VBA 可以用Python代码取代VBA编写宏 windows中可以用Python写Excel用户自定义函数 全功能支持Numpy,Pandas,matplotlib等库 示例如何执行xlwings编写的自定义函数 将xxxx.xlsx文件另存为 hong.xlsm 文件 确保hong.py 与 hong.xlsm是在同一个文件夹下 确保hong.xlsm文件的VBA是引用xlwings的，如下图 导入自定义函数，点击“xlwings”栏目的“Import Functions”按钮，如下图 可以像调用系统自带函数一样，使用自定义开发的函数，如下图：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Plot in R]]></title>
    <url>%2F2019%2F03%2F14%2Fplotinr%2F</url>
    <content type="text"><![CDATA[本篇内容主要包含常用的可视化图的R实现代码及效果。 Histogram(直方图)base:hist()12345678910library(DMwR)library(car)library(magrittr)df &lt;- algaepar(mfrow = c(1,3))hist(df$mxPH,prob = T)hist(df$mxPH,prob = T,xlab = '',main = 'Histogram of mxPH',ylim = 0:1)density(df$mxPH, na.rm = TRUE) %&gt;% lines()jitter(df$mxPH) %&gt;% rug()qqPlot(df$mxPH, main = 'Q-Q plot of mxPH',ylab = 'mxPH') ## [1] 56 57 1par(mfrow = c(1,1)) 12345678library(lattice)library(gridExtra)hisc1 &lt;- histogram(~ mxPH | season, data = df)# multiple factorshisc2 &lt;- histogram(~ mxPH | season * speed, data = df)# stripplot waysstri1 &lt;- stripplot(size ~ mxPH | speed, data = df, jitter = TRUE)grid.arrange(hisc1,hisc2,stri1,nrow = 3) 箱线图(boxplot)lattice: bwplot()123456library(ggplot2)library(gridExtra)# condition boxplot with latticebx1 &lt;- bwplot(size ~ a1, data = df, ylab = "River Size", xlab = "Algae a1")bx2 &lt;- ggplot(aes(x = size, y = a1),data = df) + geom_boxplot() + coord_flip()grid.arrange(bx1, bx2, ncol = 2) Hmisc:bwplot()1234567891011121314151617181920library(Hmisc)# point: mean# vertical lines: quantiles# 下图结论：小型河流更高频率海藻且分布更加分散bxq1 &lt;- bwplot(size ~ a1, data = df, panel = panel.bpplot, probs = seq(0.01,0.49, by = 0.01), datadensity = TRUE, ylab = "size", xlab = "a1")# 多个因子，因子为分类变量bxq2 &lt;- bwplot(season ~ a1 | size, data = df, panel = panel.bpplot, probs = seq(0.01,0.49, by = 0.01), datadensity = TRUE)bxq3 &lt;- bwplot(season ~ a1 | size, data = df)# 多个因子，因子中包含连续变量 将其离散化后即可## 连续变量离散化mno2i &lt;- equal.count(na.omit(df$mnO2), number = 4, overlap = 1/5)## 类似散点图，从左到右从下到上bxq4 &lt;- stripplot(season ~ a1|mno2i, data = df[!is.na(df$mnO2),])grid.arrange(bxq1,bxq2,bxq4,bxq3,nrow = 2,ncol = 2) 可视化参考书籍 Statistics for Technology, Chatfield(1983)。简单很好的统计书籍。例子简单能说明问题 Introductry Statistics with R, Dalgaard(2002) Visualizing Data, Cleveland(1993), 物有所值 The Element of Graphing Data, Cleveland(1995), 更正式 Data Visualization, Chen(2008) R Graphics, Murrell(2006), R 软件绘图]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅地系列之: Sublime Text]]></title>
    <url>%2F2019%2F03%2F05%2F%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E5%9C%B0%E7%B3%BB%E5%88%97-with-Sublime-Text%2F</url>
    <content type="text"><![CDATA[主要包含与Sublime Text 3相关的快捷键，命令，插件或它特有的一些代码或功能。让速度飞起。 git bash中直接使用sublime text打开指定文件进行编辑 在C:/User/username 下新建一个.bash_profile文件文件内容为alias subl=&quot;/d/installations/tools/Sublime\ Text\ 3/sublime_text.exe&quot;。即sublime的安装地址 之后在git bash 中输入 subl .filename 即可直接用sublime text 打开该文档进行编辑]]></content>
      <tags>
        <tag>SublimeText</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Assets Categorisation]]></title>
    <url>%2F2019%2F03%2F05%2Fassetscategorisation%2F</url>
    <content type="text"><![CDATA[本研究为2014年在NUS做的研究项目ASSETS CATEGORISATION的FINAL REPORT节选。（以下内容更换了数据未更换方法） 问题描述与目标一些研究证明多元投资组合有利于使投资者以较低的风险获得较高的回报。本研究即想探索股票投资的中的多元投资组合问题。 针对这一问题的具体目标，设定为：选取构成Dow Jones Index 30家公司，通过分析找到能够尽可能降低风险；在低风险的前提下，获得较理想收益的股票投资组合。 现有一些方法分析有些投资者会根据公司所属行业将公司进行分组。例如 J.P.Morgan Chase &amp; Co(JPM), The Goldman Sachs Group Inc(GS) 被划分在银行这一分类下；CISCO System Inc(CSCO), International Business Machines Corporation(IBM) and Microsoft Corporation(MSFT) 被划分在软件这一分类下；The Procter &amp; Gamble Company(PG), Johnson &amp; Johnson(JNJ), Pfizer Inc(PFE) 被划分在制药这一分类下。传统投资者常会选择某一行业分类下的股票投资或从不同行业中各选一些股票组合后进行投资。 有些投资者也会根据股价的波动情况进行分组后投资。由于波动与风险相关，因此，可以认为投资者是基于投资风险分组后进行投资的。如将JPM, GS, PG, JNJ, PFE划分为高风险股票，而CSCO,IBM则被划分为低风险股票。投资者则根据自己的风险偏好，选择投资一组或多组进行投资。 评价准则基于30支股票2015-2017年的收益时间序列，通过分析得到股票投资组合。比较该组合在2018年，风险是否低于购买单一股票或分行业选取股票进行投资的策略。 同时，给出对应的收益率。 问题的不同解决方案因子分析运用因子分析，找到“隐藏的”导致30支股票出现这样变动的“看不见的手”，即“潜在因子”。然后，按照30支股票和“潜在因子”的关联关系，将30支股票划分至不同的“潜在因子”下。我们认为如果按照分别从这些“潜在因子”中挑选股票最终得到投资组合，是有可能降低—选择了受同一市场因素影响的总是同向变动的股票的组合—在某因素负面作用下全部下跌这一风险的。 因子分析后，将继续利用Efficient Frontier 算法，得到每支股票的投资比重，使得该投资组合有最小的风险。 数据说明数据获取Yahoo!Finance, 下载30支股票，2015-2018，周维度交易数据。 其中，2015-2017周维度交易数据作为训练集，2018作为测试集。 数据处理原始回报率 $r_i = \frac{p_i - p_j}{p_j} , j = i -1$ $r_i = \frac{p_i - p_j}{p_j} , j = i -1$ 结果与分析讨论改善附录]]></content>
      <categories>
        <category>project</category>
      </categories>
      <tags>
        <tag>case study</tag>
        <tag>fa</tag>
        <tag>stock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows:利用Hexo搭建个人博客]]></title>
    <url>%2F2019%2F02%2F27%2Fblogwithhexo%2F</url>
    <content type="text"><![CDATA[本文介绍利用Hexo搭建个人博客，并将该博客部署至git。以及对于Hexo主题，页面等的设置方法。 需要安装git和nodejs git nodejs 安装完成后，git bash中使用以下命令检查是否安装成功。12node -v npm -v git安装成功后，就可以使用git bash敲命令了，不再用windows cmd了。 安装hexo在安装好git和nodejs后，继续安装hexo。 创建一个文件夹，如命名为blog git bash中, cd到这个文件夹下(或在这个文件夹下右键git bash打开) 输入npm install -g hexo-cli 安装 使用hexo -v检查是否安装成功 至此，hexo安装结束 初始化hexo使用以下命令初始化hexo。hexo init [subdirname] 新建博客文章(post)使用以下命令新建博客文章。12hexo new post_namehexo n post_name 之后就可以在.md中尽情写文章了。 生成静态网页文章写完后，使用以下命令生成静态网页。12hexo generate hexo g 启动预览服务使用以下命令，本地预览。localhost:4000中打开。12hexo server hexo s 或本地测试1hexo debug 发布网站站点配置文件中部署blog位置blog文件夹下 _config.yml为站点配置文件。打开该文件，新增部署语句如下：1234deploy: type: git repo: https://github.com/username/username.github.io.git branch: master 安装git部署插件1npm install hexo-deployer-git --save 部署12hexo deployhexo d 至此，blog上线。 如果是内容重新更新，则在deploy之前先clean。 1hexo clean Hexo个性化设置更换themecd 到blog文件夹，git中继续输入1git clone https://github.com/iissnan/hexo-theme-next themes/next 找到站点配置文件中theme: landscape 修改为theme: next 设置分类生成”分类”并添加type属性 cd 到blog下，执行命令hexo new page categories 成功后会提示INFO Created: d:/blog/source/tags/index.md 找到index.md,添加type: &quot;tags&quot; 注意在Next的_config.yml配置以下内容。打开首页categories 123456789menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 更多分类和标签设置 给文章添加”tags”属性打开需要添加标签的文章，为其添加categories属性。下方的categories:tools表示添加这篇文章到”tools”这个分类下。注意：hexo中一篇文章只能属于一个分类，也就是说如果在”-tools”下方添加”-xxx”,hexo不会产生两个分类，而是把分类嵌套，即该文章属于”-tools”下的”-xxx”分类 1234567---title: 利用Hexo搭建个人博客date: categories:- web 前端- xxx--- 至此，成功给文章添加分类，点击首页的”分类”可以看到该分类下的所有文章。 设置标签生成”标签”并添加type属性 cd 到blog下，执行命令hexo new page tags 成功后会提示INFO Created: d:/blog/source/categories/index.md 找到index.md,添加type: &quot;categories&quot; 给文章添加”tags”属性打开需要添加标签的文章，为其添加tags属性。下方的tags: blog -hexo 就是这篇文章的标签了 123456789---title: 利用Hexo搭建个人博客date: categories:- toolstags:- blog - hexo --- 至此，成功给文章添加标签，点击首页的”标签”可以看到该标签下的所有文章。 设置about生成”about”并添加想说的话 cd 到blog下，执行命令hexo new page about 成功后会提示INFO Created: d:/blog/source/categories/index.md 找到index.md,写上你想说的话 至此，成功给文章添加标签，点击首页的”标签”可以看到该标签下的所有文章。 首页设置：不显示全文Hexo的Next主题默认首页显示每篇文章的全文内容，下面将其修改为只显示部分内容。 方法1: 修改_config.yml 文件设置打开themes/next目录下的_config.yml 文件，找到以下代码： 12345# Automatically Excerpt. Not recommend.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150 把enable的false改成true即可。length设定文章预览的文本长度。 修改后重启hexo即可。 方法2: 在.md文章内容后加上&lt;!--more--&gt;在.md文章内容后加上&lt;!--more--&gt;,首页和列表页显示的文章内容就是&lt;!--more--&gt;之前的文字，之后的不会显示 效果对比第一种会格式化文章的样式，直接把文章挤在一起显示，最后会有...。第二种不会有。 添加搜索功能：LocalSearch搜索安装hexo-generator-searchdb. cd到blog下，执行以下命令$$ npm install hexo-generator-searchdb --save 编辑站点配置文件,_config.yml,新增以下内容到任意位置 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件，_config.yml,启用本地搜索功能 123# locallocal_search: enable: ture 最后重新生成。 文章置顶安装node插件12npm uninstall hexo-generator-index --save npm install hexo-generator-index-pin-top --save 添加标记在需要置顶的文章的Front-matter中加上top:true即可。 123456title: date:tags:categories:description:top: true 显示版权信息找到主题目录下的_config.yml文件，修改以下部分： 12345# Declare license on postspost_copyright: enable: false license: CC BY-NC-SA 3.0 license_url: https://creativecommons.org/licenses/by-nc-sa/3.0/ 将其中的enable: false 改为enable: true。 同时，在站点配置文件中，修改URL为自己的站点的域名地址。 访问统计功能添加博客的访问量。 不蒜子统计显示文章的访客数，浏览量等信息找到Next主题下的配置文件_config.yml,找到busuanzi_count，将enable: false改为enable: true 1234567891011121314151617# Show PV/UV of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzi/busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: 访客数 &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; site_uv_footer: 人次 # custom pv span for the whole site site_pv: true site_pv_header: 浏览量 &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: 阅读量 &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; page_pv_footer: 次 当enable: true时，代表开启全局开关； 当site_uv: true时，代表页面底部显示站点UV 当site_pv: true时，代表页面底部显示站点PV 当page_uv: true时，代表文章页面标题下显示该页面的PV 显示文章更新时间Next主题下，找到主题配置文件_config.yml的post_meta部分: 123456# Post meta display settingspost_meta: item_text: true created_at: true updated_at: false categories: true 将updated_at: false改为updated_at: true即可。 npm 问题无响应问题在国内可能会出现npm无响应的情况，这时可以换用cnpm 1$ npm install cnpm -g --registry=https://registry.npm.taobao.org 之后使用cnpm install [name] CERT_NOT_VALID_YET先运行 1npm config set strict-ssl false 即可。 .Rmd直接导入hexo有大量的建模使用R实现; 日后查看整个建模流程或查阅一些常用命令。具体查看需求的内容包含两点： 代码以及代码的执行结果，包含图片 以上需同步到博客 针对以上两个需求的实现 用rmarkdown直接实现 借助hexo &lt;- 将rmd转成md, 放到_post文件夹下即可。 可能有bug的点，是图片以及公式能否显示成功。(已解决) 已有方案Xie yihui(谢益辉),谢大大已经有新的package: blogdown可以实现在RStudio中生成博客, 支持Hugo主题。 但自己，已经有基于Hexo搭建的博客，且现阶段基于sublime text + markdown + git bash 写博客的流程已经比较顺手且暂时不想再转换。所以，准备鼓捣利用rmarkdown，生成.md文档。还保持之前写博客的方法。 从网上查了一些资料写了以下方案，亲测代码，代码结果输出，图片，公式，网页链接引用均没有问题。 流程不算复杂，基本满足我个人需求。 解决方案.rmd -&gt; .md -&gt; hexo在rmd文件yaml head里加入以下代码： 12345--- output: md_document variant: markdown_github--- 直接knit即可生成.md，然后将这个.md移到_post文件夹下即可。 需要注意的一点是，如果.rmd中包含的了图片，在knit的时候，会创建名为rmdname_files的文件夹，存放该.md中对应的图。 md引用该图使用的是markdown语法的相对路径，即![](rmdname_files/figure-markdown_github/chunkname-1.png)。为了保证能正确显示图片，建议使用以下流程： 在_config.yml中, 将post_asset_folder: false 改为 true。 修改后，每次执行hexo n(new) post_name 将不只在_post下生成post_name.md, 同时新建一个与post_name同名的文件夹。 在Rstudio中, setwd()到post_name文件夹下。新建.rmd并保存到该文件夹下，注意.rmd命名为post_name.rmd 开始写.rmd文档。写好后，点击knit, 将会在post_name文件夹中生成post_name.md以及新文件夹:post_name_files 将post_name文件夹中的post_name.md替换_post文件夹下的同名.md即可。 其他方案参见BaoDuGe_飽蠹閣的博文：如何将我的R项目更好地展示在Hexo博客上 Next各种样式特殊内容提示note defaultdefault note infoinfo note successsuccess note warningwarnig note dangerdanger note primaryprimary 1234&#123;%note default%&#125;### note defaultdefault &#123;%endnote%&#125; 文本特殊格式复道行空, 不霁何虹。 长桥卧波,未云何龙。 1&#123;%label @复道%&#125;&#123;%label primary@行空%&#125;, &#123;%label default@不霁%&#125;&#123;%label success@何虹%&#125;。 设置文本颜色使用html语法直接写即可。 我可以设置这一句的颜色哈哈 &lt;font color=&quot;#FF0000&quot;&gt; 我可以设置这一句的颜色哈哈 &lt;/font&gt; 我还可以设置这一句的大小嘻嘻 &lt;font size=6&gt; 我还可以设置这一句的大小嘻嘻 &lt;/font&gt; 我甚至可以设置这一句的颜色和大小呵呵 &lt;font size=5 color=&quot;#FF0000&quot;&gt; 我甚至可以设置这一句的颜色和大小呵呵&lt;/font&gt; 文字居中&lt;center&gt;这一行需要居中&lt;/center&gt; 按钮BottonNexT进度条加载 1&#123;%btn url,test_tile,,title%&#125; Botton with IconNexT主题设置 1&#123;%btn url,showlabel,hand-o-right%&#125; Botton with Fix-widthNexT个性化设置 1&#123;%btn url, Next样式, hand-o-right fa-fw %&#125; Center 12&lt;div class=&quot;text-center&quot;&gt;&lt;span&gt;&#123;%btn url,,google%&#125;&#123;%btn url,,edge%&#125;&#123;%btn url,,chrome%&#125;&lt;/span&gt;&lt;span&gt;&#123;%btn url,,terminal%&#125;&#123;%btn url,,diamond fa-rotate-270%&#125;&lt;/span&gt;&lt;/div&gt; 参考 Next各种样式 Hexo各种自定义 怎么在hexo博客系统中用Rmarkdown写文章 资源文件夹 Hexo图片插入 Hexo用Latex渲染数学公式]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Way of Thinking]]></title>
    <url>%2F2019%2F02%2F25%2Fwayofthinking%2F</url>
    <content type="text"><![CDATA[类型 思维要点 STAR法则 Situation Task Action Result 背景： 阐述为什么做这件事/事情是在什么情况下发生的 目标： 目的达到什么样的效果/你是如何明确你的任务的 方案：怎么去做这件事，具体有哪些动作/针对这样的情况分析，你采用了什么行动方式 (难点：实施过程中存在哪些难点，可形成里程碑式攻克点) 成果：最终取得的成果/结果怎样，在这样的情况下你学习到了什么 分析 数据实际情况 发现的问题点 建议解决方案 监控追踪是分析的基础 问题-&gt;全方案 描述具体问题：发现并描述一个具体问题点 评估整体影响：问题对整体项目的影响 给出单一回答：精准回答此问题的解决办法 给出规模化方案：规模化复制形成迭代的新方案 5W2H方法 What：工作的内容和达成的目标 Why：做这项工作的原因 When：在什么时间进行工作 Where：工作发生的地点 Where：工作发生的地点 How much：需要多少成本 SMART目标管理原则 Specific：具体的 Measurable：可测量的 Attainable：可达到的 Relevant：相关的 Time based：时间的]]></content>
  </entry>
  <entry>
    <title><![CDATA[分析报告包含的内容框架参考]]></title>
    <url>%2F2018%2F12%2F08%2Freportframework%2F</url>
    <content type="text"><![CDATA[本篇内容主要为分析内容的框架参考。 问题描述与目标 一般包含问题的背景（遇到什么问题/解决问题的意义） 主要解决的问题 解决该问题带来的收益 样本以及变量概述 数据说明 数据来源 数据集每条记录概述，变量描述 数据集训练集，测试集划分 数据可视化和摘要 统计描述，一般是探索性数据分析，连续变量均值，中位数，四分位数，极值等一系列统计信息。 可以通过观察均值与中位数的差异以及四分位距，了解数据偏度和分散情况；对于离散性变量，可以看到每个取值的频数，了解样本分布是否平均等信息。 可以绘制直方图(箱线图)了解数据分布 数据缺失定义预测任务 预测什么 变量是什么 预测任务 特征选择 问题的不同解决方案 模型评价准则 实验方法 预测模型 如何应用训练集数据建模 建模工具/技术 从预测到实践 如何应用预测模型 与实际相关的评价准则 模型集成 模型评价和选择 模型比较 实验比较 结果分析 系统集成]]></content>
      <tags>
        <tag>case study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caret]]></title>
    <url>%2F2017%2F09%2F23%2Fcaret%2F</url>
    <content type="text"></content>
  </entry>
</search>
